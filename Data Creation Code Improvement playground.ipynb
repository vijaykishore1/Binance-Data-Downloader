{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6887b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:43:26.146950Z",
     "start_time": "2023-10-02T10:43:22.014955Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bcec77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:13:36.566512Z",
     "start_time": "2024-01-16T12:13:34.140671Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Start a new instance of the Chrome driver\n",
    "webdriver_service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=webdriver_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108c2a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:13:50.319366Z",
     "start_time": "2024-01-16T12:13:36.568017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\downloaded_data\n",
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\extracted_data\n",
      "['1000BONKUSDT', '1000BTTCUSDT', '1000FLOKIUSDT', '1000LUNCBUSD', '1000LUNCUSDT', '1000PEPEUSDT', '1000RATSUSDT', '1000SATSUSDT', '1000SHIBBUSD', '1000SHIBUSDT', '1000XECUSDT', '1INCHUSDT', 'AAVEUSDT', 'ACEUSDT', 'ACHUSDT', 'ADABUSD', 'ADAUSDT', 'AGIXBUSD', 'AGIXUSDT', 'AGLDUSDT', 'AKROUSDT', 'ALGOUSDT', 'ALICEUSDT', 'ALPHAUSDT', 'AMBBUSD', 'AMBUSDT', 'ANCBUSD', 'ANCUSDT', 'ANKRUSDT', 'ANTUSDT', 'APEBUSD', 'APEUSDT', 'API3USDT', 'APTBUSD', 'APTUSDT', 'ARBUSDT', 'ARKMUSDT', 'ARKUSDT', 'ARPAUSDT', 'ARUSDT', 'ASTRUSDT', 'ATAUSDT', 'ATOMUSDT', 'AUCTIONBUSD', 'AUCTIONUSDT', 'AUDIOUSDT', 'AVAXBUSD', 'AVAXUSDT', 'AXSUSDT', 'BADGERUSDT', 'BAKEUSDT', 'BALUSDT', 'BANDUSDT', 'BATUSDT', 'BCHUSDT', 'BEAMXUSDT', 'BELUSDT', 'BICOUSDT', 'BIGTIMEUSDT', 'BLUEBIRDUSDT', 'BLURUSDT', 'BLZUSDT', 'BNBBUSD', 'BNBUSDT', 'BNTUSDT', 'BNXUSDT', 'BNXUSDTSETTLED', 'BONDUSDT', 'BSVUSDT', 'BTCBUSD', 'BTCBUSD_210129', 'BTCBUSD_210226', 'BTCDOMUSDT', 'BTCSTUSDT', 'BTCUSDT', 'BTCUSDT_210326', 'BTCUSDT_210625', 'BTCUSDT_210924', 'BTCUSDT_211231', 'BTCUSDT_220325', 'BTCUSDT_220624', 'BTCUSDT_220930', 'BTCUSDT_221230', 'BTCUSDT_230331', 'BTCUSDT_230630', 'BTCUSDT_230929', 'BTCUSDT_231229', 'BTCUSDT_240329', 'BTCUSDT_240628', 'BTSUSDT', 'BTTUSDT', 'BZRXUSDT', 'C98USDT', 'CAKEUSDT', 'CELOUSDT', 'CELRUSDT', 'CFXUSDT', 'CHRUSDT', 'CHZUSDT', 'CKBUSDT', 'COCOSUSDT', 'COMBOUSDT', 'COMPUSDT', 'COTIUSDT', 'CRVUSDT', 'CTKUSDT', 'CTSIUSDT', 'CVCUSDT', 'CVXBUSD', 'CVXUSDT', 'CYBERUSDT', 'DARUSDT', 'DASHUSDT', 'DEFIUSDT', 'DENTUSDT', 'DGBUSDT', 'DODOBUSD', 'DODOUSDT', 'DODOXUSDT', 'DOGEBUSD', 'DOGEUSDT', 'DOTBUSD', 'DOTECOUSDT', 'DOTUSDT', 'DUSKUSDT', 'DYDXUSDT', 'EDUUSDT', 'EGLDUSDT', 'ENJUSDT', 'ENSUSDT', 'EOSUSDT', 'ETCBUSD', 'ETCUSDT', 'ETHBTC', 'ETHBUSD', 'ETHUSDT', 'ETHUSDT_210326', 'ETHUSDT_210625', 'ETHUSDT_210924', 'ETHUSDT_211231', 'ETHUSDT_220325', 'ETHUSDT_220624', 'ETHUSDT_220930', 'ETHUSDT_221230', 'ETHUSDT_230331', 'ETHUSDT_230630', 'ETHUSDT_230929', 'ETHUSDT_231229', 'ETHUSDT_240329', 'ETHUSDT_240628', 'ETHWUSDT', 'FETUSDT', 'FILBUSD', 'FILUSDT', 'FLMUSDT', 'FLOWUSDT', 'FOOTBALLUSDT', 'FRONTUSDT', 'FTMBUSD', 'FTMUSDT', 'FTTBUSD', 'FTTUSDT', 'FXSUSDT', 'GALABUSD', 'GALAUSDT', 'GALBUSD', 'GALUSDT', 'GASUSDT', 'GLMRUSDT', 'GMTBUSD', 'GMTUSDT', 'GMXUSDT', 'GRTUSDT', 'GTCUSDT', 'HBARUSDT', 'HFTUSDT', 'HIFIUSDT', 'HIGHUSDT', 'HNTUSDT', 'HOOKUSDT', 'HOTUSDT', 'ICPBUSD', 'ICPUSDT', 'ICPUSDT_SETTLED', 'ICXUSDT', 'IDEXUSDT', 'IDUSDT', 'ILVUSDT', 'IMXUSDT', 'INJUSDT', 'IOSTUSDT', 'IOTAUSDT', 'IOTXUSDT', 'JASMYUSDT', 'JOEUSDT', 'JTOUSDT', 'KASUSDT', 'KAVAUSDT', 'KEEPUSDT', 'KEYUSDT', 'KLAYUSDT', 'KNCUSDT', 'KSMUSDT', 'LDOBUSD', 'LDOUSDT', 'LENDUSDT', 'LEVERBUSD', 'LEVERUSDT', 'LINAUSDT', 'LINKBUSD', 'LINKUSDT', 'LITUSDT', 'LOOMUSDT', 'LPTUSDT', 'LQTYUSDT', 'LRCUSDT', 'LTCBUSD', 'LTCUSDT', 'LUNA2BUSD', 'LUNA2USDT', 'LUNABUSD', 'LUNAUSDT', 'MAGICUSDT', 'MANAUSDT', 'MASKUSDT', 'MATICBUSD', 'MATICUSDT', 'MAVUSDT', 'MBLUSDT', 'MDTUSDT', 'MEMEUSDT', 'MINAUSDT', 'MINAUSDTSETTLED', 'MKRUSDT', 'MOVRUSDT', 'MTLUSDT', 'NEARBUSD', 'NEARUSDT', 'NEOUSDT', 'NFPUSDT', 'NKNUSDT', 'NMRUSDT', 'NTRNUSDT', 'NUUSDT', 'OCEANUSDT', 'OGNUSDT', 'OMGUSDT', 'ONEUSDT', 'ONGUSDT', 'ONTUSDT', 'OPUSDT', 'ORBSUSDT', 'ORDIUSDT', 'OXTUSDT', 'PENDLEUSDT', 'PEOPLEUSDT', 'PERPUSDT', 'PHBBUSD', 'PHBUSDT', 'POLYXUSDT', 'POWRUSDT', 'PYTHUSDT', 'QNTUSDT', 'QTUMUSDT', 'RADUSDT', 'RAYUSDT', 'RDNTUSDT', 'REEFUSDT', 'RENUSDT', 'RIFUSDT', 'RLCUSDT', 'RNDRUSDT', 'ROSEUSDT', 'RSRUSDT', 'RUNEUSDT', 'RVNUSDT', 'SANDBUSD', 'SANDUSDT', 'SCUSDT', 'SEIUSDT', 'SFPUSDT', 'SKLUSDT', 'SLPUSDT', 'SNTUSDT', 'SNXUSDT', 'SOLBUSD', 'SOLUSDT', 'SPELLUSDT', 'SRMUSDT', 'SSVUSDT', 'STEEMUSDT', 'STGUSDT', 'STMXUSDT', 'STORJUSDT', 'STPTUSDT', 'STRAXUSDT', 'STXUSDT', 'SUIUSDT', 'SUPERUSDT', 'SUSHIUSDT', 'SXPUSDT', 'THETAUSDT', 'TIAUSDT', 'TLMBUSD', 'TLMUSDT', 'TLMUSDTSETTLED', 'TOKENUSDT', 'TOMOUSDT', 'TRBUSDT', 'TRUUSDT', 'TRXBUSD', 'TRXUSDT', 'TUSDT', 'TWTUSDT', 'UMAUSDT', 'UNFIUSDT', 'UNIBUSD', 'UNIUSDT', 'USDCUSDT', 'USTCUSDT', 'VETUSDT', 'WAVESBUSD', 'WAVESUSDT', 'WAXPUSDT', 'WLDUSDT', 'WOOUSDT', 'XEMUSDT', 'XLMUSDT', 'XMRUSDT', 'XRPBUSD', 'XRPUSDT', 'XTZUSDT', 'XVGUSDT', 'XVSUSDT', 'YFIIUSDT', 'YFIUSDT', 'YGGUSDT', 'ZECUSDT', 'ZENUSDT', 'ZILUSDT', 'ZRXUSDT']\n",
      "['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02', '2024-03', '2024-04', '2024-05', '2024-07', '2024-08', '2024-09', '2024-10', '2024-11', '2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05', '2025-06', '2025-07', '2025-08', '2025-09', '2025-10', '2025-11', '2025-12', '2026-01', '2026-02', '2026-03', '2026-04', '2026-05', '2026-06', '2026-07', '2026-08', '2026-09', '2026-10', '2026-11', '2026-12', '2027-01', '2027-02', '2027-03', '2027-04', '2027-05', '2027-06', '2027-07', '2027-08', '2027-09', '2027-10', '2027-11', '2027-12', '2028-01', '2028-02', '2028-03', '2028-04', '2028-05', '2028-06', '2028-07', '2028-08', '2028-09', '2028-10', '2028-11', '2028-12', '2029-01', '2029-03', '2029-04', '2029-05', '2029-06', '2029-07', '2029-08', '2029-09', '2029-10', '2029-11', '2029-12', '2030-01', '2030-02', '2030-03', '2030-04', '2030-05', '2030-06', '2030-07', '2030-08', '2030-09', '2030-10', '2030-11', '2030-12', '2031-01', '2031-02', '2031-03', '2031-04', '2031-05', '2031-06', '2031-07', '2031-08', '2031-09', '2031-10', '2031-11', '2031-12', '2032-01', '2032-02', '2032-03', '2032-04', '2032-05', '2032-06', '2032-07', '2032-08', '2032-09', '2032-10', '2032-11', '2032-12', '2033-01', '2033-02', '2033-03', '2033-04', '2033-05', '2033-06', '2033-07', '2033-08', '2033-10', '2033-11', '2033-12', '2034-01', '2034-02', '2034-03', '2034-04', '2034-05', '2034-06', '2034-07', '2034-08', '2034-09', '2034-10', '2034-11', '2034-12', '2035-01', '2035-02', '2035-03', '2035-04', '2035-05', '2035-06', '2035-07', '2035-08', '2035-09', '2035-10', '2035-11', '2035-12', '2036-01', '2036-02', '2036-03', '2036-04', '2036-05', '2036-06', '2036-07', '2036-08', '2036-09', '2036-10', '2036-11', '2036-12', '2037-01', '2037-02', '2037-03', '2037-04', '2037-05', '2037-06', '2037-07', '2037-08', '2037-09', '2037-10', '2037-11', '2037-12', '2038-01', '2038-03', '2038-04', '2038-05', '2038-06', '2038-07', '2038-08', '2038-09', '2038-10', '2038-11', '2038-12', '2039-01', '2039-02', '2039-03', '2039-04', '2039-05', '2039-06', '2039-07', '2039-08', '2039-09', '2039-10', '2039-11', '2039-12', '2040-01', '2040-02', '2040-03', '2040-04', '2040-05', '2040-06', '2040-07', '2040-08', '2040-09', '2040-10', '2040-11', '2040-12', '2041-01', '2041-02', '2041-03', '2041-04', '2041-05', '2041-06', '2041-07', '2041-08', '2041-09', '2041-10', '2041-11', '2041-12', '2042-01', '2042-02', '2042-03', '2042-04', '2042-05', '2042-06', '2042-07', '2042-08', '2042-09', '2042-10', '2042-12', '2043-01', '2043-02', '2043-03', '2043-04', '2043-05', '2043-06', '2043-07', '2043-08', '2043-09', '2043-10', '2043-11', '2043-12', '2044-01', '2044-02', '2044-03', '2044-04', '2044-05', '2044-06', '2044-07', '2044-08', '2044-09', '2044-10', '2044-11', '2044-12', '2045-01', '2045-02', '2045-03', '2045-04', '2045-05', '2045-06', '2045-07', '2045-08', '2045-09', '2045-10', '2045-11', '2045-12', '2046-01', '2046-02', '2046-03', '2046-04', '2046-05', '2046-06', '2046-07', '2046-08', '2046-09', '2046-10', '2046-11', '2046-12', '2047-01', '2047-02', '2047-03', '2047-05', '2047-06', '2047-07', '2047-08', '2047-09', '2047-10', '2047-11', '2047-12', '2048-01', '2048-02', '2048-03', '2048-04', '2048-05', '2048-06', '2048-07', '2048-08', '2048-09', '2048-10', '2048-11', '2048-12', '2049-01', '2049-02', '2049-03', '2049-04', '2049-05', '2049-06', '2049-07', '2049-08', '2049-09', '2049-10', '2049-11', '2049-12', '2050-01', '2050-02', '2050-03', '2050-04', '2050-05', '2050-06', '2050-07', '2050-08', '2050-09', '2050-10', '2050-11', '2050-12', '2051-01', '2051-02', '2051-03', '2051-04', '2051-05', '2051-06', '2051-07', '2051-08', '2051-09', '2051-10', '2051-11', '2051-12', '2052-01', '2052-03', '2052-04', '2052-05', '2052-06', '2052-07', '2052-08', '2052-09', '2052-10', '2052-11', '2052-12', '2053-01', '2053-02', '2053-03', '2053-04', '2053-05', '2053-06', '2053-07', '2053-08', '2053-09', '2053-10', '2053-11', '2053-12', '2054-01', '2054-02', '2054-03', '2054-04', '2054-05', '2054-06', '2054-07', '2054-08', '2054-09', '2054-10', '2054-11', '2054-12', '2055-01', '2055-02', '2055-03', '2055-04', '2055-05', '2055-06', '2055-07', '2055-08', '2055-09', '2055-10', '2055-11', '2055-12', '2056-01', '2056-02', '2056-03', '2056-04', '2056-05', '2056-06', '2056-07', '2056-08', '2056-10', '2056-11', '2056-12', '2057-01', '2057-02', '2057-03', '2057-04', '2057-05', '2057-06', '2057-07', '2057-08', '2057-09', '2057-10', '2057-11', '2057-12', '2058-01', '2058-02', '2058-03', '2058-04', '2058-05', '2058-06', '2058-07', '2058-08', '2058-09', '2058-10', '2058-11', '2058-12', '2059-01', '2059-02', '2059-03', '2059-04', '2059-05', '2059-06', '2059-07', '2059-08', '2059-09', '2059-10', '2059-11', '2059-12', '2060-01', '2060-02', '2060-03', '2060-04', '2060-05', '2060-06', '2060-07', '2060-08', '2060-09', '2060-10', '2060-11', '2060-12', '2061-01', '2061-03', '2061-04', '2061-05', '2061-06', '2061-07', '2061-08', '2061-09', '2061-10', '2061-11', '2061-12', '2062-01', '2062-02', '2062-03', '2062-04', '2062-05', '2062-06', '2062-07', '2062-08', '2062-09', '2062-10', '2062-11', '2062-12', '2063-01', '2063-02', '2063-03', '2063-04', '2063-05', '2063-06', '2063-07', '2063-08', '2063-09', '2063-10', '2063-11', '2063-12', '2064-01', '2064-02', '2064-03', '2064-04', '2064-05', '2064-06', '2064-07', '2064-08', '2064-09', '2064-10', '2064-11', '2064-12', '2065-01', '2065-02', '2065-03', '2065-04', '2065-05', '2065-06', '2065-07', '2065-08', '2065-09', '2065-10', '2065-12', '2066-01', '2066-02', '2066-03', '2066-04', '2066-05', '2066-06', '2066-07', '2066-08', '2066-09', '2066-10', '2066-11', '2066-12', '2067-01', '2067-02', '2067-03', '2067-04', '2067-05', '2067-06', '2067-07', '2067-08', '2067-09', '2067-10', '2067-11', '2067-12', '2068-01', '2068-02', '2068-03', '2068-04', '2068-05', '2068-06', '2068-07', '2068-08', '2068-09', '2068-10', '2068-11', '2068-12', '2069-01', '2069-02', '2069-03', '2069-04', '2069-05', '2069-06', '2069-07', '2069-08', '2069-09', '2069-10', '2069-11', '2069-12', '2070-01', '2070-02', '2070-03', '2070-05', '2070-06', '2070-07', '2070-08', '2070-09', '2070-10', '2070-11', '2070-12', '2071-01', '2071-02', '2071-03', '2071-04', '2071-05', '2071-06', '2071-07', '2071-08', '2071-09', '2071-10', '2071-11', '2071-12', '2072-01', '2072-02', '2072-03', '2072-04', '2072-05', '2072-06', '2072-07', '2072-08', '2072-09', '2072-10', '2072-11', '2072-12', '2073-01', '2073-02', '2073-03', '2073-04', '2073-05', '2073-06', '2073-07', '2073-08', '2073-09', '2073-10', '2073-11', '2073-12', '2074-01', '2074-02', '2074-03', '2074-04', '2074-05', '2074-06', '2074-07', '2074-08', '2074-09', '2074-10', '2074-11', '2074-12', '2075-01', '2075-03', '2075-04', '2075-05', '2075-06', '2075-07', '2075-08', '2075-09', '2075-10', '2075-11', '2075-12', '2076-01', '2076-02', '2076-03', '2076-04', '2076-05', '2076-06', '2076-07', '2076-08', '2076-09', '2076-10', '2076-11', '2076-12', '2077-01', '2077-02', '2077-03', '2077-04', '2077-05', '2077-06', '2077-07', '2077-08', '2077-09', '2077-10', '2077-11', '2077-12', '2078-01', '2078-02', '2078-03', '2078-04', '2078-05', '2078-06', '2078-07', '2078-08', '2078-09', '2078-10', '2078-11', '2078-12', '2079-01', '2079-02', '2079-03', '2079-04', '2079-05', '2079-07', '2079-08', '2079-09', '2079-10', '2079-11', '2079-12']\n",
      "['12h', '15m', '1d', '1h', '1m', '1mo', '1w', '2h', '30m', '3d', '3m', '4h', '5m', '6h', '8h']\n",
      "Symbol,month and chart arrays are successfully created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data creation utilities successfully initialized\n"
     ]
    }
   ],
   "source": [
    "%run ./data_creation_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d6e7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:13:50.335251Z",
     "start_time": "2024-01-16T12:13:50.320367Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from selenium import webdriver\n",
    "import talib\n",
    "import datetime\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import glob\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import inspect\n",
    "import talib\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.request\n",
    "from datetime import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c20fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:13:50.350365Z",
     "start_time": "2024-01-16T12:13:50.337253Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f346cdf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:13:50.365595Z",
     "start_time": "2024-01-16T12:13:50.351366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01\n",
      "['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01']\n"
     ]
    }
   ],
   "source": [
    "# Get the current date\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date as a string in the desired format\n",
    "date_string = now.strftime(\"%Y-%m\")\n",
    "\n",
    "# Print the date string\n",
    "print(date_string)\n",
    "\n",
    "idx = MONTH_ARRAY.index(date_string)\n",
    "MONTH_ARRAY = MONTH_ARRAY[:idx + 1]\n",
    "# months = MONTH_ARRAY\n",
    "print(MONTH_ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79712b02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:13:50.381101Z",
     "start_time": "2024-01-16T12:13:50.367099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-06', '2024-01-07', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12', '2024-01-13', '2024-01-14', '2024-01-15', '2024-01-16']\n"
     ]
    }
   ],
   "source": [
    "# Get the first day of the current month\n",
    "first_day = now.replace(day=1)\n",
    "\n",
    "# Create an empty list to store the days\n",
    "DAY_ARRAY = []\n",
    "\n",
    "# Loop through the days from the first day of the current month to today\n",
    "while first_day <= now:\n",
    "    # Format the date as a string in the desired format\n",
    "    date_string = first_day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Add the date string to the list\n",
    "    DAY_ARRAY.append(date_string)\n",
    "\n",
    "    # Move to the next day\n",
    "    first_day += timedelta(days=1)\n",
    "print(DAY_ARRAY)\n",
    "# days = DAY_ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d5e1553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:13:50.397266Z",
     "start_time": "2024-01-16T12:13:50.382104Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "master_dictionary = {\n",
    "    \"symbols\": [\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('BTCUSDT')],\n",
    "#         SYMBOL_ARRAY[SYMBOL_ARRAY.index('1000PEPEUSDT')],\n",
    "        SYMBOL_ARRAY[SYMBOL_ARRAY.index('PYTHUSDT')],\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('ETHUSDT')],\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('ETHBUSD')],\n",
    "        #         SYMBOL_ARRAY[SYMBOL_ARRAY.index('BTCBUSD')]\n",
    "    ],\n",
    "    \"chart_times\": [\n",
    "        #     CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('5m')],\n",
    "        #     CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('1m')],\n",
    "        CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('4h')]\n",
    "    ],\n",
    "    \"timeperiods\": [5, 8, 13, 21, 30, 34, 50, 55, 89, 100, 144, 200, 233],\n",
    "    \"win_percentage\":\n",
    "    0.8,\n",
    "    \"loss_percentage\":\n",
    "    0.4,\n",
    "    \"monthly_or_daily_1_or_2\":\n",
    "    1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1e13f",
   "metadata": {},
   "source": [
    "# code for downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d18c6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T09:29:12.460451Z",
     "start_time": "2024-01-15T09:29:12.409087Z"
    }
   },
   "outputs": [],
   "source": [
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "#         root_dir = Path.cwd()\n",
    "#         # Create the new folder path\n",
    "#         folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "#         folder_path.mkdir(parents=True, exist_ok=True)\n",
    "#         count = 0\n",
    "#         for month in master_dictionary[\"months\"]:\n",
    "#             # Construct the link\n",
    "#             link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "#             symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "#             # Create the file path\n",
    "#             file_path = Path(folder_path) / symbol_object\n",
    "#             if not file_path.exists():\n",
    "#                 try:\n",
    "#                     # Download the file\n",
    "#                     urllib.request.urlretrieve(link, file_path)\n",
    "#                     count += 1\n",
    "#                 except:\n",
    "# #                     print(f'{link} not found')\n",
    "#                     continue\n",
    "#         if count > 0:\n",
    "#             print(\"***                  ***\")\n",
    "#             print(f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "#         else:\n",
    "#             print(\"you're already up to date\")\n",
    "\n",
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "#         root_dir = Path.cwd()\n",
    "#         # Create the new folder path\n",
    "#         folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "#         folder_path.mkdir(parents=True, exist_ok=True)\n",
    "#         count = 0\n",
    "#         for day in master_dictionary[\"days\"]:\n",
    "#             # Construct the link\n",
    "#             link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "#             symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "#             # Create the file path\n",
    "#             file_path = Path(folder_path) / symbol_object\n",
    "#             if not file_path.exists():\n",
    "#                 try:\n",
    "#                     # Download the file\n",
    "#                     urllib.request.urlretrieve(link, file_path)\n",
    "#                     count += 1\n",
    "#                 except:\n",
    "# #                     print(f'{link} not found')\n",
    "#                     continue\n",
    "#         if count > 0:\n",
    "#             print(\"***                  ***\")\n",
    "#             print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "#         else:\n",
    "#             print(\"you're already up to date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a99449",
   "metadata": {},
   "source": [
    "## new pyspark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d565a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T10:04:29.268631Z",
     "start_time": "2023-08-20T10:04:28.161759Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "# #         # Set up an empty list for the data frames\n",
    "# #         df_list = []\n",
    "#         spark = SparkSession.builder.appName('DataProcessing').getOrCreate()\n",
    "#         # Compile the regular expression pattern\n",
    "#         pattern = re.compile(f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "#         # Create the new folder path for ZIP files\n",
    "#         new_zip_folder_path = os.path.join(download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "#         # Create the new folder path for CSV files\n",
    "#         print(output_dir)\n",
    "#         new_csv_folder_path = os.path.join(output_dir, f\"{symbol}-{chart_time}\")\n",
    "#         df_final = None\n",
    "\n",
    "#         # Iterate over the files in the new zip folder\n",
    "#         for file in os.listdir(new_zip_folder_path):\n",
    "#             # Check if the file matches the pattern\n",
    "#             if pattern.match(file):\n",
    "#                 # Construct the file path\n",
    "#                 file_path = os.path.join(new_zip_folder_path, file)\n",
    "\n",
    "#                 # Extract the ZIP file\n",
    "#                 with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "#                     zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "#                 # Construct the CSV file path\n",
    "#                 csv_file_path = os.path.join(new_csv_folder_path, f\"{symbol}-{chart_time}{file[-12:-4]}.csv\")\n",
    "\n",
    "#                 # Read the CSV file into a Spark DataFrame\n",
    "#                 df = spark.read.csv(csv_file_path, header=True)\n",
    "                \n",
    "# #                 df.show()\n",
    "\n",
    "# #                 # Filter out the header row\n",
    "# #                 df = df.filter(df['open_time'] != 'open_time')\n",
    "        \n",
    "#                 if df_final is None:\n",
    "#                     df_final = df\n",
    "#                 else:\n",
    "#                     df_final = df_final.union(df)\n",
    "#                 # Add it to the list\n",
    "#                 df_list.append(df)\n",
    "\n",
    "#         df_final.show()\n",
    "#         # Convert 'open_time' and 'close_time' columns to timestamp\n",
    "#         df_final = df_final.withColumn(\"open_time\", df_final[\"open_time\"].cast(\"timestamp\"))\n",
    "#         df_final = df_final.withColumn(\"close_time\", df_final[\"close_time\"].cast(\"timestamp\"))\n",
    "\n",
    "#         # Drop the 'ignore' column\n",
    "#         df_final = df_final.drop(\"ignore\")\n",
    "\n",
    "#         # Your existing code to create the df_final DataFrame\n",
    "#         windowSpec = Window.orderBy(\"open_time\")\n",
    "#         df_final = df_final.withColumn(\"entry\", lag(\"close\").over(windowSpec))\n",
    "        \n",
    "#         df_final.show()\n",
    "#         # Convert PySpark DataFrame to Pandas DataFrame\n",
    "#         df_final_pandas = df_final.toPandas()\n",
    "        \n",
    "#         spark.stop()\n",
    "\n",
    "# #         # Set the file name\n",
    "# #         file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "# #         # Construct the file path\n",
    "# #         file_path = os.path.join(new_csv_folder_path, file_name)\n",
    "# #         print(file_path)\n",
    "\n",
    "# #         # Write the Pandas DataFrame to a CSV file\n",
    "# #         df_final_pandas.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d026ee3",
   "metadata": {},
   "source": [
    "## download data functions, process zip, concatenate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b360c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T11:42:24.163646Z",
     "start_time": "2024-01-15T11:42:24.092792Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_monthly_data(month_array, symbol, chart_time):\n",
    "    #downloading monthly data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(\n",
    "        download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for month in month_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print (f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print (f\"you're already up to date for monthly data for {symbol},{chart_time}\")\n",
    "    \n",
    "\n",
    "    \n",
    "def download_daily_data(day_array, symbol, chart_time):\n",
    "    #downloading daily data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(\n",
    "        download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for day in day_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print(f\"you're already up to date for daily data for {symbol},{chart_time}\")\n",
    "        \n",
    "def construct_csv_file_path(folder_path, symbol, chart_time, file, is_daily=False):\n",
    "    if is_daily:\n",
    "        # For daily data, use a different pattern\n",
    "        return os.path.join(\n",
    "            folder_path,\n",
    "            f\"{symbol}-{chart_time}-{file.split('-')[-3]}-{file.split('-')[-2]}-{file.split('-')[-1][:-4]}.csv\"\n",
    "        )\n",
    "    else:\n",
    "        # For monthly data, use the original pattern\n",
    "        return os.path.join(\n",
    "            folder_path,\n",
    "            f\"{symbol}-{chart_time}{file[-12:-4]}.csv\"\n",
    "        )\n",
    "\n",
    "def process_zip_folder(folder_path, pattern, new_csv_folder_path, symbol, chart_time, df_list, is_daily=False):\n",
    "    for file in os.listdir(folder_path):\n",
    "        # Check if the file matches the pattern\n",
    "        if pattern.match(file):\n",
    "            # Construct the file path\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            # Extract the ZIP file\n",
    "            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "            # Construct the CSV file path using the helper function\n",
    "            csv_file_path = construct_csv_file_path(\n",
    "                new_csv_folder_path, symbol, chart_time, file, is_daily)\n",
    "\n",
    "            # Read the CSV file into a data frame, ignoring the headers\n",
    "            df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "            # Remove the first row (which contains the header)\n",
    "            df = df.iloc[1:]\n",
    "\n",
    "            # Add it to the list\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list\n",
    "\n",
    "\n",
    "\n",
    "def concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time):\n",
    "    # Concatenate the data frames in the list\n",
    "    df_final = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    csv_file_path = os.path.join(new_csv_folder_path, os.listdir(new_csv_folder_path)[0])\n",
    "\n",
    "    # Read the headers from the first CSV file\n",
    "    headers = pd.read_csv(csv_file_path, nrows=1).columns\n",
    "    \n",
    "    # Set the headers as the column names of the final dataframe\n",
    "    df_final.columns = headers\n",
    "\n",
    "    # Convert 'open_time' and 'close_time' columns to datetime\n",
    "    df_final['open_time'] = pd.to_datetime(\n",
    "        df_final['open_time'],\n",
    "        unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "    df_final['close_time'] = pd.to_datetime(\n",
    "        df_final['close_time'],\n",
    "        unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "    # Delete the 'ignore' column\n",
    "    df_final = df_final.drop(['ignore'], axis=1)\n",
    "\n",
    "    # Add a new column called 'entry' that will take previous close\n",
    "    df_final['entry'] = df_final['close'].shift(1)\n",
    "\n",
    "    # Set the file name\n",
    "    concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "    # Construct the file path\n",
    "    concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                          concatenated_file_name)\n",
    "\n",
    "    # Write the data frame to the CSV file\n",
    "    df_final.to_csv(concatenated_file_path, index=False)\n",
    "\n",
    "    directory_final = Path(\n",
    "        concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "    # Deleting all the other CSVs\n",
    "    for file_path in directory_final.iterdir():\n",
    "        if file_path != Path(concatenated_file_path):\n",
    "            if file_path.is_file():\n",
    "                file_path.unlink()\n",
    "    return (\"data concatenated, individual csvs deleted\")\n",
    "\n",
    "\n",
    "def calculate_indicators_using_talib(timeperiods, df):\n",
    "    new_columns = pd.DataFrame()\n",
    "\n",
    "    # List to store indicator columns\n",
    "    indicator_columns = []\n",
    "    indicator_columns.append(('HT_TRENDLINE', talib.HT_TRENDLINE(df['close'])))\n",
    "    # indicator_columns.append(('MAMA', df['MAMA']), ('FAMA', df['FAMA']))\n",
    "    # indicator_columns.append(('MAVP', df['MAVP']))\n",
    "    indicator_columns.append(('SAR', talib.SAR(df['high'], df['low'], acceleration=0, maximum=0)))\n",
    "    indicator_columns.append(('SAREXT', talib.SAREXT(df['high'], df['low'])))\n",
    "    indicator_columns.append(('T3', talib.T3(df['close'], timeperiod=5, vfactor=0)))\n",
    "    # Momentum Indicators\n",
    "    indicator_columns.append(('APO', talib.APO(df['close'], fastperiod=12, slowperiod=26)))\n",
    "    indicator_columns.append(('BOP', talib.BOP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    macd, macd_signal, macd_hist = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    indicator_columns.append(('MACD', macd))\n",
    "    indicator_columns.append(('MACD_signal', macd_signal))\n",
    "    indicator_columns.append(('MACD_hist', macd_hist))\n",
    "    indicator_columns.append(('PPO', talib.PPO(df['close'], fastperiod=12, slowperiod=26, matype=0)))\n",
    "    indicator_columns.append(('TRIX', talib.TRIX(df['close'])))\n",
    "    indicator_columns.append(('ULTOSC', talib.ULTOSC(df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('WILLR', talib.WILLR(df['high'], df['low'], df['close'])))\n",
    "\n",
    "\n",
    "#     # Not Working ATM\n",
    "#     indicator_columns.append(('STOCH', talib.STOCH(df['high'], df['low'], df['close'])))\n",
    "#     indicator_columns.append(('STOCHF', talib.STOCHF(df['high'], df['low'], df['close'])))\n",
    "#     indicator_columns.append(('STOCHRSI', talib.STOCHRSI(df['close'])))\n",
    "#     indicator_columns.append(('MACDEXT', talib.MACDEXT(df['close'], fastperiod=12, fastmatype=0, slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)))\n",
    "#     indicator_columns.append(('MACDFIX', talib.MACDFIX(df['close'], signalperiod=9)))\n",
    "    \n",
    "    \n",
    "    #########Volume Indicators\n",
    "    indicator_columns.append(('AD', talib.AD(df['high'], df['low'], df['close'], df['volume'])))\n",
    "    indicator_columns.append(('ADOSC', talib.ADOSC(df['high'], df['low'], df['close'], df['volume'], fastperiod=3, slowperiod=10)))\n",
    "    indicator_columns.append(('OBV', talib.OBV(df['close'], df['volume'])))\n",
    "\n",
    "    #########Cycle Indicators\n",
    "    indicator_columns.append(('HT_DCPERIOD', talib.HT_DCPERIOD(df['close'])))\n",
    "    indicator_columns.append(('HT_DCPHASE', talib.HT_DCPHASE(df['close'])))\n",
    "    phasor_inphase, phasor_quadrature = talib.HT_PHASOR(df['close'])\n",
    "    indicator_columns.append(('HT_PHASOR_inphase', phasor_inphase))\n",
    "    indicator_columns.append(('HT_PHASOR_quadrature', phasor_quadrature))\n",
    "    # indicator_columns.append(('HT_SINE', talib.HT_SINE(df['close'])))\n",
    "    indicator_columns.append(('HT_TRENDMODE', talib.HT_TRENDMODE(df['close'])))\n",
    "\n",
    "    #########Price Transform\n",
    "    indicator_columns.append(('AVGPRICE', talib.AVGPRICE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('MEDPRICE', talib.MEDPRICE(df['high'], df['low'])))\n",
    "    indicator_columns.append(('TYPPRICE', talib.TYPPRICE(df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('WCLPRICE', talib.WCLPRICE(df['high'], df['low'], df['close'])))\n",
    "    #########Volatility Indicators\n",
    "    indicator_columns.append(('TRANGE', talib.TRANGE(df['high'], df['low'], df['close'])))\n",
    "    #########Pattern Recognition\n",
    "    indicator_columns.append(('CDL2CROWS', talib.CDL2CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3BLACKCROWS', talib.CDL3BLACKCROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3INSIDE', talib.CDL3INSIDE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3LINESTRIKE', talib.CDL3LINESTRIKE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3OUTSIDE', talib.CDL3OUTSIDE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3STARSINSOUTH', talib.CDL3STARSINSOUTH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3WHITESOLDIERS', talib.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLABANDONEDBABY', talib.CDLABANDONEDBABY(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLADVANCEBLOCK', talib.CDLADVANCEBLOCK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLBELTHOLD', talib.CDLBELTHOLD(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLBREAKAWAY', talib.CDLBREAKAWAY(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCLOSINGMARUBOZU', talib.CDLCLOSINGMARUBOZU(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCONCEALBABYSWALL', talib.CDLCONCEALBABYSWALL(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCOUNTERATTACK', talib.CDLCOUNTERATTACK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDARKCLOUDCOVER', talib.CDLDARKCLOUDCOVER(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLDOJI', talib.CDLDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDOJISTAR', talib.CDLDOJISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDRAGONFLYDOJI', talib.CDLDRAGONFLYDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLENGULFING', talib.CDLENGULFING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLEVENINGDOJISTAR', talib.CDLEVENINGDOJISTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLEVENINGSTAR', talib.CDLEVENINGSTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "    indicator_columns.append(('CDLGAPSIDESIDEWHITE', talib.CDLGAPSIDESIDEWHITE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLGRAVESTONEDOJI', talib.CDLGRAVESTONEDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHAMMER', talib.CDLHAMMER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHANGINGMAN', talib.CDLHANGINGMAN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHARAMI', talib.CDLHARAMI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHARAMICROSS', talib.CDLHARAMICROSS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIGHWAVE', talib.CDLHIGHWAVE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIKKAKE', talib.CDLHIKKAKE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIKKAKEMOD', talib.CDLHIKKAKEMOD(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHOMINGPIGEON', talib.CDLHOMINGPIGEON(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLIDENTICAL3CROWS', talib.CDLIDENTICAL3CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLINNECK', talib.CDLINNECK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLINVERTEDHAMMER', talib.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLKICKING', talib.CDLKICKING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLKICKINGBYLENGTH', talib.CDLKICKINGBYLENGTH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLADDERBOTTOM', talib.CDLLADDERBOTTOM(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLONGLEGGEDDOJI', talib.CDLLONGLEGGEDDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLONGLINE', talib.CDLLONGLINE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMARUBOZU', talib.CDLMARUBOZU(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMATCHINGLOW', talib.CDLMATCHINGLOW(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMATHOLD', talib.CDLMATHOLD(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "    indicator_columns.append(('CDLMORNINGDOJISTAR', talib.CDLMORNINGDOJISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMORNINGSTAR', talib.CDLMORNINGSTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLONNECK', talib.CDLONNECK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLPIERCING', talib.CDLPIERCING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLRICKSHAWMAN', talib.CDLRICKSHAWMAN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLRISEFALL3METHODS', talib.CDLRISEFALL3METHODS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSEPARATINGLINES', talib.CDLSEPARATINGLINES(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSHOOTINGSTAR', talib.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSHORTLINE', talib.CDLSHORTLINE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSPINNINGTOP', talib.CDLSPINNINGTOP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSTALLEDPATTERN', talib.CDLSTALLEDPATTERN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSTICKSANDWICH', talib.CDLSTICKSANDWICH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTAKURI', talib.CDLTAKURI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTASUKIGAP', talib.CDLTASUKIGAP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTHRUSTING', talib.CDLTHRUSTING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTRISTAR', talib.CDLTRISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLUNIQUE3RIVER', talib.CDLUNIQUE3RIVER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLUPSIDEGAP2CROWS', talib.CDLUPSIDEGAP2CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLXSIDEGAP3METHODS', talib.CDLXSIDEGAP3METHODS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    #########Statistic Functions\n",
    "    indicator_columns.append(('LINEARREG', talib.LINEARREG(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_ANGLE', talib.LINEARREG_ANGLE(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_INTERCEPT', talib.LINEARREG_INTERCEPT(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_SLOPE', talib.LINEARREG_SLOPE(df['close'])))\n",
    "    # new_columns['STDDEV'] = df['close'].rolling(timeperiod).std()\n",
    "    indicator_columns.append(('TSF', talib.TSF(df['close'])))\n",
    "    indicator_columns.append(('VAR', talib.VAR(df['close'])))\n",
    "    # Iterate over the time periods\n",
    "    for timeperiod in timeperiods:\n",
    "        #########Overlap Studies\n",
    "        upper_band, middle_band, lower_band = talib.BBANDS(df['close'], timeperiod=timeperiod)\n",
    "        indicator_columns.append((f'BB_upper_{timeperiod}', upper_band))\n",
    "        indicator_columns.append((f'BB_middle_{timeperiod}', middle_band))\n",
    "        indicator_columns.append((f'BB_lower_{timeperiod}', lower_band))\n",
    "        indicator_columns.append((f'DEMA_{timeperiod}', talib.DEMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'EMA_{timeperiod}', talib.EMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'KAMA_{timeperiod}', talib.KAMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MA_{timeperiod}', talib.MA(df['close'], timeperiod=timeperiod)))\n",
    "        # new_columns['MAMA'], new_columns['FAMA'] = talib.MAMA(df['close'], fastlimit=0, slowlimit=0)\n",
    "        # new_columns['MAVP'] = talib.MAVP(df['close'], periods=None, minperiod=2, maxperiod=30, matype=0)\n",
    "        indicator_columns.append((f'MIDPOINT_{timeperiod}', talib.MIDPOINT(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MIDPRICE_{timeperiod}', talib.MIDPRICE(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'SMA_{timeperiod}', talib.SMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'TEMA_{timeperiod}', talib.TEMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'TRIMA_{timeperiod}', talib.TRIMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'WMA_{timeperiod}', talib.WMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ADX_{timeperiod}', talib.ADX(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ADXR_{timeperiod}', talib.ADXR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        aroon_up, aroon_down = talib.AROON(df['high'], df['low'], timeperiod=timeperiod)\n",
    "        indicator_columns.append((f'AROON_up_{timeperiod}', aroon_up))\n",
    "        indicator_columns.append((f'AROON_down_{timeperiod}', aroon_down))\n",
    "        indicator_columns.append((f'AROONOSC_{timeperiod}', talib.AROONOSC(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CCI_{timeperiod}', talib.CCI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CMO_{timeperiod}', talib.CMO(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'DX_{timeperiod}', talib.DX(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MFI_{timeperiod}', talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MINUS_DI_{timeperiod}', talib.MINUS_DI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MINUS_DM_{timeperiod}', talib.MINUS_DM(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MOM_{timeperiod}', talib.MOM(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'PLUS_DI_{timeperiod}', talib.PLUS_DI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'PLUS_DM_{timeperiod}', talib.PLUS_DM(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROC_{timeperiod}', talib.ROC(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCP_{timeperiod}', talib.ROCP(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCR_{timeperiod}', talib.ROCR(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCR100_{timeperiod}', talib.ROCR100(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'RSI_{timeperiod}', talib.RSI(df['close'], timeperiod=timeperiod)))\n",
    "\n",
    "        indicator_columns.append((f'ATR_{timeperiod}', talib.ATR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'NATR_{timeperiod}', talib.NATR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        #########Statistic Functions\n",
    "        indicator_columns.append((f'BETA_{timeperiod}', talib.BETA(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CORREL_{timeperiod}', talib.CORREL(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "    new_columns = pd.concat([pd.DataFrame(data, columns=[name]) for name, data in indicator_columns], axis=1)\n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c7ad0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:11:46.482320Z",
     "start_time": "2024-01-15T10:11:46.468906Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_data_and_concatenate(master_dictionary, month_array, day_array):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"setting up things for {symbol},{chart_time}\")\n",
    "\n",
    "            # Set up an empty list for the data frames\n",
    "            df_list = []\n",
    "\n",
    "            # Compile the regular expression pattern\n",
    "            pattern = re.compile(f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Compile the regular expression pattern for daily zip files\n",
    "            pattern_daily = re.compile(\n",
    "                f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Create the new folder path for daily ZIP files\n",
    "            new_daily_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-daily_data\")\n",
    "\n",
    "            # Create the new folder path for ZIP files\n",
    "            new_monthly_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "            # Create the new folder path for CSV files\n",
    "            new_csv_folder_path = os.path.join(output_dir,\n",
    "                                               f\"{symbol}-{chart_time}\")\n",
    "\n",
    "            # Set the file name\n",
    "            concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "            # Construct the file path\n",
    "            concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                                  concatenated_file_name)\n",
    "\n",
    "            if not Path(concatenated_file_path).exists():\n",
    "                download_monthly_data(month_array, symbol, chart_time)  \n",
    "                download_daily_data(day_array, symbol, chart_time)  \n",
    "\n",
    "                # Process the monthly ZIP folder and add to df_list\n",
    "                df_list = process_zip_folder(new_monthly_zip_folder_path, pattern, new_csv_folder_path, symbol, chart_time, df_list)\n",
    "\n",
    "                # Process the daily ZIP folder and add to df_list\n",
    "                df_list = process_zip_folder(new_daily_zip_folder_path, pattern_daily, new_csv_folder_path, symbol, chart_time, df_list, is_daily=True)\n",
    "\n",
    "                # Call the function to concatenate and process the data frames\n",
    "                print(concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time))\n",
    "#             else:\n",
    "#                 df_list = []\n",
    "#                 df = pd.read_csv(concatenated_file_path, header=None)\n",
    "#                 last_record = df.iloc[-1:]\n",
    "#                 df_list.append(last_record)\n",
    "\n",
    "#                 df = pd.read_csv(concatenated_file_path)\n",
    "#                 df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "#                 last_record = df.iloc[-1:]\n",
    "\n",
    "\n",
    "#                 # Check if the time part of 'open_time' is \"05:15:00+05:30\"\n",
    "#                 if last_record['open_time'].dt.strftime(\n",
    "#                         '%H:%M:%S%z').item() == \"05:15:00+0530\":\n",
    "#                     # Calculate the last processed date by subtracting 1 day from the date part\n",
    "#                     last_processed_date = (\n",
    "#                         last_record['open_time'].dt.date -\n",
    "#                         pd.DateOffset(days=1)).item().strftime('%Y-%m-%d')\n",
    "#                     last_processed_date = datetime.strptime(\n",
    "#                         last_processed_date, \"%Y-%m-%d\")\n",
    "\n",
    "#                     # Get today's date\n",
    "#                     today = datetime.today()\n",
    "\n",
    "#                     # Initialize DAY_ARRAY\n",
    "#                     DAY_ARRAY = []\n",
    "\n",
    "#                     # Start from the day after last_processed_date\n",
    "#                     current_day = last_processed_date + timedelta(days=1)\n",
    "\n",
    "#                     while current_day <= today:\n",
    "#                         # Format the date as a string in the desired format\n",
    "#                         date_string = current_day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#                         # Add the date string to DAY_ARRAY\n",
    "#                         DAY_ARRAY.append(date_string)\n",
    "\n",
    "#                         # Move to the next day\n",
    "#                         current_day += timedelta(days=1)\n",
    "\n",
    "#                     download_daily_data()\n",
    "#                     # Iterate over the files in the new daily zip folder\n",
    "#                     for file in Path(new_daily_zip_folder_path).iterdir():\n",
    "#                         # Check if the file matches the pattern\n",
    "#                         if pattern_daily.match(file.name):\n",
    "#                             # Extract the date part from the file name (e.g., \"2023-08-19\")\n",
    "#                             zip_date = file.name.split('-')[-3:]\n",
    "#                             zip_date_str = '-'.join(zip_date).replace('.zip', '')\n",
    "\n",
    "#                             # Check if the date is in DAY_ARRAY\n",
    "#                             if zip_date_str in DAY_ARRAY:\n",
    "#                                 # Construct the file path\n",
    "#                                 file_path = file\n",
    "\n",
    "#                                 # Extract the ZIP file\n",
    "#                                 with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "#                                     zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "#                                 # Construct the CSV file path\n",
    "#                                 csv_file_path = Path(new_csv_folder_path) / f\"{symbol}-{chart_time}-{zip_date_str}.csv\"\n",
    "\n",
    "#                                 # Read the CSV file into a data frame, ignoring the headers\n",
    "#                                 df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "#                                 # Remove the first row (which contains the header)\n",
    "#                                 df = df.iloc[1:]\n",
    "\n",
    "#                                 # Add it to the list\n",
    "#                                 df_list.append(df)\n",
    "#                     # Concatenate the data frames in the list\n",
    "#                     df_final = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "#                     # Read the headers from the first CSV file\n",
    "#                     headers = pd.read_csv(concatenated_file_path, nrows=1).columns\n",
    "\n",
    "#                     # Set the headers as the column names of the final dataframe\n",
    "#                     df_final.columns = headers\n",
    "\n",
    "#                     # Convert 'open_time' and 'close_time' columns to datetime\n",
    "#                     unix_time_format = df_final['open_time'].str.contains(r'^\\d{13}$')\n",
    "#                     df_final.loc[unix_time_format, 'open_time'] = pd.to_datetime(\n",
    "#                         df_final.loc[unix_time_format, 'open_time'],\n",
    "#                         unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "#                     unix_time_format = df_final['close_time'].str.contains(r'^\\d{13}$')\n",
    "#                     df_final.loc[unix_time_format, 'close_time'] = pd.to_datetime(\n",
    "#                         df_final.loc[unix_time_format, 'close_time'],\n",
    "#                         unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "\n",
    "#                     # Add a new column called 'entry' that will take previous close\n",
    "#                     df_final['entry'] = df_final['close'].shift(1)\n",
    "#                     # Append the data frame to the existing CSV file in append mode without headers\n",
    "#                     df_final[1:].to_csv(concatenated_file_path, mode='a', header=False, index=False)\n",
    "#                     directory_final = Path(\n",
    "#                     concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "#                     # deleting all the other csvs\n",
    "#                     for file_path in directory_final.iterdir():\n",
    "#                         if file_path != Path(concatenated_file_path):\n",
    "#                             if file_path.is_file():\n",
    "#                                 file_path.unlink()\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     print(\"Something is wrong with data\")\n",
    "    return (\"data downloaded and concatenated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9d97d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:16:03.585856Z",
     "start_time": "2024-01-15T10:15:27.610207Z"
    }
   },
   "outputs": [],
   "source": [
    "download_data_and_concatenate(master_dictionary, MONTH_ARRAY, DAY_ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba603fd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:19:39.341715Z",
     "start_time": "2024-01-15T10:19:39.326821Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wins_losses(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"calculating for {symbol}{chart_time}\")\n",
    "            # Construct the file name\n",
    "            og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "            og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "            new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            new_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{new_file_name}\"\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(og_file_path)\n",
    "            df[\"if_short\"] = 0\n",
    "            df[\"if_long\"] = 0\n",
    "            df[\"long_target\"] = np.nan\n",
    "            df[\"short_target\"] = np.nan\n",
    "            df[\"long_stop_loss\"] = np.nan\n",
    "            df[\"short_stop_loss\"] = np.nan\n",
    "            df[\"shorts_win_after\"] = np.nan\n",
    "            df[\"longs_win_after\"] = np.nan\n",
    "            df[\"dual_loss\"] = 0\n",
    "            df[\"entered_before\"] = np.nan\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                if df[\"entry\"][i]:\n",
    "                    long_target = df[\"entry\"][i] * (1 + win_perc / 100)\n",
    "                    short_target = df[\"entry\"][i] * (1 - win_perc / 100)\n",
    "                    long_stop_loss = df[\"entry\"][i] * (1 - loss_perc / 100)\n",
    "                    short_stop_loss = df[\"entry\"][i] * (1 + loss_perc / 100)\n",
    "                    df.loc[i, 'long_target'] = long_target\n",
    "                    df.loc[i, 'long_stop_loss'] = long_stop_loss\n",
    "                    for j in range(i, len(df)):\n",
    "                        if df[\"high\"][j] >= long_target:\n",
    "                            if df[\"low\"][j] <= long_stop_loss:\n",
    "                                df.loc[i, 'if_long'] = -1\n",
    "                                df.loc[i, 'dual_loss'] = 1\n",
    "                                df.loc[i, 'entered_before'] = j - i\n",
    "                            else:\n",
    "                                df.loc[i, 'if_long'] = 1\n",
    "                                df.loc[i, 'longs_win_after'] = j - i\n",
    "                            break\n",
    "                        elif df[\"low\"][j] <= long_stop_loss:\n",
    "                            df.loc[i, 'if_long'] = -1\n",
    "                            break\n",
    "                    df.loc[i, 'short_target'] = short_target\n",
    "                    df.loc[i, 'short_stop_loss'] = short_stop_loss\n",
    "                    for j in range(i, len(df)):\n",
    "                        if df[\"low\"][j] <= short_target:\n",
    "                            if df[\"high\"][j] >= short_stop_loss:\n",
    "                                df.loc[i, 'if_short'] = -1\n",
    "                                df.loc[i, 'dual_loss'] = 1\n",
    "                                df.loc[i, 'entered_before'] = j - i\n",
    "                            else:\n",
    "                                df.loc[i, 'if_short'] = 1\n",
    "                                df.loc[i, 'shorts_win_after'] = j - i\n",
    "                            break\n",
    "                        elif df[\"high\"][j] >= short_stop_loss:\n",
    "                            df.loc[i, 'if_short'] = -1\n",
    "                            break\n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df.to_csv(new_file_path, index=False)\n",
    "\n",
    "    return (\"calculated wins and losses \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a7857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:26:36.250423Z",
     "start_time": "2024-01-15T10:26:36.140515Z"
    }
   },
   "outputs": [],
   "source": [
    "calculate_wins_losses(master_dictionary, win_perc=20, loss_perc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110c233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T11:39:31.339511Z",
     "start_time": "2024-01-15T11:39:31.328923Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_indicator_values(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            # Construct the file name\n",
    "            file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{file_name}\"\n",
    "            if not Path(file_path).exists():\n",
    "                print(f\"File path for {file_name} doesn't exist. Breaking.\")\n",
    "                break\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(df.dtypes)\n",
    "            new_columns = calculate_indicators_using_talib(master_dictionary, df)\n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df = pd.concat([df, new_columns], axis=1)\n",
    "            df.to_csv(file_path, index=False)\n",
    "    return (\"indicators are added to the csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e5767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T11:42:29.056020Z",
     "start_time": "2024-01-15T11:42:28.924183Z"
    }
   },
   "outputs": [],
   "source": [
    "calculate_indicator_values_new(master_dictionary, win_perc=20, loss_perc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890635b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e264912d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:16:24.700843Z",
     "start_time": "2024-01-16T12:16:24.681654Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data_between_dates(master_dictionary, start_date, end_date, data_path):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"displaying filtered for {symbol}-{chart_time}\")\n",
    "            display(filter_data_for_symbol_charttime(symbol, chart_time, start_date, end_date, data_path))\n",
    "\n",
    "def filter_data_for_symbol_charttime(symbol, chart_time, start_date, end_date, data_path):\n",
    "    # Read the CSV file into a data frame\n",
    "    df = pd.read_csv(Path(data_path) / f\"{symbol}-{chart_time}\" / f\"{symbol}-{chart_time}.csv\")\n",
    "    \n",
    "    # Convert 'open_time' to datetime format\n",
    "    df['open_time'] = pd.to_datetime(df['open_time']).dt.date\n",
    "    \n",
    "    # Convert start_date and end_date to datetime objects\n",
    "    start_date = pd.to_datetime(start_date, format='%Y%m%d').date()\n",
    "    end_date = pd.to_datetime(end_date, format='%Y%m%d').date()\n",
    "\n",
    "    # Filter data between start_date and end_date based on the date part of 'open_time'\n",
    "    df_filtered = df[(df['open_time'] >= start_date) & (df['open_time'] <= end_date)]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "#sample function call: read_data_between_dates(master_dictionary, \"20231206\", \"20231206\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13f01cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:16:24.997126Z",
     "start_time": "2024-01-16T12:16:24.961588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying filtered for PYTHUSDT-4h\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_time</th>\n",
       "      <th>quote_volume</th>\n",
       "      <th>count</th>\n",
       "      <th>taker_buy_volume</th>\n",
       "      <th>taker_buy_quote_volume</th>\n",
       "      <th>entry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4576</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>0.4325</td>\n",
       "      <td>0.4373</td>\n",
       "      <td>53522185</td>\n",
       "      <td>2023-12-06 05:29:59.999000+05:30</td>\n",
       "      <td>2.408112e+07</td>\n",
       "      <td>169828</td>\n",
       "      <td>24521577</td>\n",
       "      <td>1.105276e+07</td>\n",
       "      <td>0.4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4373</td>\n",
       "      <td>0.4817</td>\n",
       "      <td>0.4372</td>\n",
       "      <td>0.4579</td>\n",
       "      <td>74705918</td>\n",
       "      <td>2023-12-06 09:29:59.999000+05:30</td>\n",
       "      <td>3.422330e+07</td>\n",
       "      <td>267004</td>\n",
       "      <td>34673012</td>\n",
       "      <td>1.588850e+07</td>\n",
       "      <td>0.4373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>174882197</td>\n",
       "      <td>2023-12-06 13:29:59.999000+05:30</td>\n",
       "      <td>8.381688e+07</td>\n",
       "      <td>536522</td>\n",
       "      <td>82566229</td>\n",
       "      <td>3.961470e+07</td>\n",
       "      <td>0.4579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4616</td>\n",
       "      <td>0.4969</td>\n",
       "      <td>0.4395</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>133680747</td>\n",
       "      <td>2023-12-06 17:29:59.999000+05:30</td>\n",
       "      <td>6.247885e+07</td>\n",
       "      <td>402503</td>\n",
       "      <td>61170688</td>\n",
       "      <td>2.861713e+07</td>\n",
       "      <td>0.4615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.4691</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.4554</td>\n",
       "      <td>81093191</td>\n",
       "      <td>2023-12-06 21:29:59.999000+05:30</td>\n",
       "      <td>3.665976e+07</td>\n",
       "      <td>252901</td>\n",
       "      <td>37430528</td>\n",
       "      <td>1.693037e+07</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4555</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>0.4509</td>\n",
       "      <td>59226842</td>\n",
       "      <td>2023-12-07 01:29:59.999000+05:30</td>\n",
       "      <td>2.694526e+07</td>\n",
       "      <td>177402</td>\n",
       "      <td>28067560</td>\n",
       "      <td>1.277794e+07</td>\n",
       "      <td>0.4554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     open_time    open    high     low   close     volume  \\\n",
       "80  2023-12-06  0.4576  0.4668  0.4325  0.4373   53522185   \n",
       "81  2023-12-06  0.4373  0.4817  0.4372  0.4579   74705918   \n",
       "82  2023-12-06  0.4580  0.5000  0.4525  0.4615  174882197   \n",
       "83  2023-12-06  0.4616  0.4969  0.4395  0.4590  133680747   \n",
       "84  2023-12-06  0.4590  0.4691  0.4352  0.4554   81093191   \n",
       "85  2023-12-06  0.4555  0.4636  0.4473  0.4509   59226842   \n",
       "\n",
       "                          close_time  quote_volume   count  taker_buy_volume  \\\n",
       "80  2023-12-06 05:29:59.999000+05:30  2.408112e+07  169828          24521577   \n",
       "81  2023-12-06 09:29:59.999000+05:30  3.422330e+07  267004          34673012   \n",
       "82  2023-12-06 13:29:59.999000+05:30  8.381688e+07  536522          82566229   \n",
       "83  2023-12-06 17:29:59.999000+05:30  6.247885e+07  402503          61170688   \n",
       "84  2023-12-06 21:29:59.999000+05:30  3.665976e+07  252901          37430528   \n",
       "85  2023-12-07 01:29:59.999000+05:30  2.694526e+07  177402          28067560   \n",
       "\n",
       "    taker_buy_quote_volume   entry  \n",
       "80            1.105276e+07  0.4577  \n",
       "81            1.588850e+07  0.4373  \n",
       "82            3.961470e+07  0.4579  \n",
       "83            2.861713e+07  0.4615  \n",
       "84            1.693037e+07  0.4590  \n",
       "85            1.277794e+07  0.4554  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "read_data_between_dates(master_dictionary, \"20231206\", \"20231206\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96178cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:16:25.353576Z",
     "start_time": "2024-01-16T12:16:25.332123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_time</th>\n",
       "      <th>quote_volume</th>\n",
       "      <th>count</th>\n",
       "      <th>taker_buy_volume</th>\n",
       "      <th>taker_buy_quote_volume</th>\n",
       "      <th>entry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4576</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>0.4325</td>\n",
       "      <td>0.4373</td>\n",
       "      <td>53522185</td>\n",
       "      <td>2023-12-06 05:29:59.999000+05:30</td>\n",
       "      <td>2.408112e+07</td>\n",
       "      <td>169828</td>\n",
       "      <td>24521577</td>\n",
       "      <td>1.105276e+07</td>\n",
       "      <td>0.4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4373</td>\n",
       "      <td>0.4817</td>\n",
       "      <td>0.4372</td>\n",
       "      <td>0.4579</td>\n",
       "      <td>74705918</td>\n",
       "      <td>2023-12-06 09:29:59.999000+05:30</td>\n",
       "      <td>3.422330e+07</td>\n",
       "      <td>267004</td>\n",
       "      <td>34673012</td>\n",
       "      <td>1.588850e+07</td>\n",
       "      <td>0.4373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>174882197</td>\n",
       "      <td>2023-12-06 13:29:59.999000+05:30</td>\n",
       "      <td>8.381688e+07</td>\n",
       "      <td>536522</td>\n",
       "      <td>82566229</td>\n",
       "      <td>3.961470e+07</td>\n",
       "      <td>0.4579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4616</td>\n",
       "      <td>0.4969</td>\n",
       "      <td>0.4395</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>133680747</td>\n",
       "      <td>2023-12-06 17:29:59.999000+05:30</td>\n",
       "      <td>6.247885e+07</td>\n",
       "      <td>402503</td>\n",
       "      <td>61170688</td>\n",
       "      <td>2.861713e+07</td>\n",
       "      <td>0.4615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.4691</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.4554</td>\n",
       "      <td>81093191</td>\n",
       "      <td>2023-12-06 21:29:59.999000+05:30</td>\n",
       "      <td>3.665976e+07</td>\n",
       "      <td>252901</td>\n",
       "      <td>37430528</td>\n",
       "      <td>1.693037e+07</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>0.4555</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>0.4509</td>\n",
       "      <td>59226842</td>\n",
       "      <td>2023-12-07 01:29:59.999000+05:30</td>\n",
       "      <td>2.694526e+07</td>\n",
       "      <td>177402</td>\n",
       "      <td>28067560</td>\n",
       "      <td>1.277794e+07</td>\n",
       "      <td>0.4554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>0.4509</td>\n",
       "      <td>0.4558</td>\n",
       "      <td>0.4203</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>51531786</td>\n",
       "      <td>2023-12-07 05:29:59.999000+05:30</td>\n",
       "      <td>2.242190e+07</td>\n",
       "      <td>144364</td>\n",
       "      <td>21197489</td>\n",
       "      <td>9.236536e+06</td>\n",
       "      <td>0.4509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.4201</td>\n",
       "      <td>0.4284</td>\n",
       "      <td>36571814</td>\n",
       "      <td>2023-12-07 09:29:59.999000+05:30</td>\n",
       "      <td>1.567046e+07</td>\n",
       "      <td>107843</td>\n",
       "      <td>17162481</td>\n",
       "      <td>7.361426e+06</td>\n",
       "      <td>0.4258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>0.4284</td>\n",
       "      <td>0.4384</td>\n",
       "      <td>0.4254</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>40505828</td>\n",
       "      <td>2023-12-07 13:29:59.999000+05:30</td>\n",
       "      <td>1.752647e+07</td>\n",
       "      <td>107728</td>\n",
       "      <td>19381834</td>\n",
       "      <td>8.394310e+06</td>\n",
       "      <td>0.4284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>0.4376</td>\n",
       "      <td>0.4505</td>\n",
       "      <td>0.4102</td>\n",
       "      <td>0.4485</td>\n",
       "      <td>85589079</td>\n",
       "      <td>2023-12-07 17:29:59.999000+05:30</td>\n",
       "      <td>3.685370e+07</td>\n",
       "      <td>242615</td>\n",
       "      <td>39119935</td>\n",
       "      <td>1.688575e+07</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>0.4484</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>0.4282</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>62782469</td>\n",
       "      <td>2023-12-07 21:29:59.999000+05:30</td>\n",
       "      <td>2.761607e+07</td>\n",
       "      <td>166393</td>\n",
       "      <td>29674149</td>\n",
       "      <td>1.306299e+07</td>\n",
       "      <td>0.4485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>0.4547</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.4302</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>46106907</td>\n",
       "      <td>2023-12-08 01:29:59.999000+05:30</td>\n",
       "      <td>2.034151e+07</td>\n",
       "      <td>139163</td>\n",
       "      <td>20260642</td>\n",
       "      <td>8.944730e+06</td>\n",
       "      <td>0.4545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     open_time    open    high     low   close     volume  \\\n",
       "80  2023-12-06  0.4576  0.4668  0.4325  0.4373   53522185   \n",
       "81  2023-12-06  0.4373  0.4817  0.4372  0.4579   74705918   \n",
       "82  2023-12-06  0.4580  0.5000  0.4525  0.4615  174882197   \n",
       "83  2023-12-06  0.4616  0.4969  0.4395  0.4590  133680747   \n",
       "84  2023-12-06  0.4590  0.4691  0.4352  0.4554   81093191   \n",
       "85  2023-12-06  0.4555  0.4636  0.4473  0.4509   59226842   \n",
       "86  2023-12-07  0.4509  0.4558  0.4203  0.4258   51531786   \n",
       "87  2023-12-07  0.4259  0.4350  0.4201  0.4284   36571814   \n",
       "88  2023-12-07  0.4284  0.4384  0.4254  0.4375   40505828   \n",
       "89  2023-12-07  0.4376  0.4505  0.4102  0.4485   85589079   \n",
       "90  2023-12-07  0.4484  0.4550  0.4282  0.4545   62782469   \n",
       "91  2023-12-07  0.4547  0.4590  0.4302  0.4329   46106907   \n",
       "\n",
       "                          close_time  quote_volume   count  taker_buy_volume  \\\n",
       "80  2023-12-06 05:29:59.999000+05:30  2.408112e+07  169828          24521577   \n",
       "81  2023-12-06 09:29:59.999000+05:30  3.422330e+07  267004          34673012   \n",
       "82  2023-12-06 13:29:59.999000+05:30  8.381688e+07  536522          82566229   \n",
       "83  2023-12-06 17:29:59.999000+05:30  6.247885e+07  402503          61170688   \n",
       "84  2023-12-06 21:29:59.999000+05:30  3.665976e+07  252901          37430528   \n",
       "85  2023-12-07 01:29:59.999000+05:30  2.694526e+07  177402          28067560   \n",
       "86  2023-12-07 05:29:59.999000+05:30  2.242190e+07  144364          21197489   \n",
       "87  2023-12-07 09:29:59.999000+05:30  1.567046e+07  107843          17162481   \n",
       "88  2023-12-07 13:29:59.999000+05:30  1.752647e+07  107728          19381834   \n",
       "89  2023-12-07 17:29:59.999000+05:30  3.685370e+07  242615          39119935   \n",
       "90  2023-12-07 21:29:59.999000+05:30  2.761607e+07  166393          29674149   \n",
       "91  2023-12-08 01:29:59.999000+05:30  2.034151e+07  139163          20260642   \n",
       "\n",
       "    taker_buy_quote_volume   entry  \n",
       "80            1.105276e+07  0.4577  \n",
       "81            1.588850e+07  0.4373  \n",
       "82            3.961470e+07  0.4579  \n",
       "83            2.861713e+07  0.4615  \n",
       "84            1.693037e+07  0.4590  \n",
       "85            1.277794e+07  0.4554  \n",
       "86            9.236536e+06  0.4509  \n",
       "87            7.361426e+06  0.4258  \n",
       "88            8.394310e+06  0.4284  \n",
       "89            1.688575e+07  0.4375  \n",
       "90            1.306299e+07  0.4485  \n",
       "91            8.944730e+06  0.4545  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_date = \"2023-12-06\"\n",
    "end_date = \"2023-12-07\"\n",
    "for symbol in master_dictionary[\"symbols\"]:\n",
    "    for chart_time in master_dictionary[\"chart_times\"]:\n",
    "        df = pd.read_csv(Path(output_dir) / f\"{symbol}-{chart_time}\" / f\"{symbol}-{chart_time}.csv\")\n",
    "#         display(df)\n",
    "        df['open_time'] = pd.to_datetime(df['open_time']).dt.date\n",
    "        df_filtered = df[(df['open_time'] >= pd.to_datetime(start_date).date()) & (df['open_time'] <= pd.to_datetime(end_date).date())]\n",
    "        display(df_filtered)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cc17489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:26:32.710890Z",
     "start_time": "2024-01-16T12:26:32.690392Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wins_losses_for_dataframe(df, win_perc, loss_perc):\n",
    "    df[\"if_short\"] = 0\n",
    "    df[\"if_long\"] = 0\n",
    "    df[\"long_target\"] = np.nan\n",
    "    df[\"short_target\"] = np.nan\n",
    "    df[\"long_stop_loss\"] = np.nan\n",
    "    df[\"short_stop_loss\"] = np.nan\n",
    "    df[\"shorts_win_after\"] = np.nan\n",
    "    df[\"longs_win_after\"] = np.nan\n",
    "    df[\"dual_loss\"] = 0\n",
    "    df[\"entered_before\"] = np.nan\n",
    "\n",
    "    for i in df.index:\n",
    "        if df[\"entry\"][i]:\n",
    "            long_target = df[\"entry\"][i] * (1 + win_perc / 100)\n",
    "            short_target = df[\"entry\"][i] * (1 - win_perc / 100)\n",
    "            long_stop_loss = df[\"entry\"][i] * (1 - loss_perc / 100)\n",
    "            short_stop_loss = df[\"entry\"][i] * (1 + loss_perc / 100)\n",
    "            df.loc[i, 'long_target'] = long_target\n",
    "            df.loc[i, 'long_stop_loss'] = long_stop_loss\n",
    "            for j in range(i, len(df)):\n",
    "                if df[\"high\"][j] >= long_target:\n",
    "                    if df[\"low\"][j] <= long_stop_loss:\n",
    "                        df.loc[i, 'if_long'] = -1\n",
    "                        df.loc[i, 'dual_loss'] = 1\n",
    "                        df.loc[i, 'entered_before'] = j - i\n",
    "                    else:\n",
    "                        df.loc[i, 'if_long'] = 1\n",
    "                        df.loc[i, 'longs_win_after'] = j - i\n",
    "                    break\n",
    "                elif df[\"low\"][j] <= long_stop_loss:\n",
    "                    df.loc[i, 'if_long'] = -1\n",
    "                    break\n",
    "            df.loc[i, 'short_target'] = short_target\n",
    "            df.loc[i, 'short_stop_loss'] = short_stop_loss\n",
    "            for j in range(i, len(df)):\n",
    "                if df[\"low\"][j] <= short_target:\n",
    "                    if df[\"high\"][j] >= short_stop_loss:\n",
    "                        df.loc[i, 'if_short'] = -1\n",
    "                        df.loc[i, 'dual_loss'] = 1\n",
    "                        df.loc[i, 'entered_before'] = j - i\n",
    "                    else:\n",
    "                        df.loc[i, 'if_short'] = 1\n",
    "                        df.loc[i, 'shorts_win_after'] = j - i\n",
    "                    break\n",
    "                elif df[\"high\"][j] >= short_stop_loss:\n",
    "                    df.loc[i, 'if_short'] = -1\n",
    "                    break\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_wins_losses(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"calculating for {symbol}{chart_time}\")\n",
    "            # Construct the file name\n",
    "            og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "            og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "            new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            new_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{new_file_name}\"\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(og_file_path)\n",
    "            # Calculate wins and losses using the function\n",
    "            df_calculated = calculate_wins_losses_for_dataframe(df, win_perc, loss_perc)\n",
    "            # Save the updated dataframe to the new CSV file\n",
    "            df_calculated.to_csv(new_file_path, index=False)\n",
    "\n",
    "    return (\"calculated wins and losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e078ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T12:28:33.157668Z",
     "start_time": "2024-01-16T12:28:32.877418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_time</th>\n",
       "      <th>quote_volume</th>\n",
       "      <th>count</th>\n",
       "      <th>taker_buy_volume</th>\n",
       "      <th>...</th>\n",
       "      <th>PLUS_DM_8</th>\n",
       "      <th>ROC_8</th>\n",
       "      <th>ROCP_8</th>\n",
       "      <th>ROCR_8</th>\n",
       "      <th>ROCR100_8</th>\n",
       "      <th>RSI_8</th>\n",
       "      <th>ATR_8</th>\n",
       "      <th>NATR_8</th>\n",
       "      <th>BETA_8</th>\n",
       "      <th>CORREL_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>0.4189</td>\n",
       "      <td>0.3935</td>\n",
       "      <td>0.4154</td>\n",
       "      <td>24819063.0</td>\n",
       "      <td>2023-11-28 05:29:59.999000+05:30</td>\n",
       "      <td>1.009918e+07</td>\n",
       "      <td>79568.0</td>\n",
       "      <td>11839833.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>0.4155</td>\n",
       "      <td>0.4360</td>\n",
       "      <td>0.4117</td>\n",
       "      <td>0.4202</td>\n",
       "      <td>35057435.0</td>\n",
       "      <td>2023-11-28 09:29:59.999000+05:30</td>\n",
       "      <td>1.490047e+07</td>\n",
       "      <td>123631.0</td>\n",
       "      <td>15673550.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>0.4031</td>\n",
       "      <td>0.4114</td>\n",
       "      <td>35363966.0</td>\n",
       "      <td>2023-11-28 13:29:59.999000+05:30</td>\n",
       "      <td>1.459483e+07</td>\n",
       "      <td>125797.0</td>\n",
       "      <td>16160899.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>0.4114</td>\n",
       "      <td>0.4446</td>\n",
       "      <td>0.4045</td>\n",
       "      <td>0.4437</td>\n",
       "      <td>46709097.0</td>\n",
       "      <td>2023-11-28 17:29:59.999000+05:30</td>\n",
       "      <td>1.993299e+07</td>\n",
       "      <td>147066.0</td>\n",
       "      <td>22172027.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>0.4440</td>\n",
       "      <td>0.4498</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.4306</td>\n",
       "      <td>61217835.0</td>\n",
       "      <td>2023-11-28 21:29:59.999000+05:30</td>\n",
       "      <td>2.682785e+07</td>\n",
       "      <td>193480.0</td>\n",
       "      <td>27052015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>0.3485</td>\n",
       "      <td>0.3604</td>\n",
       "      <td>0.3417</td>\n",
       "      <td>0.3555</td>\n",
       "      <td>84277352.0</td>\n",
       "      <td>2024-01-14 21:29:59.999000+05:30</td>\n",
       "      <td>2.958133e+07</td>\n",
       "      <td>112481.0</td>\n",
       "      <td>41707776.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062508</td>\n",
       "      <td>23.781337</td>\n",
       "      <td>0.237813</td>\n",
       "      <td>1.237813</td>\n",
       "      <td>123.781337</td>\n",
       "      <td>77.846939</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>6.313905</td>\n",
       "      <td>0.197285</td>\n",
       "      <td>0.887472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>0.3555</td>\n",
       "      <td>0.3639</td>\n",
       "      <td>0.3458</td>\n",
       "      <td>0.3511</td>\n",
       "      <td>84120519.0</td>\n",
       "      <td>2024-01-15 01:29:59.999000+05:30</td>\n",
       "      <td>2.991980e+07</td>\n",
       "      <td>130467.0</td>\n",
       "      <td>41688617.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058194</td>\n",
       "      <td>25.752149</td>\n",
       "      <td>0.257521</td>\n",
       "      <td>1.257521</td>\n",
       "      <td>125.752149</td>\n",
       "      <td>73.791244</td>\n",
       "      <td>0.021903</td>\n",
       "      <td>6.238305</td>\n",
       "      <td>0.178541</td>\n",
       "      <td>0.886021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      open_time    open    high     low   close      volume  \\\n",
       "32   2023-11-28  0.3960  0.4189  0.3935  0.4154  24819063.0   \n",
       "33   2023-11-28  0.4155  0.4360  0.4117  0.4202  35057435.0   \n",
       "34   2023-11-28  0.4199  0.4238  0.4031  0.4114  35363966.0   \n",
       "35   2023-11-28  0.4114  0.4446  0.4045  0.4437  46709097.0   \n",
       "36   2023-11-28  0.4440  0.4498  0.4260  0.4306  61217835.0   \n",
       "..          ...     ...     ...     ...     ...         ...   \n",
       "318  2024-01-14  0.3485  0.3604  0.3417  0.3555  84277352.0   \n",
       "319  2024-01-14  0.3555  0.3639  0.3458  0.3511  84120519.0   \n",
       "0           NaN     NaN     NaN     NaN     NaN         NaN   \n",
       "1           NaN     NaN     NaN     NaN     NaN         NaN   \n",
       "2           NaN     NaN     NaN     NaN     NaN         NaN   \n",
       "\n",
       "                           close_time  quote_volume     count  \\\n",
       "32   2023-11-28 05:29:59.999000+05:30  1.009918e+07   79568.0   \n",
       "33   2023-11-28 09:29:59.999000+05:30  1.490047e+07  123631.0   \n",
       "34   2023-11-28 13:29:59.999000+05:30  1.459483e+07  125797.0   \n",
       "35   2023-11-28 17:29:59.999000+05:30  1.993299e+07  147066.0   \n",
       "36   2023-11-28 21:29:59.999000+05:30  2.682785e+07  193480.0   \n",
       "..                                ...           ...       ...   \n",
       "318  2024-01-14 21:29:59.999000+05:30  2.958133e+07  112481.0   \n",
       "319  2024-01-15 01:29:59.999000+05:30  2.991980e+07  130467.0   \n",
       "0                                 NaN           NaN       NaN   \n",
       "1                                 NaN           NaN       NaN   \n",
       "2                                 NaN           NaN       NaN   \n",
       "\n",
       "     taker_buy_volume  ...  PLUS_DM_8      ROC_8    ROCP_8    ROCR_8  \\\n",
       "32         11839833.0  ...        NaN        NaN       NaN       NaN   \n",
       "33         15673550.0  ...        NaN        NaN       NaN       NaN   \n",
       "34         16160899.0  ...        NaN        NaN       NaN       NaN   \n",
       "35         22172027.0  ...        NaN        NaN       NaN       NaN   \n",
       "36         27052015.0  ...        NaN        NaN       NaN       NaN   \n",
       "..                ...  ...        ...        ...       ...       ...   \n",
       "318        41707776.0  ...   0.062508  23.781337  0.237813  1.237813   \n",
       "319        41688617.0  ...   0.058194  25.752149  0.257521  1.257521   \n",
       "0                 NaN  ...        NaN        NaN       NaN       NaN   \n",
       "1                 NaN  ...        NaN        NaN       NaN       NaN   \n",
       "2                 NaN  ...        NaN        NaN       NaN       NaN   \n",
       "\n",
       "      ROCR100_8      RSI_8     ATR_8    NATR_8    BETA_8  CORREL_8  \n",
       "32          NaN        NaN       NaN       NaN       NaN       NaN  \n",
       "33          NaN        NaN       NaN       NaN       NaN       NaN  \n",
       "34          NaN        NaN       NaN       NaN       NaN       NaN  \n",
       "35          NaN        NaN       NaN       NaN       NaN       NaN  \n",
       "36          NaN        NaN       NaN       NaN       NaN       NaN  \n",
       "..          ...        ...       ...       ...       ...       ...  \n",
       "318  123.781337  77.846939  0.022446  6.313905  0.197285  0.887472  \n",
       "319  125.752149  73.791244  0.021903  6.238305  0.178541  0.886021  \n",
       "0           NaN        NaN       NaN       NaN       NaN       NaN  \n",
       "1           NaN        NaN       NaN       NaN       NaN       NaN  \n",
       "2           NaN        NaN       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[291 rows x 187 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = filter_data_for_symbol_charttime(\"PYTHUSDT\", \"4h\", \"20231128\", \"20240114\", output_dir)\n",
    "df = calculate_wins_losses_for_dataframe(df, 20, 16)\n",
    "new_columns = calculate_indicators_using_talib([5, 8], df)\n",
    "df = pd.concat([df, new_columns], axis=1)\n",
    "display(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
