{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Start a new instance of the Chrome driver\n",
    "webdriver_service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=webdriver_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./data_creation_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from selenium import webdriver\n",
    "import talib\n",
    "import datetime\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import glob\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import inspect\n",
    "import talib\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.request\n",
    "from datetime import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date as a string in the desired format\n",
    "date_string = now.strftime(\"%Y-%m\")\n",
    "\n",
    "# Print the date string\n",
    "print(date_string)\n",
    "\n",
    "idx = MONTH_ARRAY.index(date_string)\n",
    "MONTH_ARRAY = MONTH_ARRAY[:idx + 1]\n",
    "# months = MONTH_ARRAY\n",
    "print(MONTH_ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first day of the current month\n",
    "first_day = now.replace(day=1)\n",
    "\n",
    "# Create an empty list to store the days\n",
    "DAY_ARRAY = []\n",
    "\n",
    "# Loop through the days from the first day of the current month to today\n",
    "while first_day <= now:\n",
    "    # Format the date as a string in the desired format\n",
    "    date_string = first_day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Add the date string to the list\n",
    "    DAY_ARRAY.append(date_string)\n",
    "\n",
    "    # Move to the next day\n",
    "    first_day += timedelta(days=1)\n",
    "print(DAY_ARRAY)\n",
    "# days = DAY_ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_dictionary = {\n",
    "    \"symbols\": [\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('BTCUSDT')],\n",
    "#         SYMBOL_ARRAY[SYMBOL_ARRAY.index('1000PEPEUSDT')],\n",
    "        SYMBOL_ARRAY[SYMBOL_ARRAY.index('PYTHUSDT')],\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('ETHUSDT')],\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('ETHBUSD')],\n",
    "        #         SYMBOL_ARRAY[SYMBOL_ARRAY.index('BTCBUSD')]\n",
    "    ],\n",
    "    \"chart_times\": [\n",
    "        #     CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('5m')],\n",
    "        #     CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('1m')],\n",
    "        CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('4h')]\n",
    "    ],\n",
    "    \"timeperiods\": [5, 8, 13, 21, 30, 34, 50, 55, 89, 100, 144, 200, 233],\n",
    "    \"win_percentage\":\n",
    "    0.8,\n",
    "    \"loss_percentage\":\n",
    "    0.4,\n",
    "    \"monthly_or_daily_1_or_2\":\n",
    "    1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# code for downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "#         root_dir = Path.cwd()\n",
    "#         # Create the new folder path\n",
    "#         folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "#         folder_path.mkdir(parents=True, exist_ok=True)\n",
    "#         count = 0\n",
    "#         for month in master_dictionary[\"months\"]:\n",
    "#             # Construct the link\n",
    "#             link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "#             symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "#             # Create the file path\n",
    "#             file_path = Path(folder_path) / symbol_object\n",
    "#             if not file_path.exists():\n",
    "#                 try:\n",
    "#                     # Download the file\n",
    "#                     urllib.request.urlretrieve(link, file_path)\n",
    "#                     count += 1\n",
    "#                 except:\n",
    "# #                     print(f'{link} not found')\n",
    "#                     continue\n",
    "#         if count > 0:\n",
    "#             print(\"***                  ***\")\n",
    "#             print(f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "#         else:\n",
    "#             print(\"you're already up to date\")\n",
    "\n",
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "#         root_dir = Path.cwd()\n",
    "#         # Create the new folder path\n",
    "#         folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "#         folder_path.mkdir(parents=True, exist_ok=True)\n",
    "#         count = 0\n",
    "#         for day in master_dictionary[\"days\"]:\n",
    "#             # Construct the link\n",
    "#             link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "#             symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "#             # Create the file path\n",
    "#             file_path = Path(folder_path) / symbol_object\n",
    "#             if not file_path.exists():\n",
    "#                 try:\n",
    "#                     # Download the file\n",
    "#                     urllib.request.urlretrieve(link, file_path)\n",
    "#                     count += 1\n",
    "#                 except:\n",
    "# #                     print(f'{link} not found')\n",
    "#                     continue\n",
    "#         if count > 0:\n",
    "#             print(\"***                  ***\")\n",
    "#             print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "#         else:\n",
    "#             print(\"you're already up to date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## new pyspark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "# #         # Set up an empty list for the data frames\n",
    "# #         df_list = []\n",
    "#         spark = SparkSession.builder.appName('DataProcessing').getOrCreate()\n",
    "#         # Compile the regular expression pattern\n",
    "#         pattern = re.compile(f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "#         # Create the new folder path for ZIP files\n",
    "#         new_zip_folder_path = os.path.join(download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "#         # Create the new folder path for CSV files\n",
    "#         print(output_dir)\n",
    "#         new_csv_folder_path = os.path.join(output_dir, f\"{symbol}-{chart_time}\")\n",
    "#         df_final = None\n",
    "\n",
    "#         # Iterate over the files in the new zip folder\n",
    "#         for file in os.listdir(new_zip_folder_path):\n",
    "#             # Check if the file matches the pattern\n",
    "#             if pattern.match(file):\n",
    "#                 # Construct the file path\n",
    "#                 file_path = os.path.join(new_zip_folder_path, file)\n",
    "\n",
    "#                 # Extract the ZIP file\n",
    "#                 with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "#                     zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "#                 # Construct the CSV file path\n",
    "#                 csv_file_path = os.path.join(new_csv_folder_path, f\"{symbol}-{chart_time}{file[-12:-4]}.csv\")\n",
    "\n",
    "#                 # Read the CSV file into a Spark DataFrame\n",
    "#                 df = spark.read.csv(csv_file_path, header=True)\n",
    "                \n",
    "# #                 df.show()\n",
    "\n",
    "# #                 # Filter out the header row\n",
    "# #                 df = df.filter(df['open_time'] != 'open_time')\n",
    "        \n",
    "#                 if df_final is None:\n",
    "#                     df_final = df\n",
    "#                 else:\n",
    "#                     df_final = df_final.union(df)\n",
    "#                 # Add it to the list\n",
    "#                 df_list.append(df)\n",
    "\n",
    "#         df_final.show()\n",
    "#         # Convert 'open_time' and 'close_time' columns to timestamp\n",
    "#         df_final = df_final.withColumn(\"open_time\", df_final[\"open_time\"].cast(\"timestamp\"))\n",
    "#         df_final = df_final.withColumn(\"close_time\", df_final[\"close_time\"].cast(\"timestamp\"))\n",
    "\n",
    "#         # Drop the 'ignore' column\n",
    "#         df_final = df_final.drop(\"ignore\")\n",
    "\n",
    "#         # Your existing code to create the df_final DataFrame\n",
    "#         windowSpec = Window.orderBy(\"open_time\")\n",
    "#         df_final = df_final.withColumn(\"entry\", lag(\"close\").over(windowSpec))\n",
    "        \n",
    "#         df_final.show()\n",
    "#         # Convert PySpark DataFrame to Pandas DataFrame\n",
    "#         df_final_pandas = df_final.toPandas()\n",
    "        \n",
    "#         spark.stop()\n",
    "\n",
    "# #         # Set the file name\n",
    "# #         file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "# #         # Construct the file path\n",
    "# #         file_path = os.path.join(new_csv_folder_path, file_name)\n",
    "# #         print(file_path)\n",
    "\n",
    "# #         # Write the Pandas DataFrame to a CSV file\n",
    "# #         df_final_pandas.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## download data functions, process zip, concatenate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_monthly_data(month_array, symbol, chart_time):\n",
    "    #downloading monthly data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(\n",
    "        download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for month in month_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print (f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print (f\"you're already up to date for monthly data for {symbol},{chart_time}\")\n",
    "    \n",
    "\n",
    "    \n",
    "def download_daily_data(day_array, symbol, chart_time):\n",
    "    #downloading daily data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(\n",
    "        download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for day in day_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print(f\"you're already up to date for daily data for {symbol},{chart_time}\")\n",
    "        \n",
    "def construct_csv_file_path(folder_path, symbol, chart_time, file, is_daily=False):\n",
    "    if is_daily:\n",
    "        # For daily data, use a different pattern\n",
    "        return os.path.join(\n",
    "            folder_path,\n",
    "            f\"{symbol}-{chart_time}-{file.split('-')[-3]}-{file.split('-')[-2]}-{file.split('-')[-1][:-4]}.csv\"\n",
    "        )\n",
    "    else:\n",
    "        # For monthly data, use the original pattern\n",
    "        return os.path.join(\n",
    "            folder_path,\n",
    "            f\"{symbol}-{chart_time}{file[-12:-4]}.csv\"\n",
    "        )\n",
    "\n",
    "def process_zip_folder(folder_path, pattern, new_csv_folder_path, symbol, chart_time, df_list, is_daily=False):\n",
    "    for file in os.listdir(folder_path):\n",
    "        # Check if the file matches the pattern\n",
    "        if pattern.match(file):\n",
    "            # Construct the file path\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            # Extract the ZIP file\n",
    "            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "            # Construct the CSV file path using the helper function\n",
    "            csv_file_path = construct_csv_file_path(\n",
    "                new_csv_folder_path, symbol, chart_time, file, is_daily)\n",
    "\n",
    "            # Read the CSV file into a data frame, ignoring the headers\n",
    "            df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "            # Remove the first row (which contains the header)\n",
    "            df = df.iloc[1:]\n",
    "\n",
    "            # Add it to the list\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list\n",
    "\n",
    "\n",
    "\n",
    "def concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time):\n",
    "    # Concatenate the data frames in the list\n",
    "    df_final = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    csv_file_path = os.path.join(new_csv_folder_path, os.listdir(new_csv_folder_path)[0])\n",
    "\n",
    "    # Read the headers from the first CSV file\n",
    "    headers = pd.read_csv(csv_file_path, nrows=1).columns\n",
    "    \n",
    "    # Set the headers as the column names of the final dataframe\n",
    "    df_final.columns = headers\n",
    "\n",
    "    # Convert 'open_time' and 'close_time' columns to datetime\n",
    "    df_final['open_time'] = pd.to_datetime(\n",
    "        df_final['open_time'],\n",
    "        unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "    df_final['close_time'] = pd.to_datetime(\n",
    "        df_final['close_time'],\n",
    "        unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "    # Delete the 'ignore' column\n",
    "    df_final = df_final.drop(['ignore'], axis=1)\n",
    "\n",
    "    # Add a new column called 'entry' that will take previous close\n",
    "    df_final['entry'] = df_final['close'].shift(1)\n",
    "\n",
    "    # Set the file name\n",
    "    concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "    # Construct the file path\n",
    "    concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                          concatenated_file_name)\n",
    "\n",
    "    # Write the data frame to the CSV file\n",
    "    df_final.to_csv(concatenated_file_path, index=False)\n",
    "\n",
    "    directory_final = Path(\n",
    "        concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "    # Deleting all the other CSVs\n",
    "    for file_path in directory_final.iterdir():\n",
    "        if file_path != Path(concatenated_file_path):\n",
    "            if file_path.is_file():\n",
    "                file_path.unlink()\n",
    "    return (\"data concatenated, individual csvs deleted\")\n",
    "\n",
    "\n",
    "def calculate_indicators_using_talib(timeperiods, df):\n",
    "    new_columns = pd.DataFrame()\n",
    "\n",
    "    # List to store indicator columns\n",
    "    indicator_columns = []\n",
    "    indicator_columns.append(('HT_TRENDLINE', talib.HT_TRENDLINE(df['close'])))\n",
    "    # indicator_columns.append(('MAMA', df['MAMA']), ('FAMA', df['FAMA']))\n",
    "    # indicator_columns.append(('MAVP', df['MAVP']))\n",
    "    indicator_columns.append(('SAR', talib.SAR(df['high'], df['low'], acceleration=0, maximum=0)))\n",
    "    indicator_columns.append(('SAREXT', talib.SAREXT(df['high'], df['low'])))\n",
    "    indicator_columns.append(('T3', talib.T3(df['close'], timeperiod=5, vfactor=0)))\n",
    "    # Momentum Indicators\n",
    "    indicator_columns.append(('APO', talib.APO(df['close'], fastperiod=12, slowperiod=26)))\n",
    "    indicator_columns.append(('BOP', talib.BOP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    macd, macd_signal, macd_hist = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    indicator_columns.append(('MACD', macd))\n",
    "    indicator_columns.append(('MACD_signal', macd_signal))\n",
    "    indicator_columns.append(('MACD_hist', macd_hist))\n",
    "    indicator_columns.append(('PPO', talib.PPO(df['close'], fastperiod=12, slowperiod=26, matype=0)))\n",
    "    indicator_columns.append(('TRIX', talib.TRIX(df['close'])))\n",
    "    indicator_columns.append(('ULTOSC', talib.ULTOSC(df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('WILLR', talib.WILLR(df['high'], df['low'], df['close'])))\n",
    "\n",
    "\n",
    "#     # Not Working ATM\n",
    "#     indicator_columns.append(('STOCH', talib.STOCH(df['high'], df['low'], df['close'])))\n",
    "#     indicator_columns.append(('STOCHF', talib.STOCHF(df['high'], df['low'], df['close'])))\n",
    "#     indicator_columns.append(('STOCHRSI', talib.STOCHRSI(df['close'])))\n",
    "#     indicator_columns.append(('MACDEXT', talib.MACDEXT(df['close'], fastperiod=12, fastmatype=0, slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)))\n",
    "#     indicator_columns.append(('MACDFIX', talib.MACDFIX(df['close'], signalperiod=9)))\n",
    "    \n",
    "    \n",
    "    #########Volume Indicators\n",
    "    indicator_columns.append(('AD', talib.AD(df['high'], df['low'], df['close'], df['volume'])))\n",
    "    indicator_columns.append(('ADOSC', talib.ADOSC(df['high'], df['low'], df['close'], df['volume'], fastperiod=3, slowperiod=10)))\n",
    "    indicator_columns.append(('OBV', talib.OBV(df['close'], df['volume'])))\n",
    "\n",
    "    #########Cycle Indicators\n",
    "    indicator_columns.append(('HT_DCPERIOD', talib.HT_DCPERIOD(df['close'])))\n",
    "    indicator_columns.append(('HT_DCPHASE', talib.HT_DCPHASE(df['close'])))\n",
    "    phasor_inphase, phasor_quadrature = talib.HT_PHASOR(df['close'])\n",
    "    indicator_columns.append(('HT_PHASOR_inphase', phasor_inphase))\n",
    "    indicator_columns.append(('HT_PHASOR_quadrature', phasor_quadrature))\n",
    "    # indicator_columns.append(('HT_SINE', talib.HT_SINE(df['close'])))\n",
    "    indicator_columns.append(('HT_TRENDMODE', talib.HT_TRENDMODE(df['close'])))\n",
    "\n",
    "    #########Price Transform\n",
    "    indicator_columns.append(('AVGPRICE', talib.AVGPRICE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('MEDPRICE', talib.MEDPRICE(df['high'], df['low'])))\n",
    "    indicator_columns.append(('TYPPRICE', talib.TYPPRICE(df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('WCLPRICE', talib.WCLPRICE(df['high'], df['low'], df['close'])))\n",
    "    #########Volatility Indicators\n",
    "    indicator_columns.append(('TRANGE', talib.TRANGE(df['high'], df['low'], df['close'])))\n",
    "    #########Pattern Recognition\n",
    "    indicator_columns.append(('CDL2CROWS', talib.CDL2CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3BLACKCROWS', talib.CDL3BLACKCROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3INSIDE', talib.CDL3INSIDE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3LINESTRIKE', talib.CDL3LINESTRIKE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3OUTSIDE', talib.CDL3OUTSIDE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3STARSINSOUTH', talib.CDL3STARSINSOUTH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3WHITESOLDIERS', talib.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLABANDONEDBABY', talib.CDLABANDONEDBABY(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLADVANCEBLOCK', talib.CDLADVANCEBLOCK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLBELTHOLD', talib.CDLBELTHOLD(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLBREAKAWAY', talib.CDLBREAKAWAY(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCLOSINGMARUBOZU', talib.CDLCLOSINGMARUBOZU(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCONCEALBABYSWALL', talib.CDLCONCEALBABYSWALL(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCOUNTERATTACK', talib.CDLCOUNTERATTACK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDARKCLOUDCOVER', talib.CDLDARKCLOUDCOVER(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLDOJI', talib.CDLDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDOJISTAR', talib.CDLDOJISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDRAGONFLYDOJI', talib.CDLDRAGONFLYDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLENGULFING', talib.CDLENGULFING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLEVENINGDOJISTAR', talib.CDLEVENINGDOJISTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLEVENINGSTAR', talib.CDLEVENINGSTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "    indicator_columns.append(('CDLGAPSIDESIDEWHITE', talib.CDLGAPSIDESIDEWHITE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLGRAVESTONEDOJI', talib.CDLGRAVESTONEDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHAMMER', talib.CDLHAMMER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHANGINGMAN', talib.CDLHANGINGMAN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHARAMI', talib.CDLHARAMI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHARAMICROSS', talib.CDLHARAMICROSS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIGHWAVE', talib.CDLHIGHWAVE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIKKAKE', talib.CDLHIKKAKE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIKKAKEMOD', talib.CDLHIKKAKEMOD(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHOMINGPIGEON', talib.CDLHOMINGPIGEON(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLIDENTICAL3CROWS', talib.CDLIDENTICAL3CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLINNECK', talib.CDLINNECK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLINVERTEDHAMMER', talib.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLKICKING', talib.CDLKICKING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLKICKINGBYLENGTH', talib.CDLKICKINGBYLENGTH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLADDERBOTTOM', talib.CDLLADDERBOTTOM(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLONGLEGGEDDOJI', talib.CDLLONGLEGGEDDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLONGLINE', talib.CDLLONGLINE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMARUBOZU', talib.CDLMARUBOZU(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMATCHINGLOW', talib.CDLMATCHINGLOW(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMATHOLD', talib.CDLMATHOLD(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "    indicator_columns.append(('CDLMORNINGDOJISTAR', talib.CDLMORNINGDOJISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMORNINGSTAR', talib.CDLMORNINGSTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLONNECK', talib.CDLONNECK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLPIERCING', talib.CDLPIERCING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLRICKSHAWMAN', talib.CDLRICKSHAWMAN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLRISEFALL3METHODS', talib.CDLRISEFALL3METHODS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSEPARATINGLINES', talib.CDLSEPARATINGLINES(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSHOOTINGSTAR', talib.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSHORTLINE', talib.CDLSHORTLINE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSPINNINGTOP', talib.CDLSPINNINGTOP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSTALLEDPATTERN', talib.CDLSTALLEDPATTERN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSTICKSANDWICH', talib.CDLSTICKSANDWICH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTAKURI', talib.CDLTAKURI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTASUKIGAP', talib.CDLTASUKIGAP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTHRUSTING', talib.CDLTHRUSTING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTRISTAR', talib.CDLTRISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLUNIQUE3RIVER', talib.CDLUNIQUE3RIVER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLUPSIDEGAP2CROWS', talib.CDLUPSIDEGAP2CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLXSIDEGAP3METHODS', talib.CDLXSIDEGAP3METHODS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    #########Statistic Functions\n",
    "    indicator_columns.append(('LINEARREG', talib.LINEARREG(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_ANGLE', talib.LINEARREG_ANGLE(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_INTERCEPT', talib.LINEARREG_INTERCEPT(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_SLOPE', talib.LINEARREG_SLOPE(df['close'])))\n",
    "    # new_columns['STDDEV'] = df['close'].rolling(timeperiod).std()\n",
    "    indicator_columns.append(('TSF', talib.TSF(df['close'])))\n",
    "    indicator_columns.append(('VAR', talib.VAR(df['close'])))\n",
    "    # Iterate over the time periods\n",
    "    for timeperiod in timeperiods:\n",
    "        #########Overlap Studies\n",
    "        upper_band, middle_band, lower_band = talib.BBANDS(df['close'], timeperiod=timeperiod)\n",
    "        indicator_columns.append((f'BB_upper_{timeperiod}', upper_band))\n",
    "        indicator_columns.append((f'BB_middle_{timeperiod}', middle_band))\n",
    "        indicator_columns.append((f'BB_lower_{timeperiod}', lower_band))\n",
    "        indicator_columns.append((f'DEMA_{timeperiod}', talib.DEMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'EMA_{timeperiod}', talib.EMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'KAMA_{timeperiod}', talib.KAMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MA_{timeperiod}', talib.MA(df['close'], timeperiod=timeperiod)))\n",
    "        # new_columns['MAMA'], new_columns['FAMA'] = talib.MAMA(df['close'], fastlimit=0, slowlimit=0)\n",
    "        # new_columns['MAVP'] = talib.MAVP(df['close'], periods=None, minperiod=2, maxperiod=30, matype=0)\n",
    "        indicator_columns.append((f'MIDPOINT_{timeperiod}', talib.MIDPOINT(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MIDPRICE_{timeperiod}', talib.MIDPRICE(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'SMA_{timeperiod}', talib.SMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'TEMA_{timeperiod}', talib.TEMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'TRIMA_{timeperiod}', talib.TRIMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'WMA_{timeperiod}', talib.WMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ADX_{timeperiod}', talib.ADX(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ADXR_{timeperiod}', talib.ADXR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        aroon_up, aroon_down = talib.AROON(df['high'], df['low'], timeperiod=timeperiod)\n",
    "        indicator_columns.append((f'AROON_up_{timeperiod}', aroon_up))\n",
    "        indicator_columns.append((f'AROON_down_{timeperiod}', aroon_down))\n",
    "        indicator_columns.append((f'AROONOSC_{timeperiod}', talib.AROONOSC(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CCI_{timeperiod}', talib.CCI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CMO_{timeperiod}', talib.CMO(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'DX_{timeperiod}', talib.DX(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MFI_{timeperiod}', talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MINUS_DI_{timeperiod}', talib.MINUS_DI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MINUS_DM_{timeperiod}', talib.MINUS_DM(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MOM_{timeperiod}', talib.MOM(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'PLUS_DI_{timeperiod}', talib.PLUS_DI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'PLUS_DM_{timeperiod}', talib.PLUS_DM(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROC_{timeperiod}', talib.ROC(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCP_{timeperiod}', talib.ROCP(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCR_{timeperiod}', talib.ROCR(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCR100_{timeperiod}', talib.ROCR100(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'RSI_{timeperiod}', talib.RSI(df['close'], timeperiod=timeperiod)))\n",
    "\n",
    "        indicator_columns.append((f'ATR_{timeperiod}', talib.ATR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'NATR_{timeperiod}', talib.NATR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        #########Statistic Functions\n",
    "        indicator_columns.append((f'BETA_{timeperiod}', talib.BETA(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CORREL_{timeperiod}', talib.CORREL(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "    new_columns = pd.concat([pd.DataFrame(data, columns=[name]) for name, data in indicator_columns], axis=1)\n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_and_concatenate(master_dictionary, month_array, day_array):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"setting up things for {symbol},{chart_time}\")\n",
    "\n",
    "            # Set up an empty list for the data frames\n",
    "            df_list = []\n",
    "\n",
    "            # Compile the regular expression pattern\n",
    "            pattern = re.compile(f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Compile the regular expression pattern for daily zip files\n",
    "            pattern_daily = re.compile(\n",
    "                f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Create the new folder path for daily ZIP files\n",
    "            new_daily_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-daily_data\")\n",
    "\n",
    "            # Create the new folder path for ZIP files\n",
    "            new_monthly_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "            # Create the new folder path for CSV files\n",
    "            new_csv_folder_path = os.path.join(output_dir,\n",
    "                                               f\"{symbol}-{chart_time}\")\n",
    "\n",
    "            # Set the file name\n",
    "            concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "            # Construct the file path\n",
    "            concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                                  concatenated_file_name)\n",
    "\n",
    "            if not Path(concatenated_file_path).exists():\n",
    "                download_monthly_data(month_array, symbol, chart_time)  \n",
    "                download_daily_data(day_array, symbol, chart_time)  \n",
    "\n",
    "                # Process the monthly ZIP folder and add to df_list\n",
    "                df_list = process_zip_folder(new_monthly_zip_folder_path, pattern, new_csv_folder_path, symbol, chart_time, df_list)\n",
    "\n",
    "                # Process the daily ZIP folder and add to df_list\n",
    "                df_list = process_zip_folder(new_daily_zip_folder_path, pattern_daily, new_csv_folder_path, symbol, chart_time, df_list, is_daily=True)\n",
    "\n",
    "                # Call the function to concatenate and process the data frames\n",
    "                print(concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time))\n",
    "#             else:\n",
    "#                 df_list = []\n",
    "#                 df = pd.read_csv(concatenated_file_path, header=None)\n",
    "#                 last_record = df.iloc[-1:]\n",
    "#                 df_list.append(last_record)\n",
    "\n",
    "#                 df = pd.read_csv(concatenated_file_path)\n",
    "#                 df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "#                 last_record = df.iloc[-1:]\n",
    "\n",
    "\n",
    "#                 # Check if the time part of 'open_time' is \"05:15:00+05:30\"\n",
    "#                 if last_record['open_time'].dt.strftime(\n",
    "#                         '%H:%M:%S%z').item() == \"05:15:00+0530\":\n",
    "#                     # Calculate the last processed date by subtracting 1 day from the date part\n",
    "#                     last_processed_date = (\n",
    "#                         last_record['open_time'].dt.date -\n",
    "#                         pd.DateOffset(days=1)).item().strftime('%Y-%m-%d')\n",
    "#                     last_processed_date = datetime.strptime(\n",
    "#                         last_processed_date, \"%Y-%m-%d\")\n",
    "\n",
    "#                     # Get today's date\n",
    "#                     today = datetime.today()\n",
    "\n",
    "#                     # Initialize DAY_ARRAY\n",
    "#                     DAY_ARRAY = []\n",
    "\n",
    "#                     # Start from the day after last_processed_date\n",
    "#                     current_day = last_processed_date + timedelta(days=1)\n",
    "\n",
    "#                     while current_day <= today:\n",
    "#                         # Format the date as a string in the desired format\n",
    "#                         date_string = current_day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#                         # Add the date string to DAY_ARRAY\n",
    "#                         DAY_ARRAY.append(date_string)\n",
    "\n",
    "#                         # Move to the next day\n",
    "#                         current_day += timedelta(days=1)\n",
    "\n",
    "#                     download_daily_data()\n",
    "#                     # Iterate over the files in the new daily zip folder\n",
    "#                     for file in Path(new_daily_zip_folder_path).iterdir():\n",
    "#                         # Check if the file matches the pattern\n",
    "#                         if pattern_daily.match(file.name):\n",
    "#                             # Extract the date part from the file name (e.g., \"2023-08-19\")\n",
    "#                             zip_date = file.name.split('-')[-3:]\n",
    "#                             zip_date_str = '-'.join(zip_date).replace('.zip', '')\n",
    "\n",
    "#                             # Check if the date is in DAY_ARRAY\n",
    "#                             if zip_date_str in DAY_ARRAY:\n",
    "#                                 # Construct the file path\n",
    "#                                 file_path = file\n",
    "\n",
    "#                                 # Extract the ZIP file\n",
    "#                                 with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "#                                     zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "#                                 # Construct the CSV file path\n",
    "#                                 csv_file_path = Path(new_csv_folder_path) / f\"{symbol}-{chart_time}-{zip_date_str}.csv\"\n",
    "\n",
    "#                                 # Read the CSV file into a data frame, ignoring the headers\n",
    "#                                 df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "#                                 # Remove the first row (which contains the header)\n",
    "#                                 df = df.iloc[1:]\n",
    "\n",
    "#                                 # Add it to the list\n",
    "#                                 df_list.append(df)\n",
    "#                     # Concatenate the data frames in the list\n",
    "#                     df_final = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "#                     # Read the headers from the first CSV file\n",
    "#                     headers = pd.read_csv(concatenated_file_path, nrows=1).columns\n",
    "\n",
    "#                     # Set the headers as the column names of the final dataframe\n",
    "#                     df_final.columns = headers\n",
    "\n",
    "#                     # Convert 'open_time' and 'close_time' columns to datetime\n",
    "#                     unix_time_format = df_final['open_time'].str.contains(r'^\\d{13}$')\n",
    "#                     df_final.loc[unix_time_format, 'open_time'] = pd.to_datetime(\n",
    "#                         df_final.loc[unix_time_format, 'open_time'],\n",
    "#                         unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "#                     unix_time_format = df_final['close_time'].str.contains(r'^\\d{13}$')\n",
    "#                     df_final.loc[unix_time_format, 'close_time'] = pd.to_datetime(\n",
    "#                         df_final.loc[unix_time_format, 'close_time'],\n",
    "#                         unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "\n",
    "#                     # Add a new column called 'entry' that will take previous close\n",
    "#                     df_final['entry'] = df_final['close'].shift(1)\n",
    "#                     # Append the data frame to the existing CSV file in append mode without headers\n",
    "#                     df_final[1:].to_csv(concatenated_file_path, mode='a', header=False, index=False)\n",
    "#                     directory_final = Path(\n",
    "#                     concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "#                     # deleting all the other csvs\n",
    "#                     for file_path in directory_final.iterdir():\n",
    "#                         if file_path != Path(concatenated_file_path):\n",
    "#                             if file_path.is_file():\n",
    "#                                 file_path.unlink()\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     print(\"Something is wrong with data\")\n",
    "    return (\"data downloaded and concatenated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_and_concatenate(master_dictionary, MONTH_ARRAY, DAY_ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wins_losses(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"calculating for {symbol}{chart_time}\")\n",
    "            # Construct the file name\n",
    "            og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "            og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "            new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            new_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{new_file_name}\"\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(og_file_path)\n",
    "            df[\"if_short\"] = 0\n",
    "            df[\"if_long\"] = 0\n",
    "            df[\"long_target\"] = np.nan\n",
    "            df[\"short_target\"] = np.nan\n",
    "            df[\"long_stop_loss\"] = np.nan\n",
    "            df[\"short_stop_loss\"] = np.nan\n",
    "            df[\"shorts_win_after\"] = np.nan\n",
    "            df[\"longs_win_after\"] = np.nan\n",
    "            df[\"dual_loss\"] = 0\n",
    "            df[\"entered_before\"] = np.nan\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                if df[\"entry\"][i]:\n",
    "                    long_target = df[\"entry\"][i] * (1 + win_perc / 100)\n",
    "                    short_target = df[\"entry\"][i] * (1 - win_perc / 100)\n",
    "                    long_stop_loss = df[\"entry\"][i] * (1 - loss_perc / 100)\n",
    "                    short_stop_loss = df[\"entry\"][i] * (1 + loss_perc / 100)\n",
    "                    df.loc[i, 'long_target'] = long_target\n",
    "                    df.loc[i, 'long_stop_loss'] = long_stop_loss\n",
    "                    for j in range(i, len(df)):\n",
    "                        if df[\"high\"][j] >= long_target:\n",
    "                            if df[\"low\"][j] <= long_stop_loss:\n",
    "                                df.loc[i, 'if_long'] = -1\n",
    "                                df.loc[i, 'dual_loss'] = 1\n",
    "                                df.loc[i, 'entered_before'] = j - i\n",
    "                            else:\n",
    "                                df.loc[i, 'if_long'] = 1\n",
    "                                df.loc[i, 'longs_win_after'] = j - i\n",
    "                            break\n",
    "                        elif df[\"low\"][j] <= long_stop_loss:\n",
    "                            df.loc[i, 'if_long'] = -1\n",
    "                            break\n",
    "                    df.loc[i, 'short_target'] = short_target\n",
    "                    df.loc[i, 'short_stop_loss'] = short_stop_loss\n",
    "                    for j in range(i, len(df)):\n",
    "                        if df[\"low\"][j] <= short_target:\n",
    "                            if df[\"high\"][j] >= short_stop_loss:\n",
    "                                df.loc[i, 'if_short'] = -1\n",
    "                                df.loc[i, 'dual_loss'] = 1\n",
    "                                df.loc[i, 'entered_before'] = j - i\n",
    "                            else:\n",
    "                                df.loc[i, 'if_short'] = 1\n",
    "                                df.loc[i, 'shorts_win_after'] = j - i\n",
    "                            break\n",
    "                        elif df[\"high\"][j] >= short_stop_loss:\n",
    "                            df.loc[i, 'if_short'] = -1\n",
    "                            break\n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df.to_csv(new_file_path, index=False)\n",
    "\n",
    "    return (\"calculated wins and losses \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_wins_losses(master_dictionary, win_perc=20, loss_perc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indicator_values(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            # Construct the file name\n",
    "            file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{file_name}\"\n",
    "            if not Path(file_path).exists():\n",
    "                print(f\"File path for {file_name} doesn't exist. Breaking.\")\n",
    "                break\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(df.dtypes)\n",
    "            new_columns = calculate_indicators_using_talib(master_dictionary, df)\n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df = pd.concat([df, new_columns], axis=1)\n",
    "            df.to_csv(file_path, index=False)\n",
    "    return (\"indicators are added to the csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_indicator_values_new(master_dictionary, win_perc=20, loss_perc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_between_dates(master_dictionary, start_date, end_date, data_path):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"displaying filtered for {symbol}-{chart_time}\")\n",
    "            display(filter_data_for_symbol_charttime(symbol, chart_time, start_date, end_date, data_path))\n",
    "\n",
    "def filter_data_for_symbol_charttime(symbol, chart_time, start_date, end_date, data_path):\n",
    "    # Read the CSV file into a data frame\n",
    "    df = pd.read_csv(Path(data_path) / f\"{symbol}-{chart_time}\" / f\"{symbol}-{chart_time}.csv\")\n",
    "    \n",
    "    # Convert 'open_time' to datetime format\n",
    "    df['open_time'] = pd.to_datetime(df['open_time']).dt.date\n",
    "    \n",
    "    # Convert start_date and end_date to datetime objects\n",
    "    start_date = pd.to_datetime(start_date, format='%Y%m%d').date()\n",
    "    end_date = pd.to_datetime(end_date, format='%Y%m%d').date()\n",
    "\n",
    "    # Filter data between start_date and end_date based on the date part of 'open_time'\n",
    "    df_filtered = df[(df['open_time'] >= start_date) & (df['open_time'] <= end_date)]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "#sample function call: read_data_between_dates(master_dictionary, \"20231206\", \"20231206\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_between_dates(master_dictionary, \"20231206\", \"20231206\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2023-12-06\"\n",
    "end_date = \"2023-12-07\"\n",
    "for symbol in master_dictionary[\"symbols\"]:\n",
    "    for chart_time in master_dictionary[\"chart_times\"]:\n",
    "        df = pd.read_csv(Path(output_dir) / f\"{symbol}-{chart_time}\" / f\"{symbol}-{chart_time}.csv\")\n",
    "#         display(df)\n",
    "        df['open_time'] = pd.to_datetime(df['open_time']).dt.date\n",
    "        df_filtered = df[(df['open_time'] >= pd.to_datetime(start_date).date()) & (df['open_time'] <= pd.to_datetime(end_date).date())]\n",
    "        display(df_filtered)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wins_losses_for_dataframe(df, win_perc, loss_perc):\n",
    "    df[\"if_short\"] = 0\n",
    "    df[\"if_long\"] = 0\n",
    "    df[\"long_target\"] = np.nan\n",
    "    df[\"short_target\"] = np.nan\n",
    "    df[\"long_stop_loss\"] = np.nan\n",
    "    df[\"short_stop_loss\"] = np.nan\n",
    "    df[\"shorts_win_after\"] = np.nan\n",
    "    df[\"longs_win_after\"] = np.nan\n",
    "    df[\"dual_loss\"] = 0\n",
    "    df[\"entered_before\"] = np.nan\n",
    "\n",
    "    for i in df.index:\n",
    "        if df[\"entry\"][i]:\n",
    "            long_target = df[\"entry\"][i] * (1 + win_perc / 100)\n",
    "            short_target = df[\"entry\"][i] * (1 - win_perc / 100)\n",
    "            long_stop_loss = df[\"entry\"][i] * (1 - loss_perc / 100)\n",
    "            short_stop_loss = df[\"entry\"][i] * (1 + loss_perc / 100)\n",
    "            df.loc[i, 'long_target'] = long_target\n",
    "            df.loc[i, 'long_stop_loss'] = long_stop_loss\n",
    "            for j in range(i, len(df)):\n",
    "                if df[\"high\"][j] >= long_target:\n",
    "                    if df[\"low\"][j] <= long_stop_loss:\n",
    "                        df.loc[i, 'if_long'] = -1\n",
    "                        df.loc[i, 'dual_loss'] = 1\n",
    "                        df.loc[i, 'entered_before'] = j - i\n",
    "                    else:\n",
    "                        df.loc[i, 'if_long'] = 1\n",
    "                        df.loc[i, 'longs_win_after'] = j - i\n",
    "                    break\n",
    "                elif df[\"low\"][j] <= long_stop_loss:\n",
    "                    df.loc[i, 'if_long'] = -1\n",
    "                    break\n",
    "            df.loc[i, 'short_target'] = short_target\n",
    "            df.loc[i, 'short_stop_loss'] = short_stop_loss\n",
    "            for j in range(i, len(df)):\n",
    "                if df[\"low\"][j] <= short_target:\n",
    "                    if df[\"high\"][j] >= short_stop_loss:\n",
    "                        df.loc[i, 'if_short'] = -1\n",
    "                        df.loc[i, 'dual_loss'] = 1\n",
    "                        df.loc[i, 'entered_before'] = j - i\n",
    "                    else:\n",
    "                        df.loc[i, 'if_short'] = 1\n",
    "                        df.loc[i, 'shorts_win_after'] = j - i\n",
    "                    break\n",
    "                elif df[\"high\"][j] >= short_stop_loss:\n",
    "                    df.loc[i, 'if_short'] = -1\n",
    "                    break\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_wins_losses(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"calculating for {symbol}{chart_time}\")\n",
    "            # Construct the file name\n",
    "            og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "            og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "            new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            new_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{new_file_name}\"\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(og_file_path)\n",
    "            # Calculate wins and losses using the function\n",
    "            df_calculated = calculate_wins_losses_for_dataframe(df, win_perc, loss_perc)\n",
    "            # Save the updated dataframe to the new CSV file\n",
    "            df_calculated.to_csv(new_file_path, index=False)\n",
    "\n",
    "    return (\"calculated wins and losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_data_for_symbol_charttime(\"PYTHUSDT\", \"4h\", \"20231128\", \"20240114\", output_dir)\n",
    "df = calculate_wins_losses_for_dataframe(df, 20, 16)\n",
    "new_columns = calculate_indicators_using_talib([5, 8], df)\n",
    "df = pd.concat([df, new_columns], axis=1)\n",
    "display(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
