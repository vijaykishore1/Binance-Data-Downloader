{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e4da54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:54:48.827620Z",
     "start_time": "2023-10-02T10:54:35.042770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\downloaded_data\n",
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\extracted_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3316\\3069871549.py:11: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(CHROME_DRIVER_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1000BTTCUSDT', '1000FLOKIUSDT', '1000LUNCBUSD', '1000LUNCUSDT', '1000PEPEUSDT', '1000SHIBBUSD', '1000SHIBUSDT', '1000XECUSDT', '1INCHUSDT', 'AAVEUSDT', 'ACHUSDT', 'ADABUSD', 'ADAUSDT', 'AGIXBUSD', 'AGIXUSDT', 'AGLDUSDT', 'AKROUSDT', 'ALGOUSDT', 'ALICEUSDT', 'ALPHAUSDT', 'AMBBUSD', 'AMBUSDT', 'ANCBUSD', 'ANCUSDT', 'ANKRUSDT', 'ANTUSDT', 'APEBUSD', 'APEUSDT', 'API3USDT', 'APTBUSD', 'APTUSDT', 'ARBUSDT', 'ARKMUSDT', 'ARKUSDT', 'ARPAUSDT', 'ARUSDT', 'ASTRUSDT', 'ATAUSDT', 'ATOMUSDT', 'AUCTIONBUSD', 'AUDIOUSDT', 'AVAXBUSD', 'AVAXUSDT', 'AXSUSDT', 'BAKEUSDT', 'BALUSDT', 'BANDUSDT', 'BATUSDT', 'BCHUSDT', 'BELUSDT', 'BICOUSDT', 'BLUEBIRDUSDT', 'BLURUSDT', 'BLZUSDT', 'BNBBUSD', 'BNBUSDT', 'BNTUSDT', 'BNXUSDT', 'BNXUSDTSETTLED', 'BTCBUSD', 'BTCBUSD_210129', 'BTCBUSD_210226', 'BTCDOMUSDT', 'BTCSTUSDT', 'BTCUSDT', 'BTCUSDT_210326', 'BTCUSDT_210625', 'BTCUSDT_210924', 'BTCUSDT_211231', 'BTCUSDT_220325', 'BTCUSDT_220624', 'BTCUSDT_220930', 'BTCUSDT_221230', 'BTCUSDT_230331', 'BTCUSDT_230630', 'BTCUSDT_230929', 'BTCUSDT_231229', 'BTCUSDT_240329', 'BTSUSDT', 'BTTUSDT', 'BZRXUSDT', 'C98USDT', 'CELOUSDT', 'CELRUSDT', 'CFXUSDT', 'CHRUSDT', 'CHZUSDT', 'CKBUSDT', 'COCOSUSDT', 'COMBOUSDT', 'COMPUSDT', 'COTIUSDT', 'CRVUSDT', 'CTKUSDT', 'CTSIUSDT', 'CVCUSDT', 'CVXBUSD', 'CVXUSDT', 'CYBERUSDT', 'DARUSDT', 'DASHUSDT', 'DEFIUSDT', 'DENTUSDT', 'DGBUSDT', 'DODOBUSD', 'DODOUSDT', 'DODOXUSDT', 'DOGEBUSD', 'DOGEUSDT', 'DOTBUSD', 'DOTECOUSDT', 'DOTUSDT', 'DUSKUSDT', 'DYDXUSDT', 'EDUUSDT', 'EGLDUSDT', 'ENJUSDT', 'ENSUSDT', 'EOSUSDT', 'ETCBUSD', 'ETCUSDT', 'ETHBTC', 'ETHBUSD', 'ETHUSDT', 'ETHUSDT_210326', 'ETHUSDT_210625', 'ETHUSDT_210924', 'ETHUSDT_211231', 'ETHUSDT_220325', 'ETHUSDT_220624', 'ETHUSDT_220930', 'ETHUSDT_221230', 'ETHUSDT_230331', 'ETHUSDT_230630', 'ETHUSDT_230929', 'ETHUSDT_231229', 'ETHUSDT_240329', 'FETUSDT', 'FILBUSD', 'FILUSDT', 'FLMUSDT', 'FLOWUSDT', 'FOOTBALLUSDT', 'FRONTUSDT', 'FTMBUSD', 'FTMUSDT', 'FTTBUSD', 'FTTUSDT', 'FXSUSDT', 'GALABUSD', 'GALAUSDT', 'GALBUSD', 'GALUSDT', 'GLMRUSDT', 'GMTBUSD', 'GMTUSDT', 'GMXUSDT', 'GRTUSDT', 'GTCUSDT', 'HBARUSDT', 'HFTUSDT', 'HIFIUSDT', 'HIGHUSDT', 'HNTUSDT', 'HOOKUSDT', 'HOTUSDT', 'ICPBUSD', 'ICPUSDT', 'ICPUSDT_SETTLED', 'ICXUSDT', 'IDEXUSDT', 'IDUSDT', 'IMXUSDT', 'INJUSDT', 'IOSTUSDT', 'IOTAUSDT', 'IOTXUSDT', 'JASMYUSDT', 'JOEUSDT', 'KAVAUSDT', 'KEEPUSDT', 'KEYUSDT', 'KLAYUSDT', 'KNCUSDT', 'KSMUSDT', 'LDOBUSD', 'LDOUSDT', 'LENDUSDT', 'LEVERBUSD', 'LEVERUSDT', 'LINAUSDT', 'LINKBUSD', 'LINKUSDT', 'LITUSDT', 'LPTUSDT', 'LQTYUSDT', 'LRCUSDT', 'LTCBUSD', 'LTCUSDT', 'LUNA2BUSD', 'LUNA2USDT', 'LUNABUSD', 'LUNAUSDT', 'MAGICUSDT', 'MANAUSDT', 'MASKUSDT', 'MATICBUSD', 'MATICUSDT', 'MAVUSDT', 'MDTUSDT', 'MINAUSDT', 'MINAUSDTSETTLED', 'MKRUSDT', 'MTLUSDT', 'NEARBUSD', 'NEARUSDT', 'NEOUSDT', 'NKNUSDT', 'NMRUSDT', 'NUUSDT', 'OCEANUSDT', 'OGNUSDT', 'OMGUSDT', 'ONEUSDT', 'ONTUSDT', 'OPUSDT', 'OXTUSDT', 'PENDLEUSDT', 'PEOPLEUSDT', 'PERPUSDT', 'PHBBUSD', 'PHBUSDT', 'QNTUSDT', 'QTUMUSDT', 'RADUSDT', 'RAYUSDT', 'RDNTUSDT', 'REEFUSDT', 'RENUSDT', 'RLCUSDT', 'RNDRUSDT', 'ROSEUSDT', 'RSRUSDT', 'RUNEUSDT', 'RVNUSDT', 'SANDBUSD', 'SANDUSDT', 'SCUSDT', 'SEIUSDT', 'SFPUSDT', 'SKLUSDT', 'SNXUSDT', 'SOLBUSD', 'SOLUSDT', 'SPELLUSDT', 'SRMUSDT', 'SSVUSDT', 'STGUSDT', 'STMXUSDT', 'STORJUSDT', 'STXUSDT', 'SUIUSDT', 'SUSHIUSDT', 'SXPUSDT', 'THETAUSDT', 'TLMBUSD', 'TLMUSDT', 'TLMUSDTSETTLED', 'TOMOUSDT', 'TRBUSDT', 'TRUUSDT', 'TRXBUSD', 'TRXUSDT', 'TUSDT', 'UMAUSDT', 'UNFIUSDT', 'UNIBUSD', 'UNIUSDT', 'USDCUSDT', 'VETUSDT', 'WAVESBUSD', 'WAVESUSDT', 'WLDUSDT', 'WOOUSDT', 'XEMUSDT', 'XLMUSDT', 'XMRUSDT', 'XRPBUSD', 'XRPUSDT', 'XTZUSDT', 'XVGUSDT', 'XVSUSDT', 'YFIIUSDT', 'YFIUSDT', 'YGGUSDT', 'ZECUSDT', 'ZENUSDT', 'ZILUSDT', 'ZRXUSDT']\n",
      "['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02', '2024-03', '2024-04', '2024-05', '2024-07', '2024-08', '2024-09', '2024-10', '2024-11', '2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05', '2025-06', '2025-07', '2025-08', '2025-09', '2025-10', '2025-11', '2025-12', '2026-01', '2026-02', '2026-03', '2026-04', '2026-05', '2026-06', '2026-07', '2026-08', '2026-09', '2026-10', '2026-11', '2026-12', '2027-01', '2027-02', '2027-03', '2027-04', '2027-05', '2027-06', '2027-07', '2027-08', '2027-09', '2027-10', '2027-11', '2027-12', '2028-01', '2028-02', '2028-03', '2028-04', '2028-05', '2028-06', '2028-07', '2028-08', '2028-09', '2028-10', '2028-11', '2028-12', '2029-01', '2029-03', '2029-04', '2029-05', '2029-06', '2029-07', '2029-08', '2029-09', '2029-10', '2029-11', '2029-12', '2030-01', '2030-02', '2030-03', '2030-04', '2030-05', '2030-06', '2030-07', '2030-08', '2030-09', '2030-10', '2030-11', '2030-12', '2031-01', '2031-02', '2031-03', '2031-04', '2031-05', '2031-06', '2031-07', '2031-08', '2031-09', '2031-10', '2031-11', '2031-12', '2032-01', '2032-02', '2032-03', '2032-04', '2032-05', '2032-06', '2032-07', '2032-08', '2032-09', '2032-10', '2032-11', '2032-12', '2033-01', '2033-02', '2033-03', '2033-04', '2033-05', '2033-06', '2033-07', '2033-08', '2033-10', '2033-11', '2033-12', '2034-01', '2034-02', '2034-03', '2034-04', '2034-05', '2034-06', '2034-07', '2034-08', '2034-09', '2034-10', '2034-11', '2034-12', '2035-01', '2035-02', '2035-03', '2035-04', '2035-05', '2035-06', '2035-07', '2035-08', '2035-09', '2035-10', '2035-11', '2035-12', '2036-01', '2036-02', '2036-03', '2036-04', '2036-05', '2036-06', '2036-07', '2036-08', '2036-09', '2036-10', '2036-11', '2036-12', '2037-01', '2037-02', '2037-03', '2037-04', '2037-05', '2037-06', '2037-07', '2037-08', '2037-09', '2037-10', '2037-11', '2037-12', '2038-01', '2038-03', '2038-04', '2038-05', '2038-06', '2038-07', '2038-08', '2038-09', '2038-10', '2038-11', '2038-12', '2039-01', '2039-02', '2039-03', '2039-04', '2039-05', '2039-06', '2039-07', '2039-08', '2039-09', '2039-10', '2039-11', '2039-12', '2040-01', '2040-02', '2040-03', '2040-04', '2040-05', '2040-06', '2040-07', '2040-08', '2040-09', '2040-10', '2040-11', '2040-12', '2041-01', '2041-02', '2041-03', '2041-04', '2041-05', '2041-06', '2041-07', '2041-08', '2041-09', '2041-10', '2041-11', '2041-12', '2042-01', '2042-02', '2042-03', '2042-04', '2042-05', '2042-06', '2042-07', '2042-08', '2042-09', '2042-10', '2042-12', '2043-01', '2043-02', '2043-03', '2043-04', '2043-05', '2043-06', '2043-07', '2043-08', '2043-09', '2043-10', '2043-11', '2043-12', '2044-01', '2044-02', '2044-03', '2044-04', '2044-05', '2044-06', '2044-07', '2044-08', '2044-09', '2044-10', '2044-11', '2044-12', '2045-01', '2045-02', '2045-03', '2045-04', '2045-05', '2045-06', '2045-07', '2045-08', '2045-09', '2045-10', '2045-11', '2045-12', '2046-01', '2046-02', '2046-03', '2046-04', '2046-05', '2046-06', '2046-07', '2046-08', '2046-09', '2046-10', '2046-11', '2046-12', '2047-01', '2047-02', '2047-03', '2047-05', '2047-06', '2047-07', '2047-08', '2047-09', '2047-10', '2047-11', '2047-12', '2048-01', '2048-02', '2048-03', '2048-04', '2048-05', '2048-06', '2048-07', '2048-08', '2048-09', '2048-10', '2048-11', '2048-12', '2049-01', '2049-02', '2049-03', '2049-04', '2049-05', '2049-06', '2049-07', '2049-08', '2049-09', '2049-10', '2049-11', '2049-12', '2050-01', '2050-02', '2050-03', '2050-04', '2050-05', '2050-06', '2050-07', '2050-08', '2050-09', '2050-10', '2050-11', '2050-12', '2051-01', '2051-02', '2051-03', '2051-04', '2051-05', '2051-06', '2051-07', '2051-08', '2051-09', '2051-10', '2051-11', '2051-12', '2052-01', '2052-03', '2052-04', '2052-05', '2052-06', '2052-07', '2052-08', '2052-09', '2052-10', '2052-11', '2052-12', '2053-01', '2053-02', '2053-03', '2053-04', '2053-05', '2053-06', '2053-07', '2053-08', '2053-09', '2053-10', '2053-11', '2053-12', '2054-01', '2054-02', '2054-03', '2054-04', '2054-05', '2054-06', '2054-07', '2054-08', '2054-09', '2054-10', '2054-11', '2054-12', '2055-01', '2055-02', '2055-03', '2055-04', '2055-05', '2055-06', '2055-07', '2055-08', '2055-09', '2055-10', '2055-11', '2055-12', '2056-01', '2056-02', '2056-03', '2056-04', '2056-05', '2056-06', '2056-07', '2056-08', '2056-10', '2056-11', '2056-12', '2057-01', '2057-02', '2057-03', '2057-04', '2057-05', '2057-06', '2057-07', '2057-08', '2057-09', '2057-10', '2057-11', '2057-12', '2058-01', '2058-02', '2058-03', '2058-04', '2058-05', '2058-06', '2058-07', '2058-08', '2058-09', '2058-10', '2058-11', '2058-12', '2059-01', '2059-02', '2059-03', '2059-04', '2059-05', '2059-06', '2059-07', '2059-08', '2059-09', '2059-10', '2059-11', '2059-12', '2060-01', '2060-02', '2060-03', '2060-04', '2060-05', '2060-06', '2060-07', '2060-08', '2060-09', '2060-10', '2060-11', '2060-12', '2061-01', '2061-03', '2061-04', '2061-05', '2061-06', '2061-07', '2061-08', '2061-09', '2061-10', '2061-11', '2061-12', '2062-01', '2062-02', '2062-03', '2062-04', '2062-05', '2062-06', '2062-07', '2062-08', '2062-09', '2062-10', '2062-11', '2062-12', '2063-01', '2063-02', '2063-03', '2063-04', '2063-05', '2063-06', '2063-07', '2063-08', '2063-09', '2063-10', '2063-11', '2063-12', '2064-01', '2064-02', '2064-03', '2064-04', '2064-05', '2064-06', '2064-07', '2064-08', '2064-09', '2064-10', '2064-11', '2064-12', '2065-01', '2065-02', '2065-03', '2065-04', '2065-05', '2065-06', '2065-07', '2065-08', '2065-09', '2065-10', '2065-12', '2066-01', '2066-02', '2066-03', '2066-04', '2066-05', '2066-06', '2066-07', '2066-08', '2066-09', '2066-10', '2066-11', '2066-12', '2067-01', '2067-02', '2067-03', '2067-04', '2067-05', '2067-06', '2067-07', '2067-08', '2067-09', '2067-10', '2067-11', '2067-12', '2068-01', '2068-02', '2068-03', '2068-04', '2068-05', '2068-06', '2068-07', '2068-08', '2068-09', '2068-10', '2068-11', '2068-12', '2069-01', '2069-02', '2069-03', '2069-04', '2069-05', '2069-06', '2069-07', '2069-08', '2069-09', '2069-10', '2069-11', '2069-12', '2070-01', '2070-02', '2070-03', '2070-05', '2070-06', '2070-07', '2070-08', '2070-09', '2070-10', '2070-11', '2070-12', '2071-01', '2071-02', '2071-03', '2071-04', '2071-05', '2071-06', '2071-07', '2071-08', '2071-09', '2071-10', '2071-11', '2071-12', '2072-01', '2072-02', '2072-03', '2072-04', '2072-05', '2072-06', '2072-07', '2072-08', '2072-09', '2072-10', '2072-11', '2072-12', '2073-01', '2073-02', '2073-03', '2073-04', '2073-05', '2073-06', '2073-07', '2073-08', '2073-09', '2073-10', '2073-11', '2073-12', '2074-01', '2074-02', '2074-03', '2074-04', '2074-05', '2074-06', '2074-07', '2074-08', '2074-09', '2074-10', '2074-11', '2074-12', '2075-01', '2075-03', '2075-04', '2075-05', '2075-06', '2075-07', '2075-08', '2075-09', '2075-10', '2075-11', '2075-12', '2076-01', '2076-02', '2076-03', '2076-04', '2076-05', '2076-06', '2076-07', '2076-08', '2076-09', '2076-10', '2076-11', '2076-12', '2077-01', '2077-02', '2077-03', '2077-04', '2077-05', '2077-06', '2077-07', '2077-08', '2077-09', '2077-10', '2077-11', '2077-12', '2078-01', '2078-02', '2078-03', '2078-04', '2078-05', '2078-06', '2078-07', '2078-08', '2078-09', '2078-10', '2078-11', '2078-12', '2079-01', '2079-02', '2079-03', '2079-04', '2079-05', '2079-07', '2079-08', '2079-09', '2079-10', '2079-11', '2079-12']\n",
      "['12h', '15m', '1d', '1h', '1m', '1mo', '1w', '2h', '30m', '3d', '3m', '4h', '5m', '6h', '8h']\n",
      "Symbol,month and chart arrays are successfully created\n"
     ]
    }
   ],
   "source": [
    "%run ./creating_arrays.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f291bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T13:21:04.846841Z",
     "start_time": "2023-11-06T13:21:04.359617Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import talib\n",
    "import datetime\n",
    "import glob\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import inspect\n",
    "import talib\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1fe5526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:54:53.458934Z",
     "start_time": "2023-10-02T10:54:53.441910Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_monthly_data(month_array, symbol, chart_time):\n",
    "    #downloading monthly data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(\n",
    "        download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for month in month_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print (f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print (f\"you're already up to date for monthly data for {symbol},{chart_time}\")\n",
    "    \n",
    "\n",
    "    \n",
    "def download_daily_data(day_array, symbol, chart_time):\n",
    "    #downloading daily data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(\n",
    "        download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for day in day_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print(f\"you're already up to date for daily data for {symbol},{chart_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d27f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:55:10.071104Z",
     "start_time": "2023-10-02T10:55:10.045596Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_data_and_concatenate(\n",
    "        master_dictionary,\n",
    "        win_perc=master_dictionary[\"win_percentage\"],\n",
    "        loss_perc=master_dictionary[\"loss_percentage\"]):\n",
    "    # Set the start date and end date\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    end_date = datetime(2080, 1, 1)\n",
    "\n",
    "    # Initialize the list of year-month pairs\n",
    "    MONTH_ARRAY = []\n",
    "\n",
    "    # Iterate over the dates from start to end\n",
    "    date = start_date\n",
    "    while date < end_date:\n",
    "        # Format the date as a year-month pair\n",
    "        year_month = date.strftime(\"%Y-%m\")\n",
    "        #\n",
    "        # Add the year-month pair to the list\n",
    "        MONTH_ARRAY.append(year_month)\n",
    "\n",
    "        # Increment the date by one month\n",
    "        date += timedelta(days=31)\n",
    "    # Get the current date\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Format the date as a string in the desired format\n",
    "    date_string = now.strftime(\"%Y-%m\")\n",
    "\n",
    "    idx = MONTH_ARRAY.index(date_string)\n",
    "    MONTH_ARRAY = MONTH_ARRAY[:idx + 1]\n",
    "\n",
    "    # Get the current date\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Get the first day of the current month\n",
    "    first_day = now.replace(day=1)\n",
    "\n",
    "    # Create an empty list to store the days\n",
    "    DAY_ARRAY = []\n",
    "\n",
    "    # Loop through the days from the first day of the current month to today\n",
    "    while first_day <= now:\n",
    "        # Format the date as a string in the desired format\n",
    "        date_string = first_day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Add the date string to the list\n",
    "        DAY_ARRAY.append(date_string)\n",
    "\n",
    "        # Move to the next day\n",
    "        first_day += timedelta(days=1)\n",
    "   \n",
    "\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"setting up things for {symbol},{chart_time}\")\n",
    "\n",
    "            # Set up an empty list for the data frames\n",
    "            df_list = []\n",
    "\n",
    "            # Compile the regular expression pattern\n",
    "            pattern = re.compile(\n",
    "                f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Compile the regular expression pattern for daily zip files\n",
    "            pattern_daily = re.compile(\n",
    "                f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Create the new folder path for daily ZIP files\n",
    "            new_daily_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-daily_data\")\n",
    "\n",
    "            # Create the new folder path for ZIP files\n",
    "            new_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "            # Create the new folder path for CSV files\n",
    "            new_csv_folder_path = os.path.join(output_dir,\n",
    "                                               f\"{symbol}-{chart_time}\")\n",
    "\n",
    "            # Set the file name\n",
    "            concatenated_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "\n",
    "            # Construct the file path\n",
    "            concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                                  concatenated_file_name)\n",
    "\n",
    "            if not Path(concatenated_file_path).exists():\n",
    "                print(\n",
    "                    f\"setting up things for the new symbol-chart_time combination : {symbol}{chart_time}\"\n",
    "                )\n",
    "                download_monthly_data(month_array=MONTH_ARRAY,\n",
    "                                      symbol=symbol,\n",
    "                                      chart_time=chart_time)\n",
    "                download_daily_data(day_array=DAY_ARRAY,\n",
    "                                    symbol=symbol,\n",
    "                                    chart_time=chart_time)\n",
    "\n",
    "                # Iterate over the files in the new zip folder\n",
    "                for file in os.listdir(new_zip_folder_path):\n",
    "                    # Check if the file matches the pattern\n",
    "                    if pattern.match(file):\n",
    "                        # Construct the file path\n",
    "                        file_path = os.path.join(new_zip_folder_path, file)\n",
    "\n",
    "                        # Extract the ZIP file\n",
    "                        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                            zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "                        # Construct the CSV file path\n",
    "                        csv_file_path = os.path.join(\n",
    "                            new_csv_folder_path,\n",
    "                            f\"{symbol}-{chart_time}{file[-12:-4]}.csv\")\n",
    "\n",
    "                        # Read the CSV file into a data frame, ignoring the headers\n",
    "                        df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "                        # Remove the first row (which contains the header)\n",
    "                        df = df.iloc[1:]\n",
    "\n",
    "                        # Add it to the list\n",
    "                        df_list.append(df)\n",
    "\n",
    "                # Iterate over the files in the new daily zip folder\n",
    "                for file in os.listdir(new_daily_zip_folder_path):\n",
    "                    # Check if the file matches the pattern\n",
    "                    if pattern_daily.match(file):\n",
    "                        # Construct the file path\n",
    "                        file_path = os.path.join(new_daily_zip_folder_path,\n",
    "                                                 file)\n",
    "\n",
    "                        # Extract the ZIP file\n",
    "                        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                            zip_ref.extractall(new_csv_folder_path)\n",
    "                        # Construct the CSV file path\n",
    "                        csv_file_path = os.path.join(\n",
    "                            new_csv_folder_path,\n",
    "                            f\"{symbol}-{chart_time}-{file.split('-')[-3]}-{file.split('-')[-2]}-{file.split('-')[-1][:-4]}.csv\"\n",
    "                        )\n",
    "                        # Read the CSV file into a data frame, ignoring the headers\n",
    "                        df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "                        # Remove the first row (which contains the header)\n",
    "                        df = df.iloc[1:]\n",
    "\n",
    "                        # Add it to the list\n",
    "                        df_list.append(df)\n",
    "                # Concatenate the data frames in the list\n",
    "                df_final = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "                # Read the headers from the first CSV file\n",
    "                headers = pd.read_csv(csv_file_path, nrows=1).columns\n",
    "\n",
    "                # Set the headers as the column names of the final dataframe\n",
    "                df_final.columns = headers\n",
    "\n",
    "                # Convert 'open_time' and 'close_time' columns to datetime\n",
    "                df_final['open_time'] = pd.to_datetime(\n",
    "                    df_final['open_time'], unit='ms').dt.tz_localize(\n",
    "                        'UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "                # Delete the 'ignore' column\n",
    "                df_final = df_final.drop(['ignore', 'close_time'], axis=1)\n",
    "\n",
    "                # Add a new column called 'entry' that will take open of candle\n",
    "                df_final['entry'] = df_final['open']\n",
    "\n",
    "                # Write the data frame to the Excel file\n",
    "                df_final.to_csv(concatenated_file_path, index=False)\n",
    "\n",
    "                directory_final = Path(\n",
    "                    concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "                # deleting all the other csvs\n",
    "                for file_path in directory_final.iterdir():\n",
    "                    if file_path != Path(concatenated_file_path):\n",
    "                        if file_path.is_file():\n",
    "                            file_path.unlink()\n",
    "                return f\"things set up for the new symbol-chart_time combination : {symbol}{chart_time}\"\n",
    "            else:\n",
    "                print(\n",
    "                    f\"setting up things only for incremental data for {symbol}{chart_time}\"\n",
    "                )\n",
    "                df_list = []\n",
    "                df = pd.read_csv(concatenated_file_path, header=None)\n",
    "                last_record = df.iloc[-1:]\n",
    "                df_list.append(last_record)\n",
    "\n",
    "                df = pd.read_csv(concatenated_file_path)\n",
    "                df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "                last_record = df.iloc[-1:]\n",
    "\n",
    "                def calculate_reference_time(chart_time, base_time):\n",
    "                    # Extract the numeric value and unit from chart_time\n",
    "                    numeric_value = int(chart_time[:-1])\n",
    "                    unit = chart_time[-1]\n",
    "\n",
    "                    # Define a dictionary to map units to timedelta objects\n",
    "                    time_units = {\n",
    "                        'm': timedelta(minutes=numeric_value),\n",
    "                        'h': timedelta(hours=numeric_value),\n",
    "                    }\n",
    "\n",
    "                    # Calculate the reference time based on the unit and numeric value\n",
    "                    reference_time = base_time - time_units[unit]\n",
    "                    return reference_time.strftime('%H:%M:%S%z')\n",
    "\n",
    "                base_time = datetime.strptime(\"05:30:00+0530\", '%H:%M:%S%z')\n",
    "                reference_time = calculate_reference_time(\n",
    "                    chart_time, base_time)\n",
    "                print(reference_time)\n",
    "                # Check if the time part of 'open_time' is \"05:15:00+05:30\"\n",
    "                if last_record['open_time'].dt.strftime(\n",
    "                        '%H:%M:%S%z').item() == reference_time:\n",
    "                    # Calculate the last processed date by subtracting 1 day from the date part\n",
    "                    last_processed_date = (\n",
    "                        last_record['open_time'].dt.date -\n",
    "                        pd.DateOffset(days=1)).item().strftime('%Y-%m-%d')\n",
    "                    last_processed_date = datetime.strptime(\n",
    "                        last_processed_date, \"%Y-%m-%d\")\n",
    "                    print(last_processed_date)\n",
    "                    # Get today's date\n",
    "                    today = datetime.today()\n",
    "                    print(today)\n",
    "\n",
    "                    # Initialize DAY_ARRAY\n",
    "                    DAY_ARRAY = []\n",
    "                    # Start from the day after last_processed_date\n",
    "                    current_day = last_processed_date + timedelta(days=1)\n",
    "\n",
    "                    while current_day <= today:\n",
    "                        # Format the date as a string in the desired format\n",
    "                        date_string = current_day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                        # Add the date string to DAY_ARRAY\n",
    "                        DAY_ARRAY.append(date_string)\n",
    "\n",
    "                        # Move to the next day\n",
    "                        current_day += timedelta(days=1)\n",
    "\n",
    "                    download_daily_data(day_array=DAY_ARRAY,\n",
    "                                        symbol=symbol,\n",
    "                                        chart_time=chart_time)\n",
    "\n",
    "                    # Iterate over the files in the new daily zip folder\n",
    "                    for file in Path(new_daily_zip_folder_path).iterdir():\n",
    "                        # Check if the file matches the pattern\n",
    "                        if pattern_daily.match(file.name):\n",
    "                            # Extract the date part from the file name (e.g., \"2023-08-19\")\n",
    "                            zip_date = file.name.split('-')[-3:]\n",
    "                            zip_date_str = '-'.join(zip_date).replace(\n",
    "                                '.zip', '')\n",
    "\n",
    "                            # Check if the date is in DAY_ARRAY\n",
    "                            if zip_date_str in DAY_ARRAY:\n",
    "                                # Construct the file path\n",
    "                                file_path = file\n",
    "\n",
    "                                # Extract the ZIP file\n",
    "                                with zipfile.ZipFile(file_path,\n",
    "                                                     \"r\") as zip_ref:\n",
    "                                    zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "                                # Construct the CSV file path\n",
    "                                csv_file_path = Path(\n",
    "                                    new_csv_folder_path\n",
    "                                ) / f\"{symbol}-{chart_time}-{zip_date_str}.csv\"\n",
    "\n",
    "                                # Read the CSV file into a data frame, ignoring the headers\n",
    "                                df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "                                # Remove the first row (which contains the header)\n",
    "                                df = df.iloc[1:]\n",
    "\n",
    "                                # Add it to the list\n",
    "                                df_list.append(df)\n",
    "                    for df in df_list[1:]:\n",
    "                        # Convert 'open_time' and 'close_time' columns to datetime\n",
    "                        df[0] = pd.to_datetime(\n",
    "                            df[0], unit='ms').dt.tz_localize(\n",
    "                                'UTC').dt.tz_convert('Asia/Kolkata')\n",
    "                        cols_to_drop = [6, 11]\n",
    "                        df.drop(df.columns[cols_to_drop], axis=1, inplace=True)\n",
    "    # Concatenate the data frames in the list\n",
    "                    if len(df_list) == 1:\n",
    "                        return \"no need to do anything, you're already having all the latest data\"\n",
    "                    else:\n",
    "                        df_final = pd.concat(df_list[1:], ignore_index=True)\n",
    "                        display(df_final.head())\n",
    "                        # Read the headers from the first CSV file\n",
    "                        headers = pd.read_csv(concatenated_file_path,\n",
    "                                              nrows=1).columns\n",
    "\n",
    "                        # Set the headers as the column names of the final dataframe\n",
    "                        df_final.columns = headers[0:len(headers) - 1]\n",
    "\n",
    "                        # Add a new column called 'entry' that will take open of candle\n",
    "                        df_final['entry'] = df_final['open']\n",
    "                        display(df_final.head())\n",
    "                        # Append the data frame to the existing CSV file in append mode without headers\n",
    "                        df_final.to_csv(concatenated_file_path,\n",
    "                                        mode='a',\n",
    "                                        header=False,\n",
    "                                        index=False)\n",
    "                        directory_final = Path(\n",
    "                            concatenated_file_path\n",
    "                        ).parent  # Get the parent directory\n",
    "\n",
    "                        # deleting all the other csvs\n",
    "                        for file_path in directory_final.iterdir():\n",
    "                            if file_path != Path(concatenated_file_path):\n",
    "                                if file_path.is_file():\n",
    "                                    file_path.unlink()\n",
    "                        return f\"things set up only for incremental data for {symbol}{chart_time}\"\n",
    "\n",
    "                else:\n",
    "                    return f\"Something is wrong with data. Last recorded time is not in sync with what is expected. Expected {reference_time} but got {last_record['open_time'].dt.strftime('%H:%M:%S%z').item()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c33c9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:55:10.646549Z",
     "start_time": "2023-10-02T10:55:10.628796Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wins_losses(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"calculating for {symbol}{chart_time}\")\n",
    "            # Construct the file name\n",
    "            file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{file_name}\"\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            df[\"if_short\"] = 0\n",
    "            df[\"if_long\"] = 0\n",
    "            df[\"long_target\"] = np.nan\n",
    "            df[\"short_target\"] = np.nan\n",
    "            df[\"long_stop_loss\"] = np.nan\n",
    "            df[\"short_stop_loss\"] = np.nan\n",
    "            df[\"shorts_win_after\"] = np.nan\n",
    "            df[\"longs_win_after\"] = np.nan\n",
    "            df[\"dual_loss\"] = 0\n",
    "            df[\"entered_before\"] = np.nan\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                if df[\"entry\"][i]:\n",
    "                    long_target = df[\"entry\"][i] * (1 + win_perc / 100)\n",
    "                    short_target = df[\"entry\"][i] * (1 - win_perc / 100)\n",
    "                    long_stop_loss = df[\"entry\"][i] * (1 - loss_perc / 100)\n",
    "                    short_stop_loss = df[\"entry\"][i] * (1 + loss_perc / 100)\n",
    "                    df.loc[i, 'long_target'] = long_target\n",
    "                    df.loc[i, 'long_stop_loss'] = long_stop_loss\n",
    "                    for j in range(i, len(df)):\n",
    "                        if df[\"high\"][j] >= long_target:\n",
    "                            if df[\"low\"][j] <= long_stop_loss:\n",
    "                                df.loc[i, 'if_long'] = -1\n",
    "                                df.loc[i, 'dual_loss'] = 1\n",
    "                                df.loc[i, 'entered_before'] = j - i\n",
    "                            else:\n",
    "                                df.loc[i, 'if_long'] = 1\n",
    "                                df.loc[i, 'longs_win_after'] = j - i\n",
    "                            break\n",
    "                        elif df[\"low\"][j] <= long_stop_loss:\n",
    "                            df.loc[i, 'if_long'] = -1\n",
    "                            break\n",
    "                    df.loc[i, 'short_target'] = short_target\n",
    "                    df.loc[i, 'short_stop_loss'] = short_stop_loss\n",
    "                    for j in range(i, len(df)):\n",
    "                        if df[\"low\"][j] <= short_target:\n",
    "                            if df[\"high\"][j] >= short_stop_loss:\n",
    "                                df.loc[i, 'if_short'] = -1\n",
    "                                df.loc[i, 'dual_loss'] = 1\n",
    "                                df.loc[i, 'entered_before'] = j - i\n",
    "                            else:\n",
    "                                df.loc[i, 'if_short'] = 1\n",
    "                                df.loc[i, 'shorts_win_after'] = j - i\n",
    "                            break\n",
    "                        elif df[\"high\"][j] >= short_stop_loss:\n",
    "                            df.loc[i, 'if_short'] = -1\n",
    "                            break\n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "    return (\"calculated wins and losses \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indicator_values_new(master_dictionary, monthly_or_daily_1_2, win_perc, loss_perc):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            # Construct the file name\n",
    "            file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{file_name}\"\n",
    "\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(df.dtypes)\n",
    "            #########Overlap Studies\n",
    "            new_columns = pd.DataFrame()\n",
    "            new_columns['HT_TRENDLINE'] = talib.HT_TRENDLINE(df['close'])\n",
    "            #         df['MAMA'], df['FAMA'] = talib.MAMA(df['close'], fastlimit=0, slowlimit=0)\n",
    "            #         df['MAVP'] = talib.MAVP(df['close'], periods=None, minperiod=2, maxperiod=30, matype=0)\n",
    "            new_columns['SAR'] = talib.SAR(df['high'], df['low'], acceleration=0, maximum=0)\n",
    "            new_columns['SAREXT'] = talib.SAREXT(df['high'], df['low'])\n",
    "            new_columns['T3'] = talib.T3(df['close'], timeperiod=5, vfactor=0)\n",
    "#             #########Momentum Indicators\n",
    "            new_columns['APO'] = talib.APO(df['close'], fastperiod=12, slowperiod=26)\n",
    "            new_columns['BOP'] = talib.BOP(df['open'], df['high'], df['low'], df['close'])\n",
    "            macd, macd_signal, macd_hist = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "            new_columns['MACD'] = macd\n",
    "            new_columns['MACD_signal'] = macd_signal\n",
    "            new_columns['MACD_hist'] = macd_hist\n",
    "            new_columns['PPO'] = talib.PPO(df['close'], fastperiod=12, slowperiod=26, matype=0)\n",
    "            new_columns['TRIX'] = talib.TRIX(df['close'])\n",
    "            new_columns['ULTOSC'] = talib.ULTOSC(df['high'], df['low'], df['close'])\n",
    "            new_columns['WILLR'] = talib.WILLR(df['high'], df['low'], df['close'])\n",
    "\n",
    "            \n",
    "\n",
    "            # Not Working ATM\n",
    "            #         new_columns['STOCH'] = talib.STOCH(df['high'], df['low'], df['close'])\n",
    "            #         new_columns['STOCHF'] = talib.STOCHF(df['high'], df['low'], df['close'])\n",
    "            #         new_columns['STOCHRSI'] = talib.STOCHRSI(df['close'])\n",
    "            #         new_columns['MACDEXT'] = talib.MACDEXT(df['close'], fastperiod=12, fastmatype=0, slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)\n",
    "            #         new_columns['MACDFIX'] = talib.MACDFIX(df['close'], signalperiod=9)\n",
    "            #########Volume Indicators\n",
    "            new_columns['AD'] = talib.AD(df['high'], df['low'], df['close'], df['volume'])\n",
    "            new_columns['ADOSC'] = talib.ADOSC(df['high'], df['low'], df['close'], df['volume'], fastperiod=3, slowperiod=10)\n",
    "            new_columns['OBV'] = talib.OBV(df['close'], df['volume'])\n",
    "\n",
    "            #########Cycle Indicators\n",
    "            new_columns['HT_DCPERIOD'] = talib.HT_DCPERIOD(df['close'])\n",
    "            new_columns['HT_DCPHASE'] = talib.HT_DCPHASE(df['close'])\n",
    "            new_columns['HT_PHASOR_inphase'], new_columns['HT_PHASOR_quadrature'] = talib.HT_PHASOR(df['close'])\n",
    "            # new_columns['HT_SINE'] = talib.HT_SINE(df['close'])\n",
    "            new_columns['HT_TRENDMODE'] = talib.HT_TRENDMODE(df['close'])\n",
    "\n",
    "            #########Price Transform\n",
    "            new_columns['AVGPRICE'] = talib.AVGPRICE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['MEDPRICE'] = talib.MEDPRICE(df['high'], df['low'])\n",
    "            new_columns['TYPPRICE'] = talib.TYPPRICE(df['high'], df['low'], df['close'])\n",
    "            new_columns['WCLPRICE'] = talib.WCLPRICE(df['high'], df['low'], df['close'])\n",
    "            #########Volatility Indicators\n",
    "            new_columns['TRANGE'] = talib.TRANGE(df['high'], df['low'], df['close'])\n",
    "            #########Pattern Recognition\n",
    "            new_columns['CDL2CROWS'] = talib.CDL2CROWS(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDL3BLACKCROWS'] = talib.CDL3BLACKCROWS(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDL3INSIDE'] = talib.CDL3INSIDE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDL3LINESTRIKE'] = talib.CDL3LINESTRIKE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDL3OUTSIDE'] = talib.CDL3OUTSIDE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDL3STARSINSOUTH'] = talib.CDL3STARSINSOUTH(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDL3WHITESOLDIERS'] = talib.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLABANDONEDBABY'] = talib.CDLABANDONEDBABY(df['open'], df['high'], df['low'], df['close'], penetration=0)\n",
    "\n",
    "            new_columns['CDLADVANCEBLOCK'] = talib.CDLADVANCEBLOCK(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLBELTHOLD'] = talib.CDLBELTHOLD(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLBREAKAWAY'] = talib.CDLBREAKAWAY(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLCLOSINGMARUBOZU'] = talib.CDLCLOSINGMARUBOZU(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLCONCEALBABYSWALL'] = talib.CDLCONCEALBABYSWALL(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLCOUNTERATTACK'] = talib.CDLCOUNTERATTACK(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLDARKCLOUDCOVER'] = talib.CDLDARKCLOUDCOVER(df['open'], df['high'], df['low'], df['close'], penetration=0)\n",
    "\n",
    "            new_columns['CDLDOJI'] = talib.CDLDOJI(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLDOJISTAR'] = talib.CDLDOJISTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLDRAGONFLYDOJI'] = talib.CDLDRAGONFLYDOJI(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLENGULFING'] = talib.CDLENGULFING(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLEVENINGDOJISTAR'] = talib.CDLEVENINGDOJISTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)\n",
    "\n",
    "            new_columns['CDLEVENINGSTAR'] = talib.CDLEVENINGSTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)\n",
    "            new_columns['CDLGAPSIDESIDEWHITE'] = talib.CDLGAPSIDESIDEWHITE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLGRAVESTONEDOJI'] = talib.CDLGRAVESTONEDOJI(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLHAMMER'] = talib.CDLHAMMER(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLHANGINGMAN'] = talib.CDLHANGINGMAN(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLHARAMI'] = talib.CDLHARAMI(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLHARAMICROSS'] = talib.CDLHARAMICROSS(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLHIGHWAVE'] = talib.CDLHIGHWAVE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLHIKKAKE'] = talib.CDLHIKKAKE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLHIKKAKEMOD'] = talib.CDLHIKKAKEMOD(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLHOMINGPIGEON'] = talib.CDLHOMINGPIGEON(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLIDENTICAL3CROWS'] = talib.CDLIDENTICAL3CROWS(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLINNECK'] = talib.CDLINNECK(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLINVERTEDHAMMER'] = talib.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLKICKING'] = talib.CDLKICKING(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLKICKINGBYLENGTH'] = talib.CDLKICKINGBYLENGTH(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLLADDERBOTTOM'] = talib.CDLLADDERBOTTOM(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLLONGLEGGEDDOJI'] = talib.CDLLONGLEGGEDDOJI(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLLONGLINE'] = talib.CDLLONGLINE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLMARUBOZU'] = talib.CDLMARUBOZU(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLMATCHINGLOW'] = talib.CDLMATCHINGLOW(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLMATHOLD'] = talib.CDLMATHOLD(df['open'], df['high'], df['low'], df['close'], penetration=0)\n",
    "            new_columns['CDLMORNINGDOJISTAR'] = talib.CDLMORNINGDOJISTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLMORNINGSTAR'] = talib.CDLMORNINGSTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLONNECK'] = talib.CDLONNECK(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLPIERCING'] = talib.CDLPIERCING(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLRICKSHAWMAN'] = talib.CDLRICKSHAWMAN(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLRISEFALL3METHODS'] = talib.CDLRISEFALL3METHODS(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLSEPARATINGLINES'] = talib.CDLSEPARATINGLINES(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLSHOOTINGSTAR'] = talib.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLSHORTLINE'] = talib.CDLSHORTLINE(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLSPINNINGTOP'] = talib.CDLSPINNINGTOP(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLSTALLEDPATTERN'] = talib.CDLSTALLEDPATTERN(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLSTICKSANDWICH'] = talib.CDLSTICKSANDWICH(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLTAKURI'] = talib.CDLTAKURI(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLTASUKIGAP'] = talib.CDLTASUKIGAP(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLTHRUSTING'] = talib.CDLTHRUSTING(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLTRISTAR'] = talib.CDLTRISTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLUNIQUE3RIVER'] = talib.CDLUNIQUE3RIVER(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLUPSIDEGAP2CROWS'] = talib.CDLUPSIDEGAP2CROWS(df['open'], df['high'], df['low'], df['close'])\n",
    "            new_columns['CDLXSIDEGAP3METHODS'] = talib.CDLXSIDEGAP3METHODS(df['open'], df['high'], df['low'], df['close'])\n",
    "            #########Statistic Functions\n",
    "            new_columns['LINEARREG'] = talib.LINEARREG(df['close'])\n",
    "            new_columns['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(df['close'])\n",
    "            new_columns['LINEARREG_INTERCEPT'] = talib.LINEARREG_INTERCEPT(df['close'])\n",
    "            new_columns['LINEARREG_SLOPE'] = talib.LINEARREG_SLOPE(df['close'])\n",
    "            #         new_columns['STDDEV'] = df['close'].rolling(timeperiod).std()\n",
    "            new_columns['TSF'] = talib.TSF(df['close'])\n",
    "            new_columns['VAR'] = talib.VAR(df['close'])\n",
    "            # Iterate over the time periods\n",
    "            for timeperiod in master_dictionary[\"timeperiods\"]:\n",
    "                #########Overlap Studies\n",
    "                new_columns[f'BB_upper_{timeperiod}'], new_columns[f'BB_middle_{timeperiod}'], new_columns[\n",
    "                    f'BB_lower_{timeperiod}'] = talib.BBANDS(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'DEMA_{timeperiod}'] = talib.DEMA(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'EMA_{timeperiod}'] = talib.EMA(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'KAMA_{timeperiod}'] = talib.KAMA(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'MA_{timeperiod}'] = talib.MA(df['close'], timeperiod=timeperiod)\n",
    "                #         new_columns['MAMA'], new_columns['FAMA'] = talib.MAMA(df['close'], fastlimit=0, slowlimit=0)\n",
    "                #         new_columns['MAVP'] = talib.MAVP(df['close'], periods=None, minperiod=2, maxperiod=30, matype=0)\n",
    "                new_columns[f'MIDPOINT_{timeperiod}'] = talib.MIDPOINT(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'MIDPRICE_{timeperiod}'] = talib.MIDPRICE(df['high'], df['low'], timeperiod=timeperiod)\n",
    "                new_columns[f'SMA_{timeperiod}'] = talib.SMA(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'TEMA_{timeperiod}'] = talib.TEMA(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'TRIMA_{timeperiod}'] = talib.TRIMA(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'WMA_{timeperiod}'] = talib.WMA(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'ADX_{timeperiod}'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'ADXR_{timeperiod}'] = talib.ADXR(df['high'], df['low'], df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'AROON_up_{timeperiod}'], new_columns[f'AROON_down_{timeperiod}'] = talib.AROON(df['high'], df['low'],\n",
    "                                                                                           timeperiod=timeperiod)\n",
    "                new_columns[f'AROONOSC_{timeperiod}'] = talib.AROONOSC(df['high'], df['low'], timeperiod=timeperiod)\n",
    "                new_columns[f'CCI_{timeperiod}'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'CMO_{timeperiod}'] = talib.CMO(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'DX_{timeperiod}'] = talib.DX(df['high'], df['low'], df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'MFI_{timeperiod}'] = talib.MFI(df['high'], df['low'], df['close'], df['volume'],\n",
    "                                                    timeperiod=timeperiod)\n",
    "                new_columns[f'MINUS_DI_{timeperiod}'] = talib.MINUS_DI(df['high'], df['low'], df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'MINUS_DM_{timeperiod}'] = talib.MINUS_DM(df['high'], df['low'], timeperiod=timeperiod)\n",
    "                new_columns[f'MOM_{timeperiod}'] = talib.MOM(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'PLUS_DI_{timeperiod}'] = talib.PLUS_DI(df['high'], df['low'], df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'PLUS_DM_{timeperiod}'] = talib.PLUS_DM(df['high'], df['low'], timeperiod=timeperiod)\n",
    "                new_columns[f'ROC_{timeperiod}'] = talib.ROC(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'ROCP_{timeperiod}'] = talib.ROCP(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'ROCR_{timeperiod}'] = talib.ROCR(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'ROCR100_{timeperiod}'] = talib.ROCR100(df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'RSI_{timeperiod}'] = talib.RSI(df['close'], timeperiod=timeperiod)\n",
    "\n",
    "                new_columns[f'ATR_{timeperiod}'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=timeperiod)\n",
    "                new_columns[f'NATR_{timeperiod}'] = talib.NATR(df['high'], df['low'], df['close'], timeperiod=timeperiod)\n",
    "                #########Statistic Functions\n",
    "                new_columns[f'BETA_{timeperiod}'] = talib.BETA(df['high'], df['low'], timeperiod=timeperiod)\n",
    "                new_columns[f'CORREL_{timeperiod}'] = talib.CORREL(df['high'], df['low'], timeperiod=timeperiod)\n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df = pd.concat([df, new_columns], axis=1)\n",
    "            df.to_csv(file_path, index=False)\n",
    "    return (\"indicators are added to the csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7494dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_indicator_values_old(master_dictionary, monthly_or_daily_1_2):\n",
    "#     # Iterate over the symbols and chart times\n",
    "#     for symbol in master_dictionary[\"symbols\"]:\n",
    "#         for chart_time in master_dictionary[\"chart_times\"]:\n",
    "#             # Construct the file name\n",
    "#             file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "#             if monthly_or_daily_1_2 in [\"m\", \"M\", \"monthly\", 1]:\n",
    "#                 # Construct the file path\n",
    "#                 file_path = Path(output_dir) / f\"{symbol}-{chart_time}-monthly_data/{file_name}\"\n",
    "#             elif monthly_or_daily_1_2 in [\"d\", \"D\", \"daily\", 2]:\n",
    "#                 # Construct the file path\n",
    "#                 file_path = Path(output_dir) / f\"{symbol}-{chart_time}-daily_data/{file_name}\"\n",
    "#             # Read the CSV file into a dataframe\n",
    "#             df = pd.read_csv(file_path)\n",
    "#             print(df.dtypes)\n",
    "#             #########Overlap Studies\n",
    "#             df['BB_upper'], df['BB_middle'], df['BB_lower'] = talib.BBANDS(df['close'], timeperiod=5)\n",
    "#             df['DEMA'] = talib.DEMA(df['close'], timeperiod=30)\n",
    "#             df['EMA-50'] = talib.EMA(df['close'], timeperiod=50)\n",
    "#             df['EMA-200'] = talib.EMA(df['close'], timeperiod=200)\n",
    "#             df['HT_TRENDLINE'] = talib.HT_TRENDLINE(df['close'])\n",
    "#             df['KAMA'] = talib.KAMA(df['close'], timeperiod=30)\n",
    "#             df['MA'] = talib.MA(df['close'], timeperiod=14)\n",
    "#             #         df['MAMA'], df['FAMA'] = talib.MAMA(df['close'], fastlimit=0, slowlimit=0)\n",
    "#             #         df['MAVP'] = talib.MAVP(df['close'], periods=None, minperiod=2, maxperiod=30, matype=0)\n",
    "#             df['MIDPOINT'] = talib.MIDPOINT(df['close'], timeperiod=14)\n",
    "#             df['MIDPRICE'] = talib.MIDPRICE(df['high'], df['low'], timeperiod=14)\n",
    "#             df['SAR'] = talib.SAR(df['high'], df['low'], acceleration=0, maximum=0)\n",
    "#             df['SAREXT'] = talib.SAREXT(df['high'], df['low'])\n",
    "#             df['SMA'] = talib.SMA(df['close'], timeperiod=14)\n",
    "#             df['T3'] = talib.T3(df['close'], timeperiod=5, vfactor=0)\n",
    "#             df['TEMA'] = talib.TEMA(df['close'])\n",
    "#             df['TRIMA'] = talib.TRIMA(df['close'])\n",
    "#             df['WMA'] = talib.WMA(df['close'])\n",
    "#             #########Momentum Indicators\n",
    "#             df['ADX'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "#             df['ADXR'] = talib.ADXR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "#             df['APO'] = talib.APO(df['close'], fastperiod=12, slowperiod=26)\n",
    "#             df['AROON_up'], df['AROON_down'] = talib.AROON(df['high'], df['low'], timeperiod=14)\n",
    "#             df['AROONOSC'] = talib.AROONOSC(df['high'], df['low'], timeperiod=14)\n",
    "#             df['BOP'] = talib.BOP(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CCI'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "#             df['CMO'] = talib.CMO(df['close'], timeperiod=14)\n",
    "#             df['DX'] = talib.DX(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "#             df['MACD'], df['MACD_signal'], df['MACD_hist'] = talib.MACD(df['close'], fastperiod=12, slowperiod=26,\n",
    "#                                                                         signalperiod=9)\n",
    "#             df['MFI'] = talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=14)\n",
    "#             df['MINUS_DI'] = talib.MINUS_DI(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "#             df['MINUS_DM'] = talib.MINUS_DM(df['high'], df['low'], timeperiod=14)\n",
    "#             df['MOM'] = talib.MOM(df['close'], timeperiod=14)\n",
    "#             df['PLUS_DI'] = talib.PLUS_DI(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "#             df['PLUS_DM'] = talib.PLUS_DM(df['high'], df['low'], timeperiod=14)\n",
    "#             df['PPO'] = talib.PPO(df['close'], fastperiod=12, slowperiod=26, matype=0)\n",
    "#             df['ROC'] = talib.ROC(df['close'], timeperiod=14)\n",
    "#             df['ROCP'] = talib.ROCP(df['close'], timeperiod=14)\n",
    "#             df['ROCR'] = talib.ROCR(df['close'], timeperiod=14)\n",
    "#             df['ROCR100'] = talib.ROCR100(df['close'], timeperiod=14)\n",
    "#             df['RSI'] = talib.RSI(df['close'], timeperiod=8)\n",
    "#             df['TRIX'] = talib.TRIX(df['close'])\n",
    "#             df['ULTOSC'] = talib.ULTOSC(df['high'], df['low'], df['close'])\n",
    "#             df['WILLR'] = talib.WILLR(df['high'], df['low'], df['close'])\n",
    "\n",
    "#             # Not Working ATM\n",
    "#             #         df['STOCH'] = talib.STOCH(df['high'], df['low'], df['close'])\n",
    "#             #         df['STOCHF'] = talib.STOCHF(df['high'], df['low'], df['close'])\n",
    "#             #         df['STOCHRSI'] = talib.STOCHRSI(df['close'])\n",
    "#             #         df['MACDEXT'] = talib.MACDEXT(df['close'], fastperiod=12, fastmatype=0, slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)\n",
    "#             #         df['MACDFIX'] = talib.MACDFIX(df['close'], signalperiod=9)\n",
    "#             #########Volume Indicators\n",
    "#             df['AD'] = talib.AD(df['high'], df['low'], df['close'], df['volume'])\n",
    "#             df['ADOSC'] = talib.ADOSC(df['high'], df['low'], df['close'], df['volume'], fastperiod=3, slowperiod=10)\n",
    "#             df['OBV'] = talib.OBV(df['close'], df['volume'])\n",
    "#             #########Cycle Indicators\n",
    "#             df['HT_DCPERIOD'] = talib.HT_DCPERIOD(df['close'])\n",
    "#             df['HT_DCPHASE'] = talib.HT_DCPHASE(df['close'])\n",
    "#             df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = talib.HT_PHASOR(df['close'])\n",
    "#             #         df['HT_SINE'] = talib.HT_SINE(df['close'])\n",
    "#             df['HT_TRENDMODE'] = talib.HT_TRENDMODE(df['close'])\n",
    "#             #########Price Transform\n",
    "#             df['AVGPRICE'] = talib.AVGPRICE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['MEDPRICE'] = talib.MEDPRICE(df['high'], df['low'])\n",
    "#             df['TYPPRICE'] = talib.TYPPRICE(df['high'], df['low'], df['close'])\n",
    "#             df['WCLPRICE'] = talib.WCLPRICE(df['high'], df['low'], df['close'])\n",
    "#             #########Volatility Indicators\n",
    "#             df['ATR'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "#             df['NATR'] = talib.NATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "#             df['TRANGE'] = talib.TRANGE(df['high'], df['low'], df['close'])\n",
    "#             #########Pattern Recognition\n",
    "#             df['CDL2CROWS'] = talib.CDL2CROWS(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDL3BLACKCROWS'] = talib.CDL3BLACKCROWS(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDL3INSIDE'] = talib.CDL3INSIDE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDL3LINESTRIKE'] = talib.CDL3LINESTRIKE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDL3OUTSIDE'] = talib.CDL3OUTSIDE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDL3STARSINSOUTH'] = talib.CDL3STARSINSOUTH(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDL3WHITESOLDIERS'] = talib.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLABANDONEDBABY'] = talib.CDLABANDONEDBABY(df['open'], df['high'], df['low'], df['close'],\n",
    "#                                                             penetration=0)\n",
    "#             df['CDLADVANCEBLOCK'] = talib.CDLADVANCEBLOCK(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLBELTHOLD'] = talib.CDLBELTHOLD(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLBREAKAWAY'] = talib.CDLBREAKAWAY(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLCLOSINGMARUBOZU'] = talib.CDLCLOSINGMARUBOZU(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLCONCEALBABYSWALL'] = talib.CDLCONCEALBABYSWALL(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLCOUNTERATTACK'] = talib.CDLCOUNTERATTACK(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLDARKCLOUDCOVER'] = talib.CDLDARKCLOUDCOVER(df['open'], df['high'], df['low'], df['close'],\n",
    "#                                                               penetration=0)\n",
    "#             df['CDLDOJI'] = talib.CDLDOJI(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLDOJISTAR'] = talib.CDLDOJISTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLDRAGONFLYDOJI'] = talib.CDLDRAGONFLYDOJI(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLENGULFING'] = talib.CDLENGULFING(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLEVENINGDOJISTAR'] = talib.CDLEVENINGDOJISTAR(df['open'], df['high'], df['low'], df['close'],\n",
    "#                                                                 penetration=0)\n",
    "#             df['CDLEVENINGSTAR'] = talib.CDLEVENINGSTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)\n",
    "#             df['CDLGAPSIDESIDEWHITE'] = talib.CDLGAPSIDESIDEWHITE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLGRAVESTONEDOJI'] = talib.CDLGRAVESTONEDOJI(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLHAMMER'] = talib.CDLHAMMER(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLHANGINGMAN'] = talib.CDLHANGINGMAN(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLHARAMI'] = talib.CDLHARAMI(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLHARAMICROSS'] = talib.CDLHARAMICROSS(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLHIGHWAVE'] = talib.CDLHIGHWAVE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLHIKKAKE'] = talib.CDLHIKKAKE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLHIKKAKEMOD'] = talib.CDLHIKKAKEMOD(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLHOMINGPIGEON'] = talib.CDLHOMINGPIGEON(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLIDENTICAL3CROWS'] = talib.CDLIDENTICAL3CROWS(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLINNECK'] = talib.CDLINNECK(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLINVERTEDHAMMER'] = talib.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLKICKING'] = talib.CDLKICKING(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLKICKINGBYLENGTH'] = talib.CDLKICKINGBYLENGTH(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLLADDERBOTTOM'] = talib.CDLLADDERBOTTOM(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLLONGLEGGEDDOJI'] = talib.CDLLONGLEGGEDDOJI(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLLONGLINE'] = talib.CDLLONGLINE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLMARUBOZU'] = talib.CDLMARUBOZU(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLMATCHINGLOW'] = talib.CDLMATCHINGLOW(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLMATHOLD'] = talib.CDLMATHOLD(df['open'], df['high'], df['low'], df['close'], penetration=0)\n",
    "#             df['CDLMORNINGDOJISTAR'] = talib.CDLMORNINGDOJISTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLMORNINGSTAR'] = talib.CDLMORNINGSTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLONNECK'] = talib.CDLONNECK(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLPIERCING'] = talib.CDLPIERCING(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLRICKSHAWMAN'] = talib.CDLRICKSHAWMAN(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLRISEFALL3METHODS'] = talib.CDLRISEFALL3METHODS(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLSEPARATINGLINES'] = talib.CDLSEPARATINGLINES(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLSHOOTINGSTAR'] = talib.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLSHORTLINE'] = talib.CDLSHORTLINE(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLSPINNINGTOP'] = talib.CDLSPINNINGTOP(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLSTALLEDPATTERN'] = talib.CDLSTALLEDPATTERN(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLSTICKSANDWICH'] = talib.CDLSTICKSANDWICH(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLTAKURI'] = talib.CDLTAKURI(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLTASUKIGAP'] = talib.CDLTASUKIGAP(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLTHRUSTING'] = talib.CDLTHRUSTING(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLTRISTAR'] = talib.CDLTRISTAR(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLUNIQUE3RIVER'] = talib.CDLUNIQUE3RIVER(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLUPSIDEGAP2CROWS'] = talib.CDLUPSIDEGAP2CROWS(df['open'], df['high'], df['low'], df['close'])\n",
    "#             df['CDLXSIDEGAP3METHODS'] = talib.CDLXSIDEGAP3METHODS(df['open'], df['high'], df['low'], df['close'])\n",
    "#             #########Statistic Functions\n",
    "#             df['BETA'] = talib.BETA(df['high'], df['low'], timeperiod=5)\n",
    "#             df['CORREL'] = talib.CORREL(df['high'], df['low'], timeperiod=30)\n",
    "#             df['LINEARREG'] = talib.LINEARREG(df['close'])\n",
    "#             df['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(df['close'])\n",
    "#             df['LINEARREG_INTERCEPT'] = talib.LINEARREG_INTERCEPT(df['close'])\n",
    "#             df['LINEARREG_SLOPE'] = talib.LINEARREG_SLOPE(df['close'])\n",
    "#             #         df['STDDEV'] = df['close'].rolling(timeperiod).std()\n",
    "#             df['TSF'] = talib.TSF(df['close'])\n",
    "#             df['VAR'] = talib.VAR(df['close'])\n",
    "\n",
    "#             # Save the updated dataframe to the CSV file\n",
    "#             df.to_csv(file_path, index=False)\n",
    "#     return (\"indicators are added to the csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data creation utilities successfully initialized\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
