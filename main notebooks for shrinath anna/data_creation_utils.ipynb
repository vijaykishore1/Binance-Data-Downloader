{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e4da54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T12:21:38.112708Z",
     "start_time": "2024-09-15T12:21:26.185690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\downloaded_data\n",
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\extracted_data\n",
      "['1000BONKUSDC', '1000BONKUSDT', '1000BTTCUSDT', '1000FLOKIUSDT', '1000LUNCBUSD', '1000LUNCUSDT', '1000PEPEUSDC', '1000PEPEUSDT', '1000RATSUSDT', '1000SATSUSDT', '1000SHIBBUSD', '1000SHIBUSDC', '1000SHIBUSDT', '1000XECUSDT', '1INCHUSDT', 'AAVEUSDT', 'ACEUSDT', 'ACHUSDT', 'ADABUSD', 'ADAUSDT', 'AEVOUSDT', 'AGIXBUSD', 'AGIXUSDT', 'AGLDUSDT', 'AIUSDT', 'AKROUSDT', 'ALGOUSDT', 'ALICEUSDT', 'ALPACAUSDT', 'ALPHAUSDT', 'ALTUSDT', 'AMBBUSD', 'AMBUSDT', 'ANCBUSD', 'ANCUSDT', 'ANKRUSDT', 'ANTUSDT', 'APEBUSD', 'APEUSDT', 'API3USDT', 'APTBUSD', 'APTUSDT', 'ARBUSDC', 'ARBUSDT', 'ARKMUSDT', 'ARKUSDT', 'ARPAUSDT', 'ARUSDT', 'ASTRUSDT', 'ATAUSDT', 'ATOMUSDT', 'AUCTIONBUSD', 'AUCTIONUSDT', 'AUDIOUSDT', 'AVAXBUSD', 'AVAXUSDC', 'AVAXUSDT', 'AXLUSDT', 'AXSUSDT', 'BADGERUSDT', 'BAKEUSDT', 'BALUSDT', 'BANANAUSDT', 'BANDUSDT', 'BATUSDT', 'BBUSDT', 'BCHUSDC', 'BCHUSDT', 'BEAMXUSDT', 'BELUSDT', 'BICOUSDT', 'BIGTIMEUSDT', 'BLUEBIRDUSDT', 'BLURUSDT', 'BLZUSDT', 'BNBBUSD', 'BNBUSDC', 'BNBUSDT', 'BNTUSDT', 'BNXUSDT', 'BNXUSDTSETTLED', 'BOMEUSDC', 'BOMEUSDT', 'BONDUSDT', 'BRETTUSDT', 'BSVUSDT', 'BTCBUSD', 'BTCBUSD_210129', 'BTCBUSD_210226', 'BTCDOMUSDT', 'BTCSTUSDT', 'BTCUSDC', 'BTCUSDT', 'BTCUSDT_210326', 'BTCUSDT_210625', 'BTCUSDT_210924', 'BTCUSDT_211231', 'BTCUSDT_220325', 'BTCUSDT_220624', 'BTCUSDT_220930', 'BTCUSDT_221230', 'BTCUSDT_230331', 'BTCUSDT_230630', 'BTCUSDT_230929', 'BTCUSDT_231229', 'BTCUSDT_240329', 'BTCUSDT_240628', 'BTCUSDT_240927', 'BTCUSDT_241227', 'BTSUSDT', 'BTTUSDT', 'BZRXUSDT', 'C98USDT', 'CAKEUSDT', 'CELOUSDT', 'CELRUSDT', 'CFXUSDT', 'CHESSUSDT', 'CHRUSDT', 'CHZUSDT', 'CKBUSDT', 'COCOSUSDT', 'COMBOUSDT', 'COMPUSDT', 'COTIUSDT', 'CRVUSDC', 'CRVUSDT', 'CTKUSDT', 'CTSIUSDT', 'CVCUSDT', 'CVXBUSD', 'CVXUSDT', 'CYBERUSDT', 'DARUSDT', 'DASHUSDT', 'DEFIUSDT', 'DENTUSDT', 'DGBUSDT', 'DODOBUSD', 'DODOUSDT', 'DODOXUSDT', 'DOGEBUSD', 'DOGEUSDC', 'DOGEUSDT', 'DOGSUSDT', 'DOTBUSD', 'DOTECOUSDT', 'DOTUSDT', 'DUSKUSDT', 'DYDXUSDT', 'DYMUSDT', 'EDUUSDT', 'EGLDUSDT', 'ENAUSDC', 'ENAUSDT', 'ENJUSDT', 'ENSUSDT', 'EOSUSDT', 'ETCBUSD', 'ETCUSDT', 'ETHBTC', 'ETHBUSD', 'ETHFIUSDC', 'ETHFIUSDT', 'ETHUSDC', 'ETHUSDT', 'ETHUSDT_210326', 'ETHUSDT_210625', 'ETHUSDT_210924', 'ETHUSDT_211231', 'ETHUSDT_220325', 'ETHUSDT_220624', 'ETHUSDT_220930', 'ETHUSDT_221230', 'ETHUSDT_230331', 'ETHUSDT_230630', 'ETHUSDT_230929', 'ETHUSDT_231229', 'ETHUSDT_240329', 'ETHUSDT_240628', 'ETHUSDT_240927', 'ETHUSDT_241227', 'ETHWUSDT', 'FETUSDT', 'FILBUSD', 'FILUSDC', 'FILUSDT', 'FLMUSDT', 'FLOWUSDT', 'FOOTBALLUSDT', 'FRONTUSDT', 'FTMBUSD', 'FTMUSDT', 'FTTBUSD', 'FTTUSDT', 'FXSUSDT', 'GALABUSD', 'GALAUSDT', 'GALBUSD', 'GALUSDT', 'GASUSDT', 'GLMRUSDT', 'GLMUSDT', 'GMTBUSD', 'GMTUSDT', 'GMXUSDT', 'GRTUSDT', 'GTCUSDT', 'GUSDT', 'HBARUSDT', 'HFTUSDT', 'HIFIUSDT', 'HIGHUSDT', 'HNTUSDT', 'HOOKUSDT', 'HOTUSDT', 'ICPBUSD', 'ICPUSDT', 'ICPUSDT_SETTLED', 'ICXUSDT', 'IDEXUSDT', 'IDUSDT', 'ILVUSDT', 'IMXUSDT', 'INJUSDT', 'IOSTUSDT', 'IOTAUSDT', 'IOTXUSDT', 'IOUSDT', 'JASMYUSDT', 'JOEUSDT', 'JTOUSDT', 'JUPUSDT', 'KASUSDT', 'KAVAUSDT', 'KEEPUSDT', 'KEYUSDT', 'KLAYUSDT', 'KNCUSDT', 'KSMUSDT', 'LDOBUSD', 'LDOUSDT', 'LENDUSDT', 'LEVERBUSD', 'LEVERUSDT', 'LINAUSDT', 'LINKBUSD', 'LINKUSDC', 'LINKUSDT', 'LISTAUSDT', 'LITUSDT', 'LOOMUSDT', 'LPTUSDT', 'LQTYUSDT', 'LRCUSDT', 'LSKUSDT', 'LTCBUSD', 'LTCUSDC', 'LTCUSDT', 'LUNA2BUSD', 'LUNA2USDT', 'LUNABUSD', 'LUNAUSDT', 'MAGICUSDT', 'MANAUSDT', 'MANTAUSDT', 'MASKUSDT', 'MATICBUSD', 'MATICUSDC', 'MATICUSDT', 'MAVIAUSDT', 'MAVUSDT', 'MBLUSDT', 'MBOXUSDT', 'MDTUSDT', 'MEMEUSDT', 'METISUSDT', 'MEWUSDT', 'MINAUSDT', 'MINAUSDTSETTLED', 'MKRUSDT', 'MOVRUSDT', 'MTLUSDT', 'MYROUSDT', 'NEARBUSD', 'NEARUSDC', 'NEARUSDT', 'NEOUSDC', 'NEOUSDT', 'NFPUSDT', 'NKNUSDT', 'NMRUSDT', 'NOTUSDT', 'NTRNUSDT', 'NULSUSDT', 'NUUSDT', 'OCEANUSDT', 'OGNUSDT', 'OMGUSDT', 'OMNIUSDT', 'OMUSDT', 'ONDOUSDT', 'ONEUSDT', 'ONGUSDT', 'ONTUSDT', 'OPUSDT', 'ORBSUSDT', 'ORDIUSDC', 'ORDIUSDT', 'OXTUSDT', 'PENDLEUSDT', 'PEOPLEUSDT', 'PERPUSDT', 'PHBBUSD', 'PHBUSDT', 'PIXELUSDT', 'POLYXUSDT', 'POPCATUSDT', 'PORTALUSDT', 'POWRUSDT', 'PYTHUSDT', 'QNTUSDT', 'QTUMUSDT', 'RADUSDT', 'RAREUSDT', 'RAYUSDT', 'RDNTUSDT', 'REEFUSDT', 'RENDERUSDT', 'RENUSDT', 'REZUSDT', 'RIFUSDT', 'RLCUSDT', 'RNDRUSDT', 'RONINUSDT', 'ROSEUSDT', 'RSRUSDT', 'RUNEUSDT', 'RVNUSDT', 'SAGAUSDT', 'SANDBUSD', 'SANDUSDT', 'SCUSDT', 'SEIUSDT', 'SFPUSDT', 'SKLUSDT', 'SLPUSDT', 'SNTUSDT', 'SNXUSDT', 'SOLBUSD', 'SOLUSDC', 'SOLUSDT', 'SPELLUSDT', 'SRMUSDT', 'SSVUSDT', 'STEEMUSDT', 'STGUSDT', 'STMXUSDT', 'STORJUSDT', 'STPTUSDT', 'STRAXUSDT', 'STRKUSDT', 'STXUSDT', 'SUIUSDC', 'SUIUSDT', 'SUNUSDT', 'SUPERUSDT', 'SUSHIUSDT', 'SXPUSDT', 'SYNUSDT', 'SYSUSDT', 'TAOUSDT', 'THETAUSDT', 'TIAUSDC', 'TIAUSDT', 'TLMBUSD', 'TLMUSDT', 'TLMUSDTSETTLED', 'TNSRUSDT', 'TOKENUSDT', 'TOMOUSDT', 'TONUSDT', 'TRBUSDT', 'TRUUSDT', 'TRXBUSD', 'TRXUSDT', 'TURBOUSDT', 'TUSDT', 'TWTUSDT', 'UMAUSDT', 'UNFIUSDT', 'UNIBUSD', 'UNIUSDT', 'USDCUSDT', 'USTCUSDT', 'VANRYUSDT', 'VETUSDT', 'VIDTUSDT', 'VOXELUSDT', 'WAVESBUSD', 'WAVESUSDT', 'WAXPUSDT', 'WIFUSDC', 'WIFUSDT', 'WLDUSDC', 'WLDUSDT', 'WOOUSDT', 'WUSDT', 'XAIUSDT', 'XEMUSDT', 'XLMUSDT', 'XMRUSDT', 'XRPBUSD', 'XRPUSDC', 'XRPUSDT', 'XTZUSDT', 'XVGUSDT', 'XVSUSDT', 'YFIIUSDT', 'YFIUSDT', 'YGGUSDT', 'ZECUSDT', 'ZENUSDT', 'ZETAUSDT', 'ZILUSDT', 'ZKUSDT', 'ZROUSDT', 'ZRXUSDT']\n",
      "['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02', '2024-03', '2024-04', '2024-05', '2024-07', '2024-08', '2024-09', '2024-10', '2024-11', '2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05', '2025-06', '2025-07', '2025-08', '2025-09', '2025-10', '2025-11', '2025-12', '2026-01', '2026-02', '2026-03', '2026-04', '2026-05', '2026-06', '2026-07', '2026-08', '2026-09', '2026-10', '2026-11', '2026-12', '2027-01', '2027-02', '2027-03', '2027-04', '2027-05', '2027-06', '2027-07', '2027-08', '2027-09', '2027-10', '2027-11', '2027-12', '2028-01', '2028-02', '2028-03', '2028-04', '2028-05', '2028-06', '2028-07', '2028-08', '2028-09', '2028-10', '2028-11', '2028-12', '2029-01', '2029-03', '2029-04', '2029-05', '2029-06', '2029-07', '2029-08', '2029-09', '2029-10', '2029-11', '2029-12', '2030-01', '2030-02', '2030-03', '2030-04', '2030-05', '2030-06', '2030-07', '2030-08', '2030-09', '2030-10', '2030-11', '2030-12', '2031-01', '2031-02', '2031-03', '2031-04', '2031-05', '2031-06', '2031-07', '2031-08', '2031-09', '2031-10', '2031-11', '2031-12', '2032-01', '2032-02', '2032-03', '2032-04', '2032-05', '2032-06', '2032-07', '2032-08', '2032-09', '2032-10', '2032-11', '2032-12', '2033-01', '2033-02', '2033-03', '2033-04', '2033-05', '2033-06', '2033-07', '2033-08', '2033-10', '2033-11', '2033-12', '2034-01', '2034-02', '2034-03', '2034-04', '2034-05', '2034-06', '2034-07', '2034-08', '2034-09', '2034-10', '2034-11', '2034-12', '2035-01', '2035-02', '2035-03', '2035-04', '2035-05', '2035-06', '2035-07', '2035-08', '2035-09', '2035-10', '2035-11', '2035-12', '2036-01', '2036-02', '2036-03', '2036-04', '2036-05', '2036-06', '2036-07', '2036-08', '2036-09', '2036-10', '2036-11', '2036-12', '2037-01', '2037-02', '2037-03', '2037-04', '2037-05', '2037-06', '2037-07', '2037-08', '2037-09', '2037-10', '2037-11', '2037-12', '2038-01', '2038-03', '2038-04', '2038-05', '2038-06', '2038-07', '2038-08', '2038-09', '2038-10', '2038-11', '2038-12', '2039-01', '2039-02', '2039-03', '2039-04', '2039-05', '2039-06', '2039-07', '2039-08', '2039-09', '2039-10', '2039-11', '2039-12', '2040-01', '2040-02', '2040-03', '2040-04', '2040-05', '2040-06', '2040-07', '2040-08', '2040-09', '2040-10', '2040-11', '2040-12', '2041-01', '2041-02', '2041-03', '2041-04', '2041-05', '2041-06', '2041-07', '2041-08', '2041-09', '2041-10', '2041-11', '2041-12', '2042-01', '2042-02', '2042-03', '2042-04', '2042-05', '2042-06', '2042-07', '2042-08', '2042-09', '2042-10', '2042-12', '2043-01', '2043-02', '2043-03', '2043-04', '2043-05', '2043-06', '2043-07', '2043-08', '2043-09', '2043-10', '2043-11', '2043-12', '2044-01', '2044-02', '2044-03', '2044-04', '2044-05', '2044-06', '2044-07', '2044-08', '2044-09', '2044-10', '2044-11', '2044-12', '2045-01', '2045-02', '2045-03', '2045-04', '2045-05', '2045-06', '2045-07', '2045-08', '2045-09', '2045-10', '2045-11', '2045-12', '2046-01', '2046-02', '2046-03', '2046-04', '2046-05', '2046-06', '2046-07', '2046-08', '2046-09', '2046-10', '2046-11', '2046-12', '2047-01', '2047-02', '2047-03', '2047-05', '2047-06', '2047-07', '2047-08', '2047-09', '2047-10', '2047-11', '2047-12', '2048-01', '2048-02', '2048-03', '2048-04', '2048-05', '2048-06', '2048-07', '2048-08', '2048-09', '2048-10', '2048-11', '2048-12', '2049-01', '2049-02', '2049-03', '2049-04', '2049-05', '2049-06', '2049-07', '2049-08', '2049-09', '2049-10', '2049-11', '2049-12', '2050-01', '2050-02', '2050-03', '2050-04', '2050-05', '2050-06', '2050-07', '2050-08', '2050-09', '2050-10', '2050-11', '2050-12', '2051-01', '2051-02', '2051-03', '2051-04', '2051-05', '2051-06', '2051-07', '2051-08', '2051-09', '2051-10', '2051-11', '2051-12', '2052-01', '2052-03', '2052-04', '2052-05', '2052-06', '2052-07', '2052-08', '2052-09', '2052-10', '2052-11', '2052-12', '2053-01', '2053-02', '2053-03', '2053-04', '2053-05', '2053-06', '2053-07', '2053-08', '2053-09', '2053-10', '2053-11', '2053-12', '2054-01', '2054-02', '2054-03', '2054-04', '2054-05', '2054-06', '2054-07', '2054-08', '2054-09', '2054-10', '2054-11', '2054-12', '2055-01', '2055-02', '2055-03', '2055-04', '2055-05', '2055-06', '2055-07', '2055-08', '2055-09', '2055-10', '2055-11', '2055-12', '2056-01', '2056-02', '2056-03', '2056-04', '2056-05', '2056-06', '2056-07', '2056-08', '2056-10', '2056-11', '2056-12', '2057-01', '2057-02', '2057-03', '2057-04', '2057-05', '2057-06', '2057-07', '2057-08', '2057-09', '2057-10', '2057-11', '2057-12', '2058-01', '2058-02', '2058-03', '2058-04', '2058-05', '2058-06', '2058-07', '2058-08', '2058-09', '2058-10', '2058-11', '2058-12', '2059-01', '2059-02', '2059-03', '2059-04', '2059-05', '2059-06', '2059-07', '2059-08', '2059-09', '2059-10', '2059-11', '2059-12', '2060-01', '2060-02', '2060-03', '2060-04', '2060-05', '2060-06', '2060-07', '2060-08', '2060-09', '2060-10', '2060-11', '2060-12', '2061-01', '2061-03', '2061-04', '2061-05', '2061-06', '2061-07', '2061-08', '2061-09', '2061-10', '2061-11', '2061-12', '2062-01', '2062-02', '2062-03', '2062-04', '2062-05', '2062-06', '2062-07', '2062-08', '2062-09', '2062-10', '2062-11', '2062-12', '2063-01', '2063-02', '2063-03', '2063-04', '2063-05', '2063-06', '2063-07', '2063-08', '2063-09', '2063-10', '2063-11', '2063-12', '2064-01', '2064-02', '2064-03', '2064-04', '2064-05', '2064-06', '2064-07', '2064-08', '2064-09', '2064-10', '2064-11', '2064-12', '2065-01', '2065-02', '2065-03', '2065-04', '2065-05', '2065-06', '2065-07', '2065-08', '2065-09', '2065-10', '2065-12', '2066-01', '2066-02', '2066-03', '2066-04', '2066-05', '2066-06', '2066-07', '2066-08', '2066-09', '2066-10', '2066-11', '2066-12', '2067-01', '2067-02', '2067-03', '2067-04', '2067-05', '2067-06', '2067-07', '2067-08', '2067-09', '2067-10', '2067-11', '2067-12', '2068-01', '2068-02', '2068-03', '2068-04', '2068-05', '2068-06', '2068-07', '2068-08', '2068-09', '2068-10', '2068-11', '2068-12', '2069-01', '2069-02', '2069-03', '2069-04', '2069-05', '2069-06', '2069-07', '2069-08', '2069-09', '2069-10', '2069-11', '2069-12', '2070-01', '2070-02', '2070-03', '2070-05', '2070-06', '2070-07', '2070-08', '2070-09', '2070-10', '2070-11', '2070-12', '2071-01', '2071-02', '2071-03', '2071-04', '2071-05', '2071-06', '2071-07', '2071-08', '2071-09', '2071-10', '2071-11', '2071-12', '2072-01', '2072-02', '2072-03', '2072-04', '2072-05', '2072-06', '2072-07', '2072-08', '2072-09', '2072-10', '2072-11', '2072-12', '2073-01', '2073-02', '2073-03', '2073-04', '2073-05', '2073-06', '2073-07', '2073-08', '2073-09', '2073-10', '2073-11', '2073-12', '2074-01', '2074-02', '2074-03', '2074-04', '2074-05', '2074-06', '2074-07', '2074-08', '2074-09', '2074-10', '2074-11', '2074-12', '2075-01', '2075-03', '2075-04', '2075-05', '2075-06', '2075-07', '2075-08', '2075-09', '2075-10', '2075-11', '2075-12', '2076-01', '2076-02', '2076-03', '2076-04', '2076-05', '2076-06', '2076-07', '2076-08', '2076-09', '2076-10', '2076-11', '2076-12', '2077-01', '2077-02', '2077-03', '2077-04', '2077-05', '2077-06', '2077-07', '2077-08', '2077-09', '2077-10', '2077-11', '2077-12', '2078-01', '2078-02', '2078-03', '2078-04', '2078-05', '2078-06', '2078-07', '2078-08', '2078-09', '2078-10', '2078-11', '2078-12', '2079-01', '2079-02', '2079-03', '2079-04', '2079-05', '2079-07', '2079-08', '2079-09', '2079-10', '2079-11', '2079-12']\n",
      "['12h', '15m', '1d', '1h', '1m', '1mo', '1w', '2h', '30m', '3d', '3m', '4h', '5m', '6h', '8h']\n",
      "Symbol, month, and chart arrays are successfully created\n"
     ]
    }
   ],
   "source": [
    "%run ./creating_arrays.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f291bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:49:06.625037Z",
     "start_time": "2024-09-08T14:49:05.294447Z"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import bz2\n",
    "import datetime\n",
    "import glob\n",
    "import gzip\n",
    "import hashlib\n",
    "import inspect\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import time\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import zlib\n",
    "from decimal import Decimal, ROUND_DOWN\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import requests\n",
    "import talib\n",
    "import talib\n",
    "from numba import njit, prange\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969740d1",
   "metadata": {},
   "source": [
    "# all tiny modular functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3e463",
   "metadata": {},
   "source": [
    "## download data, concatenate data utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fe5526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T14:43:20.085846Z",
     "start_time": "2024-01-15T14:43:20.022109Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_monthly_data(month_array, symbol, chart_time):\n",
    "    # Downloading monthly data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    month_array.pop()\n",
    "    for month in month_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "                # Print which file was downloaded\n",
    "                print(f\"Downloaded monthly data for {symbol}-{chart_time}-{month}.zip\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {link}: {e}\")\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print(f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print(f\"You're already up to date for monthly data for {symbol},{chart_time}\")\n",
    "\n",
    "\n",
    "def download_daily_data(day_array, symbol, chart_time):\n",
    "    # Downloading daily data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    day_array.pop()\n",
    "    for day in day_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "                # Print which file was downloaded\n",
    "                print(f\"Downloaded daily data for {symbol}-{chart_time}-{day}.zip\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {link}: {e}\")\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print(f\"You're already up to date for daily data for {symbol},{chart_time}\")\n",
    "\n",
    "\n",
    "def construct_csv_file_path(folder_path,\n",
    "                            symbol,\n",
    "                            chart_time,\n",
    "                            file,\n",
    "                            is_daily=False):\n",
    "    if is_daily:\n",
    "        # For daily data, use a different pattern\n",
    "        return os.path.join(\n",
    "            folder_path,\n",
    "            f\"{symbol}-{chart_time}-{file.split('-')[-3]}-{file.split('-')[-2]}-{file.split('-')[-1][:-4]}.csv\"\n",
    "        )\n",
    "    else:\n",
    "        # For monthly data, use the original pattern\n",
    "        return os.path.join(folder_path,\n",
    "                            f\"{symbol}-{chart_time}{file[-12:-4]}.csv\")\n",
    "\n",
    "\n",
    "def process_zip_folder(folder_path,\n",
    "                       pattern,\n",
    "                       new_csv_folder_path,\n",
    "                       symbol,\n",
    "                       chart_time,\n",
    "                       df_list,\n",
    "                       daily_array=None,\n",
    "                       is_daily=False):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return df_list\n",
    "\n",
    "    # Iterate over files in the directory\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Construct the full path to the item\n",
    "        item_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Check if the item is a file and matches the pattern\n",
    "        if os.path.isfile(item_path) and pattern.match(file_name):\n",
    "            if is_daily and daily_array:\n",
    "                # Check if any date in the daily_array is in the file_name\n",
    "                if not any(date in file_name for date in daily_array):\n",
    "                    continue\n",
    "\n",
    "            # Extract the ZIP file\n",
    "            try:\n",
    "                with zipfile.ZipFile(item_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(new_csv_folder_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting file {file_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Construct the CSV file path using the helper function\n",
    "            csv_file_path = construct_csv_file_path(new_csv_folder_path,\n",
    "                                                    symbol, chart_time,\n",
    "                                                    file_name, is_daily)\n",
    "\n",
    "            # Read the CSV file into a data frame, ignoring the headers\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file_path, header=None)\n",
    "                # Remove the first row (which contains the header)\n",
    "                df = df.iloc[1:]\n",
    "                # Add it to the list\n",
    "                df_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {csv_file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return df_list\n",
    "\n",
    "\n",
    "def get_correct_headers(new_csv_folder_path):\n",
    "    possible_headers = [\n",
    "        'open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time',\n",
    "        'quote_volume', 'count', 'taker_buy_volume', 'taker_buy_quote_volume',\n",
    "        'ignore'\n",
    "    ]\n",
    "\n",
    "    for file_name in os.listdir(new_csv_folder_path):\n",
    "        file_path = os.path.join(new_csv_folder_path, file_name)\n",
    "        try:\n",
    "            # Read the first row to get headers\n",
    "            headers = pd.read_csv(file_path, nrows=1).columns.tolist()\n",
    "            # Check if at least 2 headers match\n",
    "            matches = [\n",
    "                header for header in headers if header in possible_headers\n",
    "            ]\n",
    "            if len(matches) >= 2:\n",
    "#                 print(f\"Found matching headers in {file_name}: {matches}\")\n",
    "                return headers\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    raise ValueError(\"Could not find matching headers in any CSV files.\")\n",
    "\n",
    "\n",
    "def concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time):\n",
    "    # Get correct headers from a CSV file\n",
    "    try:\n",
    "        correct_headers = get_correct_headers(new_csv_folder_path)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return \"Error finding headers\"\n",
    "\n",
    "    # Concatenate the data frames in the list\n",
    "    df_final = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Check if df_final has headers or not\n",
    "    if df_final.columns[0] not in correct_headers:\n",
    "#         print(\"Updating older CSVs with correct headers.\")\n",
    "        # Update headers for older CSVs that lack them\n",
    "        for file_name in os.listdir(new_csv_folder_path):\n",
    "            file_path = os.path.join(new_csv_folder_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    df_old = pd.read_csv(file_path, header=None)\n",
    "                    # Ensure we only update files with correct structure\n",
    "                    if len(df_old.columns) == len(correct_headers):\n",
    "                        df_old.columns = correct_headers\n",
    "                        df_old.to_csv(file_path, index=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating file {file_name}: {e}\")\n",
    "\n",
    "    # Set the headers as the column names of the final dataframe\n",
    "    df_final.columns = correct_headers\n",
    "\n",
    "    # Convert 'open_time' and 'close_time' columns to datetime\n",
    "    try:\n",
    "        df_final['open_time'] = pd.to_datetime(\n",
    "            df_final['open_time'],\n",
    "            unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "        df_final['close_time'] = pd.to_datetime(\n",
    "            df_final['close_time'],\n",
    "            unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "    except KeyError as e:\n",
    "        print(f\"Column not found for conversion: {e}\")\n",
    "        return \"Error in date conversion\"\n",
    "\n",
    "    # Delete the 'ignore' column if it exists\n",
    "    if 'ignore' in df_final.columns:\n",
    "        df_final = df_final.drop(['ignore'], axis=1)\n",
    "\n",
    "    # Add a new column called 'entry' that will take previous close\n",
    "    df_final['entry'] = df_final['open']\n",
    "\n",
    "    # Set the file name\n",
    "    concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "    # Construct the file path\n",
    "    concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                          concatenated_file_name)\n",
    "\n",
    "    # Write the data frame to the CSV file\n",
    "    df_final.to_csv(concatenated_file_path, index=False)\n",
    "\n",
    "    directory_final = Path(\n",
    "        concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "    # Deleting all the other CSVs except the final concatenated file\n",
    "    for file_path in directory_final.iterdir():\n",
    "        if file_path.is_file():\n",
    "            # Ensure we only delete individual CSVs used for concatenation\n",
    "            if file_path.name.startswith(\n",
    "                    f\"{symbol}-{chart_time}-\") and file_path.name.endswith(\n",
    "                        '.csv') and file_path.name != concatenated_file_name:\n",
    "                file_path.unlink()\n",
    "\n",
    "    return \"Data concatenated, individual CSVs deleted\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5404db",
   "metadata": {},
   "source": [
    "## calculate indicators util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcdcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_off = 5\n",
    "\n",
    "def calculate_indicators_using_talib(timeperiods, df):\n",
    "    def apply_indicator(func, *args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    indicator_columns = [\n",
    "        ('HT_TRENDLINE', talib.HT_TRENDLINE(df['close'])),\n",
    "        ('SAR', talib.SAR(df['high'], df['low'])),\n",
    "        ('SAREXT', talib.SAREXT(df['high'], df['low'])),\n",
    "        ('T3', talib.T3(df['close'], timeperiod=5)),\n",
    "        ('APO', np.round(talib.APO(df['close'], fastperiod=12, slowperiod=26), round_off)),\n",
    "        ('BOP', talib.BOP(df['open'], df['high'], df['low'], df['close'])),\n",
    "        *zip(['MACD', 'MACD_signal', 'MACD_hist'], talib.MACD(df['close'])),\n",
    "        ('PPO', np.round(talib.PPO(df['close']), round_off)),\n",
    "        ('TRIX', talib.TRIX(df['close'])),\n",
    "        ('ULTOSC', np.round(talib.ULTOSC(df['high'], df['low'], df['close']), round_off)),\n",
    "        ('WILLR', talib.WILLR(df['high'], df['low'], df['close'])),\n",
    "        ('AD', np.round(talib.AD(df['high'], df['low'], df['close'], df['volume']), round_off)),\n",
    "        ('ADOSC', np.round(talib.ADOSC(df['high'], df['low'], df['close'], df['volume']), round_off)),\n",
    "        ('OBV', np.round(talib.OBV(df['close'], df['volume']), round_off)),\n",
    "        ('HT_DCPERIOD', np.round(talib.HT_DCPERIOD(df['close']), round_off)),\n",
    "        ('HT_DCPHASE', np.round(talib.HT_DCPHASE(df['close']), round_off)),\n",
    "        *zip(['HT_PHASOR_inphase', 'HT_PHASOR_quadrature'], np.round(talib.HT_PHASOR(df['close']), round_off)),\n",
    "        ('HT_TRENDMODE', talib.HT_TRENDMODE(df['close'])),\n",
    "        ('AVGPRICE', talib.AVGPRICE(df['open'], df['high'], df['low'], df['close'])),\n",
    "        ('MEDPRICE', talib.MEDPRICE(df['high'], df['low'])),\n",
    "        ('TYPPRICE', talib.TYPPRICE(df['high'], df['low'], df['close'])),\n",
    "        ('WCLPRICE', talib.WCLPRICE(df['high'], df['low'], df['close'])),\n",
    "        ('TRANGE', talib.TRANGE(df['high'], df['low'], df['close'])),\n",
    "    ]\n",
    "\n",
    "    # Add pattern recognition indicators\n",
    "    pattern_funcs = [getattr(talib, f) for f in dir(talib) if f.startswith('CDL')]\n",
    "    indicator_columns.extend([\n",
    "        (f.__name__, apply_indicator(f, df['open'], df['high'], df['low'], df['close']))\n",
    "        for f in pattern_funcs\n",
    "    ])\n",
    "\n",
    "    # Add statistic functions\n",
    "    stat_funcs = [talib.LINEARREG, talib.LINEARREG_ANGLE, talib.LINEARREG_INTERCEPT, talib.LINEARREG_SLOPE, talib.TSF, talib.VAR]\n",
    "    indicator_columns.extend([\n",
    "        (f.__name__, np.round(apply_indicator(f, df['close']), round_off))\n",
    "        for f in stat_funcs\n",
    "    ])\n",
    "\n",
    "    # Add indicators with timeperiods\n",
    "    timeperiod_funcs = [\n",
    "        (talib.BBANDS, lambda t: talib.BBANDS(df['close'], timeperiod=t), ['BB_upper_{}', 'BB_middle_{}', 'BB_lower_{}']),\n",
    "        (talib.DEMA, lambda t: talib.DEMA(df['close'], timeperiod=t), ['DEMA_{}']),\n",
    "        (talib.EMA, lambda t: talib.EMA(df['close'], timeperiod=t), ['EMA_{}']),\n",
    "        (talib.KAMA, lambda t: talib.KAMA(df['close'], timeperiod=t), ['KAMA_{}']),\n",
    "        (talib.MA, lambda t: talib.MA(df['close'], timeperiod=t), ['MA_{}']),\n",
    "        (talib.MIDPOINT, lambda t: talib.MIDPOINT(df['close'], timeperiod=t), ['MIDPOINT_{}']),\n",
    "        (talib.MIDPRICE, lambda t: talib.MIDPRICE(df['high'], df['low'], timeperiod=t), ['MIDPRICE_{}']),\n",
    "        (talib.SMA, lambda t: talib.SMA(df['close'], timeperiod=t), ['SMA_{}']),\n",
    "        (talib.TEMA, lambda t: talib.TEMA(df['close'], timeperiod=t), ['TEMA_{}']),\n",
    "        (talib.TRIMA, lambda t: talib.TRIMA(df['close'], timeperiod=t), ['TRIMA_{}']),\n",
    "        (talib.WMA, lambda t: talib.WMA(df['close'], timeperiod=t), ['WMA_{}']),\n",
    "        (talib.ADX, lambda t: talib.ADX(df['high'], df['low'], df['close'], timeperiod=t), ['ADX_{}']),\n",
    "        (talib.ADXR, lambda t: talib.ADXR(df['high'], df['low'], df['close'], timeperiod=t), ['ADXR_{}']),\n",
    "        (talib.AROON, lambda t: talib.AROON(df['high'], df['low'], timeperiod=t), ['AROON_up_{}', 'AROON_down_{}']),\n",
    "        (talib.AROONOSC, lambda t: talib.AROONOSC(df['high'], df['low'], timeperiod=t), ['AROONOSC_{}']),\n",
    "        (talib.CCI, lambda t: talib.CCI(df['high'], df['low'], df['close'], timeperiod=t), ['CCI_{}']),\n",
    "        (talib.CMO, lambda t: talib.CMO(df['close'], timeperiod=t), ['CMO_{}']),\n",
    "        (talib.DX, lambda t: talib.DX(df['high'], df['low'], df['close'], timeperiod=t), ['DX_{}']),\n",
    "        (talib.MFI, lambda t: talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=t), ['MFI_{}']),\n",
    "        (talib.MINUS_DI, lambda t: talib.MINUS_DI(df['high'], df['low'], df['close'], timeperiod=t), ['MINUS_DI_{}']),\n",
    "        (talib.MINUS_DM, lambda t: talib.MINUS_DM(df['high'], df['low'], timeperiod=t), ['MINUS_DM_{}']),\n",
    "        (talib.MOM, lambda t: talib.MOM(df['close'], timeperiod=t), ['MOM_{}']),\n",
    "        (talib.PLUS_DI, lambda t: talib.PLUS_DI(df['high'], df['low'], df['close'], timeperiod=t), ['PLUS_DI_{}']),\n",
    "        (talib.PLUS_DM, lambda t: talib.PLUS_DM(df['high'], df['low'], timeperiod=t), ['PLUS_DM_{}']),\n",
    "        (talib.ROC, lambda t: talib.ROC(df['close'], timeperiod=t), ['ROC_{}']),\n",
    "        (talib.ROCP, lambda t: talib.ROCP(df['close'], timeperiod=t), ['ROCP_{}']),\n",
    "        (talib.ROCR, lambda t: talib.ROCR(df['close'], timeperiod=t), ['ROCR_{}']),\n",
    "        (talib.ROCR100, lambda t: talib.ROCR100(df['close'], timeperiod=t), ['ROCR100_{}']),\n",
    "        (talib.RSI, lambda t: talib.RSI(df['close'], timeperiod=t), ['RSI_{}']),\n",
    "        (talib.ATR, lambda t: talib.ATR(df['high'], df['low'], df['close'], timeperiod=t), ['ATR_{}']),\n",
    "        (talib.NATR, lambda t: talib.NATR(df['high'], df['low'], df['close'], timeperiod=t), ['NATR_{}']),\n",
    "        (talib.BETA, lambda t: talib.BETA(df['high'], df['low'], timeperiod=t), ['BETA_{}']),\n",
    "        (talib.CORREL, lambda t: talib.CORREL(df['high'], df['low'], timeperiod=t), ['CORREL_{}']),\n",
    "    ]\n",
    "\n",
    "#     for func, func_with_timeperiod, column_names in timeperiod_funcs:\n",
    "#         for timeperiod in timeperiods:\n",
    "#             result = func_with_timeperiod(timeperiod)\n",
    "#             if isinstance(result, tuple):\n",
    "#                 for res, col_name in zip(result, column_names):\n",
    "#                     # Round the result to 4 decimal points\n",
    "#                     res_rounded = np.round(res, round_off)\n",
    "#                     indicator_columns.append((col_name.format(timeperiod), res_rounded))\n",
    "#             else:\n",
    "#                 # Round the result to 4 decimal points\n",
    "#                 result_rounded = np.round(result, round_off)\n",
    "#                 indicator_columns.append((column_names[0].format(timeperiod), result_rounded))\n",
    "\n",
    "#     # Concatenate the new indicator columns with the original DataFrame\n",
    "#     df_indicators = pd.concat([pd.DataFrame(data, columns=[name]) for name, data in indicator_columns], axis=1)\n",
    "\n",
    "#     # Return the original DataFrame with the new indicator columns\n",
    "#     return pd.concat([df, df_indicators], axis=1)\n",
    "    \n",
    "    for func, func_with_timeperiod, column_names in timeperiod_funcs:\n",
    "        for timeperiod in timeperiods:\n",
    "            result = func_with_timeperiod(timeperiod)\n",
    "            if isinstance(result, tuple):\n",
    "                for res, col_name in zip(result, column_names):\n",
    "                    # Round the result to 4 decimal points\n",
    "                    res_rounded = np.round(res, round_off)\n",
    "                    indicator_columns.append((col_name.format(timeperiod), res_rounded))\n",
    "            else:\n",
    "                # Round the result to 4 decimal points\n",
    "                result_rounded = np.round(result, round_off)\n",
    "                indicator_columns.append((column_names[0].format(timeperiod), result_rounded))\n",
    "\n",
    "    df_indicators = pd.concat([pd.DataFrame(data, columns=[name]) for name, data in indicator_columns], axis=1)\n",
    "    \n",
    "    return df_indicators\n",
    "#     # Return the original DataFrame with the new indicator columns\n",
    "#     return pd.concat([df, df_indicators], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indicators_using_talib_new(timeperiods, df):\n",
    "    round_off = 5\n",
    "    # Convert DataFrame columns to numpy arrays for faster processing\n",
    "    open_arr = df['open'].values\n",
    "    high_arr = df['high'].values\n",
    "    low_arr = df['low'].values\n",
    "    close_arr = df['close'].values\n",
    "    volume_arr = df['volume'].values\n",
    "\n",
    "    # Initialize a dictionary to store all indicator results\n",
    "    indicators = {}\n",
    "\n",
    "    # Single-output indicators\n",
    "    single_indicators = [\n",
    "        ('HT_TRENDLINE', talib.HT_TRENDLINE, [close_arr]),\n",
    "        ('SAR', talib.SAR, [high_arr, low_arr]),\n",
    "        ('SAREXT', talib.SAREXT, [high_arr, low_arr]),\n",
    "        ('T3', lambda x: talib.T3(x, timeperiod=5), [close_arr]),\n",
    "        ('APO', lambda x: np.round(talib.APO(x, fastperiod=12, slowperiod=26), round_off), [close_arr]),\n",
    "        ('BOP', talib.BOP, [open_arr, high_arr, low_arr, close_arr]),\n",
    "        ('PPO', lambda x: np.round(talib.PPO(x), round_off), [close_arr]),\n",
    "        ('TRIX', talib.TRIX, [close_arr]),\n",
    "        ('ULTOSC', lambda x, y, z: np.round(talib.ULTOSC(x, y, z), round_off), [high_arr, low_arr, close_arr]),\n",
    "        ('WILLR', talib.WILLR, [high_arr, low_arr, close_arr]),\n",
    "        ('AD', lambda w, x, y, z: np.round(talib.AD(w, x, y, z), round_off), [high_arr, low_arr, close_arr, volume_arr]),\n",
    "        ('ADOSC', lambda w, x, y, z: np.round(talib.ADOSC(w, x, y, z), round_off), [high_arr, low_arr, close_arr, volume_arr]),\n",
    "        ('OBV', lambda x, y: np.round(talib.OBV(x, y), round_off), [close_arr, volume_arr]),\n",
    "        ('HT_DCPERIOD', lambda x: np.round(talib.HT_DCPERIOD(x), round_off), [close_arr]),\n",
    "        ('HT_DCPHASE', lambda x: np.round(talib.HT_DCPHASE(x), round_off), [close_arr]),\n",
    "        ('HT_TRENDMODE', talib.HT_TRENDMODE, [close_arr]),\n",
    "        ('AVGPRICE', talib.AVGPRICE, [open_arr, high_arr, low_arr, close_arr]),\n",
    "        ('MEDPRICE', talib.MEDPRICE, [high_arr, low_arr]),\n",
    "        ('TYPPRICE', talib.TYPPRICE, [high_arr, low_arr, close_arr]),\n",
    "        ('WCLPRICE', talib.WCLPRICE, [high_arr, low_arr, close_arr]),\n",
    "        ('TRANGE', talib.TRANGE, [high_arr, low_arr, close_arr]),\n",
    "    ]\n",
    "\n",
    "    # Calculate single-output indicators with progress bar\n",
    "    for name, func, args in tqdm(single_indicators, desc=\"Calculating single-output indicators\"):\n",
    "        indicators[name] = func(*args)\n",
    "\n",
    "    # Multi-output indicators\n",
    "    multi_indicators = [\n",
    "        ('MACD', talib.MACD, [close_arr], ['MACD', 'MACD_signal', 'MACD_hist']),\n",
    "        ('HT_PHASOR', lambda x: np.round(talib.HT_PHASOR(x), round_off), [close_arr], ['HT_PHASOR_inphase', 'HT_PHASOR_quadrature']),\n",
    "    ]\n",
    "\n",
    "    # Calculate multi-output indicators with progress bar\n",
    "    for base_name, func, args, output_names in tqdm(multi_indicators, desc=\"Calculating multi-output indicators\"):\n",
    "        results = func(*args)\n",
    "        for i, name in enumerate(output_names):\n",
    "            indicators[name] = results[i]\n",
    "\n",
    "    # Pattern recognition indicators\n",
    "    pattern_funcs = [getattr(talib, f) for f in dir(talib) if f.startswith('CDL')]\n",
    "    for func in tqdm(pattern_funcs, desc=\"Calculating pattern recognition indicators\"):\n",
    "        indicators[func.__name__] = func(open_arr, high_arr, low_arr, close_arr)\n",
    "\n",
    "    # Statistic functions\n",
    "    stat_funcs = [talib.LINEARREG, talib.LINEARREG_ANGLE, talib.LINEARREG_INTERCEPT, talib.LINEARREG_SLOPE, talib.TSF, talib.VAR]\n",
    "    for func in tqdm(stat_funcs, desc=\"Calculating statistic functions\"):\n",
    "        indicators[func.__name__] = np.round(func(close_arr), round_off)\n",
    "\n",
    "    # Indicators with timeperiods\n",
    "    timeperiod_funcs = [\n",
    "        (talib.BBANDS, lambda t: talib.BBANDS(close_arr, timeperiod=t), ['BB_upper_{}', 'BB_middle_{}', 'BB_lower_{}']),\n",
    "        (talib.DEMA, lambda t: talib.DEMA(close_arr, timeperiod=t), ['DEMA_{}']),\n",
    "        (talib.EMA, lambda t: talib.EMA(close_arr, timeperiod=t), ['EMA_{}']),\n",
    "        (talib.KAMA, lambda t: talib.KAMA(close_arr, timeperiod=t), ['KAMA_{}']),\n",
    "        (talib.MA, lambda t: talib.MA(close_arr, timeperiod=t), ['MA_{}']),\n",
    "        (talib.MIDPOINT, lambda t: talib.MIDPOINT(close_arr, timeperiod=t), ['MIDPOINT_{}']),\n",
    "        (talib.MIDPRICE, lambda t: talib.MIDPRICE(high_arr, low_arr, timeperiod=t), ['MIDPRICE_{}']),\n",
    "        (talib.SMA, lambda t: talib.SMA(close_arr, timeperiod=t), ['SMA_{}']),\n",
    "        (talib.TEMA, lambda t: talib.TEMA(close_arr, timeperiod=t), ['TEMA_{}']),\n",
    "        (talib.TRIMA, lambda t: talib.TRIMA(close_arr, timeperiod=t), ['TRIMA_{}']),\n",
    "        (talib.WMA, lambda t: talib.WMA(close_arr, timeperiod=t), ['WMA_{}']),\n",
    "        (talib.ADX, lambda t: talib.ADX(high_arr, low_arr, close_arr, timeperiod=t), ['ADX_{}']),\n",
    "        (talib.ADXR, lambda t: talib.ADXR(high_arr, low_arr, close_arr, timeperiod=t), ['ADXR_{}']),\n",
    "        (talib.AROON, lambda t: talib.AROON(high_arr, low_arr, timeperiod=t), ['AROON_up_{}', 'AROON_down_{}']),\n",
    "        (talib.AROONOSC, lambda t: talib.AROONOSC(high_arr, low_arr, timeperiod=t), ['AROONOSC_{}']),\n",
    "        (talib.CCI, lambda t: talib.CCI(high_arr, low_arr, close_arr, timeperiod=t), ['CCI_{}']),\n",
    "        (talib.CMO, lambda t: talib.CMO(close_arr, timeperiod=t), ['CMO_{}']),\n",
    "        (talib.DX, lambda t: talib.DX(high_arr, low_arr, close_arr, timeperiod=t), ['DX_{}']),\n",
    "        (talib.MFI, lambda t: talib.MFI(high_arr, low_arr, close_arr, volume_arr, timeperiod=t), ['MFI_{}']),\n",
    "        (talib.MINUS_DI, lambda t: talib.MINUS_DI(high_arr, low_arr, close_arr, timeperiod=t), ['MINUS_DI_{}']),\n",
    "        (talib.MINUS_DM, lambda t: talib.MINUS_DM(high_arr, low_arr, timeperiod=t), ['MINUS_DM_{}']),\n",
    "        (talib.MOM, lambda t: talib.MOM(close_arr, timeperiod=t), ['MOM_{}']),\n",
    "        (talib.PLUS_DI, lambda t: talib.PLUS_DI(high_arr, low_arr, close_arr, timeperiod=t), ['PLUS_DI_{}']),\n",
    "        (talib.PLUS_DM, lambda t: talib.PLUS_DM(high_arr, low_arr, timeperiod=t), ['PLUS_DM_{}']),\n",
    "        (talib.ROC, lambda t: talib.ROC(close_arr, timeperiod=t), ['ROC_{}']),\n",
    "        (talib.ROCP, lambda t: talib.ROCP(close_arr, timeperiod=t), ['ROCP_{}']),\n",
    "        (talib.ROCR, lambda t: talib.ROCR(close_arr, timeperiod=t), ['ROCR_{}']),\n",
    "        (talib.ROCR100, lambda t: talib.ROCR100(close_arr, timeperiod=t), ['ROCR100_{}']),\n",
    "        (talib.RSI, lambda t: talib.RSI(close_arr, timeperiod=t), ['RSI_{}']),\n",
    "        (talib.ATR, lambda t: talib.ATR(high_arr, low_arr, close_arr, timeperiod=t), ['ATR_{}']),\n",
    "        (talib.NATR, lambda t: talib.NATR(high_arr, low_arr, close_arr, timeperiod=t), ['NATR_{}']),\n",
    "        (talib.BETA, lambda t: talib.BETA(high_arr, low_arr, timeperiod=t), ['BETA_{}']),\n",
    "        (talib.CORREL, lambda t: talib.CORREL(high_arr, low_arr, timeperiod=t), ['CORREL_{}']),\n",
    "    ]\n",
    "\n",
    "    # Calculate indicators with timeperiods\n",
    "    total_iterations = len(timeperiod_funcs) * len(timeperiods)\n",
    "    with tqdm(total=total_iterations, desc=\"Time-based indicators\", unit=\"calculation\", dynamic_ncols=True) as pbar:\n",
    "        for _, func_with_timeperiod, column_names in timeperiod_funcs:\n",
    "            for timeperiod in timeperiods:\n",
    "                result = func_with_timeperiod(timeperiod)\n",
    "                # Prepare results for each column name\n",
    "                if isinstance(result, tuple):\n",
    "                    results_to_add = {col_name.format(timeperiod): np.round(res, round_off) for res, col_name in zip(result, column_names)}\n",
    "                else:\n",
    "                    results_to_add = {column_names[0].format(timeperiod): np.round(result, round_off)}\n",
    "\n",
    "                # Update indicators with results\n",
    "                indicators.update(results_to_add)\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Convert the indicators dictionary to a DataFrame\n",
    "    df_indicators = pd.DataFrame(indicators)\n",
    "\n",
    "    return df_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3055008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round_off = 5\n",
    "\n",
    "# def calculate_indicators_using_talib(timeperiods, df):\n",
    "#     def apply_indicator(func, *args, **kwargs):\n",
    "#         return func(*args, **kwargs)\n",
    "\n",
    "#     indicator_columns = [\n",
    "#         ('HT_TRENDLINE', talib.HT_TRENDLINE(df['close'])),\n",
    "#         ('SAR', talib.SAR(df['high'], df['low'])),\n",
    "#         ('SAREXT', talib.SAREXT(df['high'], df['low'])),\n",
    "#         ('T3', talib.T3(df['close'], timeperiod=5)),\n",
    "#         ('APO', np.round(talib.APO(df['close'], fastperiod=12, slowperiod=26), round_off)),\n",
    "#         ('BOP', talib.BOP(df['open'], df['high'], df['low'], df['close'])),\n",
    "#         *zip(['MACD', 'MACD_signal', 'MACD_hist'], talib.MACD(df['close'])),\n",
    "#         ('PPO', np.round(talib.PPO(df['close']), round_off)),\n",
    "#         ('TRIX', talib.TRIX(df['close'])),\n",
    "#         ('ULTOSC', np.round(talib.ULTOSC(df['high'], df['low'], df['close']), round_off)),\n",
    "#         ('WILLR', talib.WILLR(df['high'], df['low'], df['close'])),\n",
    "#         ('AD', np.round(talib.AD(df['high'], df['low'], df['close'], df['volume']), round_off)),\n",
    "#         ('ADOSC', np.round(talib.ADOSC(df['high'], df['low'], df['close'], df['volume']), round_off)),\n",
    "#         ('OBV', np.round(talib.OBV(df['close'], df['volume']), round_off)),\n",
    "#         ('HT_DCPERIOD', np.round(talib.HT_DCPERIOD(df['close']), round_off)),\n",
    "#         ('HT_DCPHASE', np.round(talib.HT_DCPHASE(df['close']), round_off)),\n",
    "#         *zip(['HT_PHASOR_inphase', 'HT_PHASOR_quadrature'], np.round(talib.HT_PHASOR(df['close']), round_off)),\n",
    "#         ('HT_TRENDMODE', talib.HT_TRENDMODE(df['close'])),\n",
    "#         ('AVGPRICE', talib.AVGPRICE(df['open'], df['high'], df['low'], df['close'])),\n",
    "#         ('MEDPRICE', talib.MEDPRICE(df['high'], df['low'])),\n",
    "#         ('TYPPRICE', talib.TYPPRICE(df['high'], df['low'], df['close'])),\n",
    "#         ('WCLPRICE', talib.WCLPRICE(df['high'], df['low'], df['close'])),\n",
    "#         ('TRANGE', talib.TRANGE(df['high'], df['low'], df['close'])),\n",
    "#     ]\n",
    "\n",
    "#     # Add pattern recognition indicators\n",
    "#     pattern_funcs = [getattr(talib, f) for f in dir(talib) if f.startswith('CDL')]\n",
    "#     indicator_columns.extend([\n",
    "#         (f.__name__, apply_indicator(f, df['open'], df['high'], df['low'], df['close']))\n",
    "#         for f in pattern_funcs\n",
    "#     ])\n",
    "\n",
    "#     # Add statistic functions\n",
    "#     stat_funcs = [talib.LINEARREG, talib.LINEARREG_ANGLE, talib.LINEARREG_INTERCEPT, talib.LINEARREG_SLOPE, talib.TSF, talib.VAR]\n",
    "#     indicator_columns.extend([\n",
    "#         (f.__name__, np.round(apply_indicator(f, df['close']), round_off))\n",
    "#         for f in stat_funcs\n",
    "#     ])\n",
    "\n",
    "#     # Add indicators with timeperiods\n",
    "#     timeperiod_funcs = [\n",
    "#         (talib.BBANDS, lambda t: talib.BBANDS(df['close'], timeperiod=t), ['BB_upper_{}', 'BB_middle_{}', 'BB_lower_{}']),\n",
    "#         (talib.DEMA, lambda t: talib.DEMA(df['close'], timeperiod=t), ['DEMA_{}']),\n",
    "#         (talib.EMA, lambda t: talib.EMA(df['close'], timeperiod=t), ['EMA_{}']),\n",
    "#         (talib.KAMA, lambda t: talib.KAMA(df['close'], timeperiod=t), ['KAMA_{}']),\n",
    "#         (talib.MA, lambda t: talib.MA(df['close'], timeperiod=t), ['MA_{}']),\n",
    "#         (talib.MIDPOINT, lambda t: talib.MIDPOINT(df['close'], timeperiod=t), ['MIDPOINT_{}']),\n",
    "#         (talib.MIDPRICE, lambda t: talib.MIDPRICE(df['high'], df['low'], timeperiod=t), ['MIDPRICE_{}']),\n",
    "#         (talib.SMA, lambda t: talib.SMA(df['close'], timeperiod=t), ['SMA_{}']),\n",
    "#         (talib.TEMA, lambda t: talib.TEMA(df['close'], timeperiod=t), ['TEMA_{}']),\n",
    "#         (talib.TRIMA, lambda t: talib.TRIMA(df['close'], timeperiod=t), ['TRIMA_{}']),\n",
    "#         (talib.WMA, lambda t: talib.WMA(df['close'], timeperiod=t), ['WMA_{}']),\n",
    "#         (talib.ADX, lambda t: talib.ADX(df['high'], df['low'], df['close'], timeperiod=t), ['ADX_{}']),\n",
    "#         (talib.ADXR, lambda t: talib.ADXR(df['high'], df['low'], df['close'], timeperiod=t), ['ADXR_{}']),\n",
    "#         (talib.AROON, lambda t: talib.AROON(df['high'], df['low'], timeperiod=t), ['AROON_up_{}', 'AROON_down_{}']),\n",
    "#         (talib.AROONOSC, lambda t: talib.AROONOSC(df['high'], df['low'], timeperiod=t), ['AROONOSC_{}']),\n",
    "#         (talib.CCI, lambda t: talib.CCI(df['high'], df['low'], df['close'], timeperiod=t), ['CCI_{}']),\n",
    "#         (talib.CMO, lambda t: talib.CMO(df['close'], timeperiod=t), ['CMO_{}']),\n",
    "#         (talib.DX, lambda t: talib.DX(df['high'], df['low'], df['close'], timeperiod=t), ['DX_{}']),\n",
    "#         (talib.MFI, lambda t: talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=t), ['MFI_{}']),\n",
    "#         (talib.MINUS_DI, lambda t: talib.MINUS_DI(df['high'], df['low'], df['close'], timeperiod=t), ['MINUS_DI_{}']),\n",
    "#         (talib.MINUS_DM, lambda t: talib.MINUS_DM(df['high'], df['low'], timeperiod=t), ['MINUS_DM_{}']),\n",
    "#         (talib.MOM, lambda t: talib.MOM(df['close'], timeperiod=t), ['MOM_{}']),\n",
    "#         (talib.PLUS_DI, lambda t: talib.PLUS_DI(df['high'], df['low'], df['close'], timeperiod=t), ['PLUS_DI_{}']),\n",
    "#         (talib.PLUS_DM, lambda t: talib.PLUS_DM(df['high'], df['low'], timeperiod=t), ['PLUS_DM_{}']),\n",
    "#         (talib.ROC, lambda t: talib.ROC(df['close'], timeperiod=t), ['ROC_{}']),\n",
    "#         (talib.ROCP, lambda t: talib.ROCP(df['close'], timeperiod=t), ['ROCP_{}']),\n",
    "#         (talib.ROCR, lambda t: talib.ROCR(df['close'], timeperiod=t), ['ROCR_{}']),\n",
    "#         (talib.ROCR100, lambda t: talib.ROCR100(df['close'], timeperiod=t), ['ROCR100_{}']),\n",
    "#         (talib.RSI, lambda t: talib.RSI(df['close'], timeperiod=t), ['RSI_{}']),\n",
    "#         (talib.ATR, lambda t: talib.ATR(df['high'], df['low'], df['close'], timeperiod=t), ['ATR_{}']),\n",
    "#         (talib.NATR, lambda t: talib.NATR(df['high'], df['low'], df['close'], timeperiod=t), ['NATR_{}']),\n",
    "#         (talib.BETA, lambda t: talib.BETA(df['high'], df['low'], timeperiod=t), ['BETA_{}']),\n",
    "#         (talib.CORREL, lambda t: talib.CORREL(df['high'], df['low'], timeperiod=t), ['CORREL_{}']),\n",
    "#     ]\n",
    "\n",
    "#     # Initialize the progress bar\n",
    "#     total_steps = len(pattern_funcs) + len(stat_funcs) + len(timeperiod_funcs) * len(timeperiods)\n",
    "#     with tqdm(total=total_steps, desc=\"Calculating Indicators\") as pbar:\n",
    "\n",
    "#         # Pattern recognition indicators\n",
    "#         for f in pattern_funcs:\n",
    "#             indicator_columns.append((f.__name__, apply_indicator(f, df['open'], df['high'], df['low'], df['close'])))\n",
    "#             pbar.update(1)  # Update progress bar for each function\n",
    "\n",
    "#         # Statistic functions\n",
    "#         for f in stat_funcs:\n",
    "#             result = np.round(apply_indicator(f, df['close']), round_off)\n",
    "#             indicator_columns.append((f.__name__, result))\n",
    "#             pbar.update(1)  # Update progress bar for each function\n",
    "\n",
    "#         # Timeperiod functions\n",
    "#         for func, func_with_timeperiod, column_names in timeperiod_funcs:\n",
    "#             for timeperiod in timeperiods:\n",
    "#                 result = func_with_timeperiod(timeperiod)\n",
    "#                 if isinstance(result, tuple):\n",
    "#                     for res, col_name in zip(result, column_names):\n",
    "#                         res_rounded = np.round(res, round_off)\n",
    "#                         indicator_columns.append((col_name.format(timeperiod), res_rounded))\n",
    "#                 else:\n",
    "#                     result_rounded = np.round(result, round_off)\n",
    "#                     indicator_columns.append((column_names[0].format(timeperiod), result_rounded))\n",
    "#                 pbar.update(1)  # Update progress bar for each time period\n",
    "\n",
    "#     # Concatenate the new indicator columns with the original DataFrame\n",
    "#     df_indicators = pd.concat([pd.DataFrame(data, columns=[name]) for name, data in indicator_columns], axis=1)\n",
    "    \n",
    "#     return df_indicators\n",
    "# #     # Return the original DataFrame with the new indicator columns\n",
    "# #     return pd.concat([df, df_indicators], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb764fde",
   "metadata": {},
   "source": [
    "# new download and concatenate data but with modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92950392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T14:54:35.759222Z",
     "start_time": "2024-01-15T14:54:35.746474Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_data_and_concatenate(master_dictionary, month_array, day_array):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"Setting up things for {symbol}, {chart_time}\")\n",
    "\n",
    "            # Set up an empty list for the data frames\n",
    "            df_list = []\n",
    "\n",
    "            # Compile the regular expression pattern\n",
    "            pattern = re.compile(rf\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Compile the regular expression pattern for daily zip files\n",
    "            pattern_daily = re.compile(\n",
    "                rf\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Create the new folder path for daily ZIP files\n",
    "            new_daily_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-daily_data\")\n",
    "\n",
    "            # Create the new folder path for ZIP files\n",
    "            new_monthly_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "            # Create the new folder path for CSV files\n",
    "            new_csv_folder_path = os.path.join(output_dir,\n",
    "                                               f\"{symbol}-{chart_time}\")\n",
    "\n",
    "            # Set the file name\n",
    "            concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "            # Construct the file path\n",
    "            concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                                  concatenated_file_name)\n",
    "\n",
    "            download_monthly_data(month_array, symbol, chart_time)  \n",
    "            download_daily_data(day_array, symbol, chart_time)  \n",
    "\n",
    "            # Process the monthly ZIP folder and add to df_list\n",
    "            df_list = process_zip_folder(\n",
    "                new_monthly_zip_folder_path, \n",
    "                pattern, \n",
    "                new_csv_folder_path, \n",
    "                symbol, \n",
    "                chart_time, \n",
    "                df_list,\n",
    "                day_array,\n",
    "            )\n",
    "\n",
    "            # Process the daily ZIP folder and add to df_list\n",
    "            df_list = process_zip_folder(\n",
    "                new_daily_zip_folder_path, \n",
    "                pattern_daily, \n",
    "                new_csv_folder_path, \n",
    "                symbol, \n",
    "                chart_time, \n",
    "                df_list, \n",
    "                day_array,  \n",
    "                is_daily=True\n",
    "            )\n",
    "\n",
    "            # Call the function to concatenate and process the data frames\n",
    "            print(concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time))\n",
    "    return \"Data downloaded and concatenated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c13d0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T12:21:19.054928Z",
     "start_time": "2024-09-15T12:21:19.041012Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wins_losses(master_dictionary, win_perc=0.73, loss_perc=0.4, lookahead_window=10000):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            try:\n",
    "                print(f\"Calculating for {symbol} {chart_time}, {win_perc} : {loss_perc}, with a lookahead window of {lookahead_window}\")\n",
    "\n",
    "                # Define the directory for processed data\n",
    "                processed_data_dir = Path(output_dir) / f\"{symbol}-{chart_time}/processed_data\"\n",
    "                processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Construct the file names\n",
    "                og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "                og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "                new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}_{lookahead_window}cdls.csv\"\n",
    "                new_file_path = processed_data_dir / new_file_name\n",
    "\n",
    "                # Read the CSV file into a dataframe\n",
    "                df = pd.read_csv(og_file_path, usecols=['entry', 'high', 'low', 'open', 'close', 'open_time'])\n",
    "\n",
    "                # Initialize new columns\n",
    "                df[\"if_short\"] = np.nan\n",
    "                df[\"if_long\"] = np.nan\n",
    "                df[\"long_target\"] = df[\"entry\"] * (1 + win_perc / 100)\n",
    "                df[\"short_target\"] = df[\"entry\"] * (1 - win_perc / 100)\n",
    "                df[\"long_stop_loss\"] = df[\"entry\"] * (1 - loss_perc / 100)\n",
    "                df[\"short_stop_loss\"] = df[\"entry\"] * (1 + loss_perc / 100)\n",
    "                df[\"shorts_win_after\"] = np.nan\n",
    "                df[\"longs_win_after\"] = np.nan\n",
    "\n",
    "                # Convert DataFrame columns to numpy arrays for faster processing\n",
    "                highs = df['high'].values\n",
    "                lows = df['low'].values\n",
    "                long_targets = df['long_target'].values\n",
    "                short_targets = df['short_target'].values\n",
    "                long_stop_losses = df['long_stop_loss'].values\n",
    "                short_stop_losses = df['short_stop_loss'].values\n",
    "\n",
    "                # Prepare result arrays\n",
    "                if_long_results = np.full(len(df), np.nan)\n",
    "                if_short_results = np.full(len(df), np.nan)\n",
    "                longs_win_after = np.full(len(df), np.nan)\n",
    "                shorts_win_after = np.full(len(df), np.nan)\n",
    "\n",
    "                # Process each entry row by row\n",
    "                for i in tqdm(range(len(df)), desc=\"Processing Rows\", unit='row'):\n",
    "                    # Define the lookahead window range\n",
    "                    lookahead_end = min(i + lookahead_window, len(df))\n",
    "                    \n",
    "                    # Slice the future highs and lows from current index onwards, respecting the lookahead window\n",
    "                    future_highs = highs[i:lookahead_end]\n",
    "                    future_lows = lows[i:lookahead_end]\n",
    "\n",
    "                    # Early stopping for performance improvement\n",
    "                    long_hit_idx = np.argmax(future_highs >= long_targets[i]) if np.any(future_highs >= long_targets[i]) else np.nan\n",
    "                    long_stop_idx = np.argmax(future_lows <= long_stop_losses[i]) if np.any(future_lows <= long_stop_losses[i]) else np.nan\n",
    "\n",
    "                    short_hit_idx = np.argmax(future_lows <= short_targets[i]) if np.any(future_lows <= short_targets[i]) else np.nan\n",
    "                    short_stop_idx = np.argmax(future_highs >= short_stop_losses[i]) if np.any(future_highs >= short_stop_losses[i]) else np.nan\n",
    "\n",
    "                    # Long trade logic\n",
    "                    if not np.isnan(long_hit_idx) and (np.isnan(long_stop_idx) or long_hit_idx < long_stop_idx):\n",
    "                        if_long_results[i] = 1\n",
    "                        longs_win_after[i] = long_hit_idx\n",
    "                    elif not np.isnan(long_stop_idx):\n",
    "                        if_long_results[i] = -1\n",
    "                    else:\n",
    "                        if_long_results[i] = -1  # No result, mark as loss (-1)\n",
    "\n",
    "                    # Short trade logic\n",
    "                    if not np.isnan(short_hit_idx) and (np.isnan(short_stop_idx) or short_hit_idx < short_stop_idx):\n",
    "                        if_short_results[i] = 1\n",
    "                        shorts_win_after[i] = short_hit_idx\n",
    "                    elif not np.isnan(short_stop_idx):\n",
    "                        if_short_results[i] = -1\n",
    "                    else:\n",
    "                        if_short_results[i] = -1  # No result, mark as loss (-1)\n",
    "\n",
    "                # Update DataFrame with results\n",
    "                df['if_short'] = if_short_results\n",
    "                df['if_long'] = if_long_results\n",
    "                df['shorts_win_after'] = shorts_win_after\n",
    "                df['longs_win_after'] = longs_win_after\n",
    "\n",
    "                # Save the processed data\n",
    "                df.to_csv(new_file_path, index=False)\n",
    "                print(f\"Processed file saved as {new_file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {symbol} {chart_time}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b2bb71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T19:59:08.910872Z",
     "start_time": "2024-09-07T19:59:08.892151Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_indicator_values(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            # Define the directory for input data\n",
    "            data_dir = Path(output_dir) / f\"{symbol}-{chart_time}\"\n",
    "            \n",
    "            # Construct the file name and path\n",
    "            file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "            file_path = data_dir / file_name\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                print(f\"File path for {file_name} doesn't exist. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(df.dtypes)\n",
    "            \n",
    "            # Calculate indicators using TA-Lib\n",
    "            new_columns = calculate_indicators_using_talib(master_dictionary[\"timeperiods\"], df)\n",
    "            \n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df = pd.concat([df, new_columns], axis=1)\n",
    "            new_file_name = f\"{symbol}-{chart_time}_indicators.csv\"\n",
    "            new_file_path = data_dir / new_file_name\n",
    "            df.to_csv(new_file_path, index=False)\n",
    "\n",
    "    return \"Indicators are added to the CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef63632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_file_writing(df, file_path, chunk_size=20000):\n",
    "    # Convert Path object to string or use file_path.suffix\n",
    "    file_extension = file_path.suffix\n",
    "\n",
    "    if file_extension == '.parquet':\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, str(file_path), compression='snappy')\n",
    "    elif file_extension == '.csv':\n",
    "        total_rows = len(df)\n",
    "        with tqdm(total=total_rows, desc=\"Writing file\") as pbar:\n",
    "            for i in range(0, total_rows, chunk_size):\n",
    "                chunk = df.iloc[i:i+chunk_size]\n",
    "                mode = 'w' if i == 0 else 'a'\n",
    "                chunk.to_csv(str(file_path), mode=mode, header=(i == 0), index=False)\n",
    "                pbar.update(len(chunk))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use .parquet or .csv\")\n",
    "\n",
    "def calculate_indicator_values_new(master_dictionary, win_perc=0.73, loss_perc=0.4, use_parquet=False):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in tqdm(master_dictionary[\"symbols\"], desc=\"Processing symbols\"):\n",
    "        for chart_time in tqdm(master_dictionary[\"chart_times\"], desc=f\"Processing chart times for {symbol}\", leave=False):\n",
    "            # Define the directory for input data\n",
    "            data_dir = Path(output_dir) / f\"{symbol}-{chart_time}\"\n",
    "            \n",
    "            # Construct the file name and path\n",
    "            file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "            file_path = data_dir / file_name\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                print(f\"File path for {file_name} doesn't exist. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Data types for {symbol}-{chart_time}:\")\n",
    "            print(df.dtypes)\n",
    "            \n",
    "            # Calculate indicators using TA-Lib\n",
    "            print(f\"Calculating indicators for {symbol}-{chart_time}\")\n",
    "            new_columns = calculate_indicators_using_talib_new(master_dictionary[\"timeperiods\"], df)\n",
    "            \n",
    "            # Combine original dataframe with new indicators\n",
    "            df_with_indicators = pd.concat([df, new_columns], axis=1)\n",
    "            \n",
    "            # Save the updated dataframe\n",
    "            if use_parquet:\n",
    "                new_file_name = f\"{symbol}-{chart_time}_indicators.parquet\"\n",
    "            else:\n",
    "                new_file_name = f\"{symbol}-{chart_time}_indicators.csv\"\n",
    "            \n",
    "            new_file_path = data_dir / new_file_name\n",
    "            \n",
    "            print(f\"Writing data for {symbol}-{chart_time} to {new_file_name}\")\n",
    "            optimized_file_writing(df_with_indicators, new_file_path)\n",
    "            \n",
    "            print(f\"Completed processing {symbol}-{chart_time}\")\n",
    "\n",
    "    return \"Indicators are added and saved to files\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_between_dates(master_dictionary, start_date, end_date, data_path):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"displaying filtered for {symbol}-{chart_time}\")\n",
    "            return filter_data_for_symbol_charttime(symbol, chart_time, start_date, end_date, data_path)\n",
    "\n",
    "def filter_data_for_symbol_charttime(symbol, chart_time, start_date, end_date, data_path):\n",
    "    # Read the CSV file into a data frame\n",
    "    df = pd.read_csv(Path(data_path) / f\"{symbol}-{chart_time}\" / f\"{symbol}-{chart_time}.csv\")\n",
    "    \n",
    "    # Convert 'open_time' to datetime format\n",
    "    df['open_time'] = pd.to_datetime(df['open_time']).dt.date\n",
    "    \n",
    "    # Convert start_date and end_date to datetime objects\n",
    "    start_date = pd.to_datetime(start_date, format='%Y%m%d').date()\n",
    "    end_date = pd.to_datetime(end_date, format='%Y%m%d').date()\n",
    "\n",
    "    # Filter data between start_date and end_date based on the date part of 'open_time'\n",
    "    df_filtered = df[(df['open_time'] >= start_date) & (df['open_time'] <= end_date)]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "#sample function call: read_data_between_dates(master_dictionary, \"20231206\", \"20231206\", output_dir)\n",
    "# output_dir is getting initialized when we run the creating_arrays notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data creation utilities successfully initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8ec3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
