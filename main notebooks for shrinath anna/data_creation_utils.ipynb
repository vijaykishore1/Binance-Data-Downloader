{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e4da54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T12:21:38.112708Z",
     "start_time": "2024-09-15T12:21:26.185690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\downloaded_data\n",
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\extracted_data\n",
      "['1000BONKUSDC', '1000BONKUSDT', '1000BTTCUSDT', '1000FLOKIUSDT', '1000LUNCBUSD', '1000LUNCUSDT', '1000PEPEUSDC', '1000PEPEUSDT', '1000RATSUSDT', '1000SATSUSDT', '1000SHIBBUSD', '1000SHIBUSDC', '1000SHIBUSDT', '1000XECUSDT', '1INCHUSDT', 'AAVEUSDT', 'ACEUSDT', 'ACHUSDT', 'ADABUSD', 'ADAUSDT', 'AEVOUSDT', 'AGIXBUSD', 'AGIXUSDT', 'AGLDUSDT', 'AIUSDT', 'AKROUSDT', 'ALGOUSDT', 'ALICEUSDT', 'ALPACAUSDT', 'ALPHAUSDT', 'ALTUSDT', 'AMBBUSD', 'AMBUSDT', 'ANCBUSD', 'ANCUSDT', 'ANKRUSDT', 'ANTUSDT', 'APEBUSD', 'APEUSDT', 'API3USDT', 'APTBUSD', 'APTUSDT', 'ARBUSDC', 'ARBUSDT', 'ARKMUSDT', 'ARKUSDT', 'ARPAUSDT', 'ARUSDT', 'ASTRUSDT', 'ATAUSDT', 'ATOMUSDT', 'AUCTIONBUSD', 'AUCTIONUSDT', 'AUDIOUSDT', 'AVAXBUSD', 'AVAXUSDC', 'AVAXUSDT', 'AXLUSDT', 'AXSUSDT', 'BADGERUSDT', 'BAKEUSDT', 'BALUSDT', 'BANANAUSDT', 'BANDUSDT', 'BATUSDT', 'BBUSDT', 'BCHUSDC', 'BCHUSDT', 'BEAMXUSDT', 'BELUSDT', 'BICOUSDT', 'BIGTIMEUSDT', 'BLUEBIRDUSDT', 'BLURUSDT', 'BLZUSDT', 'BNBBUSD', 'BNBUSDC', 'BNBUSDT', 'BNTUSDT', 'BNXUSDT', 'BNXUSDTSETTLED', 'BOMEUSDC', 'BOMEUSDT', 'BONDUSDT', 'BRETTUSDT', 'BSVUSDT', 'BTCBUSD', 'BTCBUSD_210129', 'BTCBUSD_210226', 'BTCDOMUSDT', 'BTCSTUSDT', 'BTCUSDC', 'BTCUSDT', 'BTCUSDT_210326', 'BTCUSDT_210625', 'BTCUSDT_210924', 'BTCUSDT_211231', 'BTCUSDT_220325', 'BTCUSDT_220624', 'BTCUSDT_220930', 'BTCUSDT_221230', 'BTCUSDT_230331', 'BTCUSDT_230630', 'BTCUSDT_230929', 'BTCUSDT_231229', 'BTCUSDT_240329', 'BTCUSDT_240628', 'BTCUSDT_240927', 'BTCUSDT_241227', 'BTSUSDT', 'BTTUSDT', 'BZRXUSDT', 'C98USDT', 'CAKEUSDT', 'CELOUSDT', 'CELRUSDT', 'CFXUSDT', 'CHESSUSDT', 'CHRUSDT', 'CHZUSDT', 'CKBUSDT', 'COCOSUSDT', 'COMBOUSDT', 'COMPUSDT', 'COTIUSDT', 'CRVUSDC', 'CRVUSDT', 'CTKUSDT', 'CTSIUSDT', 'CVCUSDT', 'CVXBUSD', 'CVXUSDT', 'CYBERUSDT', 'DARUSDT', 'DASHUSDT', 'DEFIUSDT', 'DENTUSDT', 'DGBUSDT', 'DODOBUSD', 'DODOUSDT', 'DODOXUSDT', 'DOGEBUSD', 'DOGEUSDC', 'DOGEUSDT', 'DOGSUSDT', 'DOTBUSD', 'DOTECOUSDT', 'DOTUSDT', 'DUSKUSDT', 'DYDXUSDT', 'DYMUSDT', 'EDUUSDT', 'EGLDUSDT', 'ENAUSDC', 'ENAUSDT', 'ENJUSDT', 'ENSUSDT', 'EOSUSDT', 'ETCBUSD', 'ETCUSDT', 'ETHBTC', 'ETHBUSD', 'ETHFIUSDC', 'ETHFIUSDT', 'ETHUSDC', 'ETHUSDT', 'ETHUSDT_210326', 'ETHUSDT_210625', 'ETHUSDT_210924', 'ETHUSDT_211231', 'ETHUSDT_220325', 'ETHUSDT_220624', 'ETHUSDT_220930', 'ETHUSDT_221230', 'ETHUSDT_230331', 'ETHUSDT_230630', 'ETHUSDT_230929', 'ETHUSDT_231229', 'ETHUSDT_240329', 'ETHUSDT_240628', 'ETHUSDT_240927', 'ETHUSDT_241227', 'ETHWUSDT', 'FETUSDT', 'FILBUSD', 'FILUSDC', 'FILUSDT', 'FLMUSDT', 'FLOWUSDT', 'FOOTBALLUSDT', 'FRONTUSDT', 'FTMBUSD', 'FTMUSDT', 'FTTBUSD', 'FTTUSDT', 'FXSUSDT', 'GALABUSD', 'GALAUSDT', 'GALBUSD', 'GALUSDT', 'GASUSDT', 'GLMRUSDT', 'GLMUSDT', 'GMTBUSD', 'GMTUSDT', 'GMXUSDT', 'GRTUSDT', 'GTCUSDT', 'GUSDT', 'HBARUSDT', 'HFTUSDT', 'HIFIUSDT', 'HIGHUSDT', 'HNTUSDT', 'HOOKUSDT', 'HOTUSDT', 'ICPBUSD', 'ICPUSDT', 'ICPUSDT_SETTLED', 'ICXUSDT', 'IDEXUSDT', 'IDUSDT', 'ILVUSDT', 'IMXUSDT', 'INJUSDT', 'IOSTUSDT', 'IOTAUSDT', 'IOTXUSDT', 'IOUSDT', 'JASMYUSDT', 'JOEUSDT', 'JTOUSDT', 'JUPUSDT', 'KASUSDT', 'KAVAUSDT', 'KEEPUSDT', 'KEYUSDT', 'KLAYUSDT', 'KNCUSDT', 'KSMUSDT', 'LDOBUSD', 'LDOUSDT', 'LENDUSDT', 'LEVERBUSD', 'LEVERUSDT', 'LINAUSDT', 'LINKBUSD', 'LINKUSDC', 'LINKUSDT', 'LISTAUSDT', 'LITUSDT', 'LOOMUSDT', 'LPTUSDT', 'LQTYUSDT', 'LRCUSDT', 'LSKUSDT', 'LTCBUSD', 'LTCUSDC', 'LTCUSDT', 'LUNA2BUSD', 'LUNA2USDT', 'LUNABUSD', 'LUNAUSDT', 'MAGICUSDT', 'MANAUSDT', 'MANTAUSDT', 'MASKUSDT', 'MATICBUSD', 'MATICUSDC', 'MATICUSDT', 'MAVIAUSDT', 'MAVUSDT', 'MBLUSDT', 'MBOXUSDT', 'MDTUSDT', 'MEMEUSDT', 'METISUSDT', 'MEWUSDT', 'MINAUSDT', 'MINAUSDTSETTLED', 'MKRUSDT', 'MOVRUSDT', 'MTLUSDT', 'MYROUSDT', 'NEARBUSD', 'NEARUSDC', 'NEARUSDT', 'NEOUSDC', 'NEOUSDT', 'NFPUSDT', 'NKNUSDT', 'NMRUSDT', 'NOTUSDT', 'NTRNUSDT', 'NULSUSDT', 'NUUSDT', 'OCEANUSDT', 'OGNUSDT', 'OMGUSDT', 'OMNIUSDT', 'OMUSDT', 'ONDOUSDT', 'ONEUSDT', 'ONGUSDT', 'ONTUSDT', 'OPUSDT', 'ORBSUSDT', 'ORDIUSDC', 'ORDIUSDT', 'OXTUSDT', 'PENDLEUSDT', 'PEOPLEUSDT', 'PERPUSDT', 'PHBBUSD', 'PHBUSDT', 'PIXELUSDT', 'POLYXUSDT', 'POPCATUSDT', 'PORTALUSDT', 'POWRUSDT', 'PYTHUSDT', 'QNTUSDT', 'QTUMUSDT', 'RADUSDT', 'RAREUSDT', 'RAYUSDT', 'RDNTUSDT', 'REEFUSDT', 'RENDERUSDT', 'RENUSDT', 'REZUSDT', 'RIFUSDT', 'RLCUSDT', 'RNDRUSDT', 'RONINUSDT', 'ROSEUSDT', 'RSRUSDT', 'RUNEUSDT', 'RVNUSDT', 'SAGAUSDT', 'SANDBUSD', 'SANDUSDT', 'SCUSDT', 'SEIUSDT', 'SFPUSDT', 'SKLUSDT', 'SLPUSDT', 'SNTUSDT', 'SNXUSDT', 'SOLBUSD', 'SOLUSDC', 'SOLUSDT', 'SPELLUSDT', 'SRMUSDT', 'SSVUSDT', 'STEEMUSDT', 'STGUSDT', 'STMXUSDT', 'STORJUSDT', 'STPTUSDT', 'STRAXUSDT', 'STRKUSDT', 'STXUSDT', 'SUIUSDC', 'SUIUSDT', 'SUNUSDT', 'SUPERUSDT', 'SUSHIUSDT', 'SXPUSDT', 'SYNUSDT', 'SYSUSDT', 'TAOUSDT', 'THETAUSDT', 'TIAUSDC', 'TIAUSDT', 'TLMBUSD', 'TLMUSDT', 'TLMUSDTSETTLED', 'TNSRUSDT', 'TOKENUSDT', 'TOMOUSDT', 'TONUSDT', 'TRBUSDT', 'TRUUSDT', 'TRXBUSD', 'TRXUSDT', 'TURBOUSDT', 'TUSDT', 'TWTUSDT', 'UMAUSDT', 'UNFIUSDT', 'UNIBUSD', 'UNIUSDT', 'USDCUSDT', 'USTCUSDT', 'VANRYUSDT', 'VETUSDT', 'VIDTUSDT', 'VOXELUSDT', 'WAVESBUSD', 'WAVESUSDT', 'WAXPUSDT', 'WIFUSDC', 'WIFUSDT', 'WLDUSDC', 'WLDUSDT', 'WOOUSDT', 'WUSDT', 'XAIUSDT', 'XEMUSDT', 'XLMUSDT', 'XMRUSDT', 'XRPBUSD', 'XRPUSDC', 'XRPUSDT', 'XTZUSDT', 'XVGUSDT', 'XVSUSDT', 'YFIIUSDT', 'YFIUSDT', 'YGGUSDT', 'ZECUSDT', 'ZENUSDT', 'ZETAUSDT', 'ZILUSDT', 'ZKUSDT', 'ZROUSDT', 'ZRXUSDT']\n",
      "['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02', '2024-03', '2024-04', '2024-05', '2024-07', '2024-08', '2024-09', '2024-10', '2024-11', '2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05', '2025-06', '2025-07', '2025-08', '2025-09', '2025-10', '2025-11', '2025-12', '2026-01', '2026-02', '2026-03', '2026-04', '2026-05', '2026-06', '2026-07', '2026-08', '2026-09', '2026-10', '2026-11', '2026-12', '2027-01', '2027-02', '2027-03', '2027-04', '2027-05', '2027-06', '2027-07', '2027-08', '2027-09', '2027-10', '2027-11', '2027-12', '2028-01', '2028-02', '2028-03', '2028-04', '2028-05', '2028-06', '2028-07', '2028-08', '2028-09', '2028-10', '2028-11', '2028-12', '2029-01', '2029-03', '2029-04', '2029-05', '2029-06', '2029-07', '2029-08', '2029-09', '2029-10', '2029-11', '2029-12', '2030-01', '2030-02', '2030-03', '2030-04', '2030-05', '2030-06', '2030-07', '2030-08', '2030-09', '2030-10', '2030-11', '2030-12', '2031-01', '2031-02', '2031-03', '2031-04', '2031-05', '2031-06', '2031-07', '2031-08', '2031-09', '2031-10', '2031-11', '2031-12', '2032-01', '2032-02', '2032-03', '2032-04', '2032-05', '2032-06', '2032-07', '2032-08', '2032-09', '2032-10', '2032-11', '2032-12', '2033-01', '2033-02', '2033-03', '2033-04', '2033-05', '2033-06', '2033-07', '2033-08', '2033-10', '2033-11', '2033-12', '2034-01', '2034-02', '2034-03', '2034-04', '2034-05', '2034-06', '2034-07', '2034-08', '2034-09', '2034-10', '2034-11', '2034-12', '2035-01', '2035-02', '2035-03', '2035-04', '2035-05', '2035-06', '2035-07', '2035-08', '2035-09', '2035-10', '2035-11', '2035-12', '2036-01', '2036-02', '2036-03', '2036-04', '2036-05', '2036-06', '2036-07', '2036-08', '2036-09', '2036-10', '2036-11', '2036-12', '2037-01', '2037-02', '2037-03', '2037-04', '2037-05', '2037-06', '2037-07', '2037-08', '2037-09', '2037-10', '2037-11', '2037-12', '2038-01', '2038-03', '2038-04', '2038-05', '2038-06', '2038-07', '2038-08', '2038-09', '2038-10', '2038-11', '2038-12', '2039-01', '2039-02', '2039-03', '2039-04', '2039-05', '2039-06', '2039-07', '2039-08', '2039-09', '2039-10', '2039-11', '2039-12', '2040-01', '2040-02', '2040-03', '2040-04', '2040-05', '2040-06', '2040-07', '2040-08', '2040-09', '2040-10', '2040-11', '2040-12', '2041-01', '2041-02', '2041-03', '2041-04', '2041-05', '2041-06', '2041-07', '2041-08', '2041-09', '2041-10', '2041-11', '2041-12', '2042-01', '2042-02', '2042-03', '2042-04', '2042-05', '2042-06', '2042-07', '2042-08', '2042-09', '2042-10', '2042-12', '2043-01', '2043-02', '2043-03', '2043-04', '2043-05', '2043-06', '2043-07', '2043-08', '2043-09', '2043-10', '2043-11', '2043-12', '2044-01', '2044-02', '2044-03', '2044-04', '2044-05', '2044-06', '2044-07', '2044-08', '2044-09', '2044-10', '2044-11', '2044-12', '2045-01', '2045-02', '2045-03', '2045-04', '2045-05', '2045-06', '2045-07', '2045-08', '2045-09', '2045-10', '2045-11', '2045-12', '2046-01', '2046-02', '2046-03', '2046-04', '2046-05', '2046-06', '2046-07', '2046-08', '2046-09', '2046-10', '2046-11', '2046-12', '2047-01', '2047-02', '2047-03', '2047-05', '2047-06', '2047-07', '2047-08', '2047-09', '2047-10', '2047-11', '2047-12', '2048-01', '2048-02', '2048-03', '2048-04', '2048-05', '2048-06', '2048-07', '2048-08', '2048-09', '2048-10', '2048-11', '2048-12', '2049-01', '2049-02', '2049-03', '2049-04', '2049-05', '2049-06', '2049-07', '2049-08', '2049-09', '2049-10', '2049-11', '2049-12', '2050-01', '2050-02', '2050-03', '2050-04', '2050-05', '2050-06', '2050-07', '2050-08', '2050-09', '2050-10', '2050-11', '2050-12', '2051-01', '2051-02', '2051-03', '2051-04', '2051-05', '2051-06', '2051-07', '2051-08', '2051-09', '2051-10', '2051-11', '2051-12', '2052-01', '2052-03', '2052-04', '2052-05', '2052-06', '2052-07', '2052-08', '2052-09', '2052-10', '2052-11', '2052-12', '2053-01', '2053-02', '2053-03', '2053-04', '2053-05', '2053-06', '2053-07', '2053-08', '2053-09', '2053-10', '2053-11', '2053-12', '2054-01', '2054-02', '2054-03', '2054-04', '2054-05', '2054-06', '2054-07', '2054-08', '2054-09', '2054-10', '2054-11', '2054-12', '2055-01', '2055-02', '2055-03', '2055-04', '2055-05', '2055-06', '2055-07', '2055-08', '2055-09', '2055-10', '2055-11', '2055-12', '2056-01', '2056-02', '2056-03', '2056-04', '2056-05', '2056-06', '2056-07', '2056-08', '2056-10', '2056-11', '2056-12', '2057-01', '2057-02', '2057-03', '2057-04', '2057-05', '2057-06', '2057-07', '2057-08', '2057-09', '2057-10', '2057-11', '2057-12', '2058-01', '2058-02', '2058-03', '2058-04', '2058-05', '2058-06', '2058-07', '2058-08', '2058-09', '2058-10', '2058-11', '2058-12', '2059-01', '2059-02', '2059-03', '2059-04', '2059-05', '2059-06', '2059-07', '2059-08', '2059-09', '2059-10', '2059-11', '2059-12', '2060-01', '2060-02', '2060-03', '2060-04', '2060-05', '2060-06', '2060-07', '2060-08', '2060-09', '2060-10', '2060-11', '2060-12', '2061-01', '2061-03', '2061-04', '2061-05', '2061-06', '2061-07', '2061-08', '2061-09', '2061-10', '2061-11', '2061-12', '2062-01', '2062-02', '2062-03', '2062-04', '2062-05', '2062-06', '2062-07', '2062-08', '2062-09', '2062-10', '2062-11', '2062-12', '2063-01', '2063-02', '2063-03', '2063-04', '2063-05', '2063-06', '2063-07', '2063-08', '2063-09', '2063-10', '2063-11', '2063-12', '2064-01', '2064-02', '2064-03', '2064-04', '2064-05', '2064-06', '2064-07', '2064-08', '2064-09', '2064-10', '2064-11', '2064-12', '2065-01', '2065-02', '2065-03', '2065-04', '2065-05', '2065-06', '2065-07', '2065-08', '2065-09', '2065-10', '2065-12', '2066-01', '2066-02', '2066-03', '2066-04', '2066-05', '2066-06', '2066-07', '2066-08', '2066-09', '2066-10', '2066-11', '2066-12', '2067-01', '2067-02', '2067-03', '2067-04', '2067-05', '2067-06', '2067-07', '2067-08', '2067-09', '2067-10', '2067-11', '2067-12', '2068-01', '2068-02', '2068-03', '2068-04', '2068-05', '2068-06', '2068-07', '2068-08', '2068-09', '2068-10', '2068-11', '2068-12', '2069-01', '2069-02', '2069-03', '2069-04', '2069-05', '2069-06', '2069-07', '2069-08', '2069-09', '2069-10', '2069-11', '2069-12', '2070-01', '2070-02', '2070-03', '2070-05', '2070-06', '2070-07', '2070-08', '2070-09', '2070-10', '2070-11', '2070-12', '2071-01', '2071-02', '2071-03', '2071-04', '2071-05', '2071-06', '2071-07', '2071-08', '2071-09', '2071-10', '2071-11', '2071-12', '2072-01', '2072-02', '2072-03', '2072-04', '2072-05', '2072-06', '2072-07', '2072-08', '2072-09', '2072-10', '2072-11', '2072-12', '2073-01', '2073-02', '2073-03', '2073-04', '2073-05', '2073-06', '2073-07', '2073-08', '2073-09', '2073-10', '2073-11', '2073-12', '2074-01', '2074-02', '2074-03', '2074-04', '2074-05', '2074-06', '2074-07', '2074-08', '2074-09', '2074-10', '2074-11', '2074-12', '2075-01', '2075-03', '2075-04', '2075-05', '2075-06', '2075-07', '2075-08', '2075-09', '2075-10', '2075-11', '2075-12', '2076-01', '2076-02', '2076-03', '2076-04', '2076-05', '2076-06', '2076-07', '2076-08', '2076-09', '2076-10', '2076-11', '2076-12', '2077-01', '2077-02', '2077-03', '2077-04', '2077-05', '2077-06', '2077-07', '2077-08', '2077-09', '2077-10', '2077-11', '2077-12', '2078-01', '2078-02', '2078-03', '2078-04', '2078-05', '2078-06', '2078-07', '2078-08', '2078-09', '2078-10', '2078-11', '2078-12', '2079-01', '2079-02', '2079-03', '2079-04', '2079-05', '2079-07', '2079-08', '2079-09', '2079-10', '2079-11', '2079-12']\n",
      "['12h', '15m', '1d', '1h', '1m', '1mo', '1w', '2h', '30m', '3d', '3m', '4h', '5m', '6h', '8h']\n",
      "Symbol, month, and chart arrays are successfully created\n"
     ]
    }
   ],
   "source": [
    "%run ./creating_arrays.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f291bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:49:06.625037Z",
     "start_time": "2024-09-08T14:49:05.294447Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import talib\n",
    "import datetime\n",
    "import glob\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import inspect\n",
    "import talib\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.request\n",
    "from numba import njit, prange\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969740d1",
   "metadata": {},
   "source": [
    "# all tiny modular functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fe5526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T14:43:20.085846Z",
     "start_time": "2024-01-15T14:43:20.022109Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_monthly_data(month_array, symbol, chart_time):\n",
    "    #downloading monthly data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for month in month_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print(f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"you're already up to date for monthly data for {symbol},{chart_time}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def download_daily_data(day_array, symbol, chart_time):\n",
    "    #downloading daily data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for day in day_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"you're already up to date for daily data for {symbol},{chart_time}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def construct_csv_file_path(folder_path,\n",
    "                            symbol,\n",
    "                            chart_time,\n",
    "                            file,\n",
    "                            is_daily=False):\n",
    "    if is_daily:\n",
    "        # For daily data, use a different pattern\n",
    "        return os.path.join(\n",
    "            folder_path,\n",
    "            f\"{symbol}-{chart_time}-{file.split('-')[-3]}-{file.split('-')[-2]}-{file.split('-')[-1][:-4]}.csv\"\n",
    "        )\n",
    "    else:\n",
    "        # For monthly data, use the original pattern\n",
    "        return os.path.join(folder_path,\n",
    "                            f\"{symbol}-{chart_time}{file[-12:-4]}.csv\")\n",
    "\n",
    "\n",
    "def process_zip_folder(folder_path,\n",
    "                       pattern,\n",
    "                       new_csv_folder_path,\n",
    "                       symbol,\n",
    "                       chart_time,\n",
    "                       df_list,\n",
    "                       daily_array=None,\n",
    "                       is_daily=False):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return df_list\n",
    "\n",
    "    # Iterate over files in the directory\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file matches the pattern\n",
    "        if pattern.match(file_name):\n",
    "            if is_daily and daily_array:\n",
    "                # Check if any date in the daily_array is in the file_name\n",
    "                if not any(date in file_name for date in daily_array):\n",
    "                    continue\n",
    "\n",
    "            # Construct the file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Extract the ZIP file\n",
    "            try:\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(new_csv_folder_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting file {file_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Construct the CSV file path using the helper function\n",
    "            csv_file_path = construct_csv_file_path(new_csv_folder_path,\n",
    "                                                    symbol, chart_time,\n",
    "                                                    file_name, is_daily)\n",
    "\n",
    "            # Read the CSV file into a data frame, ignoring the headers\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file_path, header=None)\n",
    "                # Remove the first row (which contains the header)\n",
    "                df = df.iloc[1:]\n",
    "                # Add it to the list\n",
    "                df_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {csv_file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return df_list\n",
    "\n",
    "\n",
    "def get_correct_headers(new_csv_folder_path):\n",
    "    possible_headers = [\n",
    "        'open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time',\n",
    "        'quote_volume', 'count', 'taker_buy_volume', 'taker_buy_quote_volume',\n",
    "        'ignore'\n",
    "    ]\n",
    "\n",
    "    for file_name in os.listdir(new_csv_folder_path):\n",
    "        file_path = os.path.join(new_csv_folder_path, file_name)\n",
    "        try:\n",
    "            # Read the first row to get headers\n",
    "            headers = pd.read_csv(file_path, nrows=1).columns.tolist()\n",
    "            # Check if at least 2 headers match\n",
    "            matches = [\n",
    "                header for header in headers if header in possible_headers\n",
    "            ]\n",
    "            if len(matches) >= 2:\n",
    "                print(f\"Found matching headers in {file_name}: {matches}\")\n",
    "                return headers\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    raise ValueError(\"Could not find matching headers in any CSV files.\")\n",
    "\n",
    "\n",
    "def concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time):\n",
    "    # Get correct headers from a CSV file\n",
    "    try:\n",
    "        correct_headers = get_correct_headers(new_csv_folder_path)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return \"Error finding headers\"\n",
    "\n",
    "    # Concatenate the data frames in the list\n",
    "    df_final = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Check if df_final has headers or not\n",
    "    if df_final.columns[0] not in correct_headers:\n",
    "        print(\"Updating older CSVs with correct headers.\")\n",
    "        # Update headers for older CSVs that lack them\n",
    "        for file_name in os.listdir(new_csv_folder_path):\n",
    "            file_path = os.path.join(new_csv_folder_path, file_name)\n",
    "            try:\n",
    "                df_old = pd.read_csv(file_path, header=None)\n",
    "                # Ensure we only update files with correct structure\n",
    "                if len(df_old.columns) == len(correct_headers):\n",
    "                    df_old.columns = correct_headers\n",
    "                    df_old.to_csv(file_path, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating file {file_name}: {e}\")\n",
    "\n",
    "    # Set the headers as the column names of the final dataframe\n",
    "    df_final.columns = correct_headers\n",
    "\n",
    "    # Convert 'open_time' and 'close_time' columns to datetime\n",
    "    try:\n",
    "        df_final['open_time'] = pd.to_datetime(\n",
    "            df_final['open_time'],\n",
    "            unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "        df_final['close_time'] = pd.to_datetime(\n",
    "            df_final['close_time'],\n",
    "            unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "    except KeyError as e:\n",
    "        print(f\"Column not found for conversion: {e}\")\n",
    "        return \"Error in date conversion\"\n",
    "\n",
    "    # Delete the 'ignore' column if it exists\n",
    "    if 'ignore' in df_final.columns:\n",
    "        df_final = df_final.drop(['ignore'], axis=1)\n",
    "\n",
    "    # Add a new column called 'entry' that will take previous close\n",
    "    df_final['entry'] = df_final['open']\n",
    "\n",
    "    # Set the file name\n",
    "    concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "    # Construct the file path\n",
    "    concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                          concatenated_file_name)\n",
    "\n",
    "    # Write the data frame to the CSV file\n",
    "    df_final.to_csv(concatenated_file_path, index=False)\n",
    "\n",
    "    directory_final = Path(\n",
    "        concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "    # Deleting all the other CSVs except the final concatenated file\n",
    "    for file_path in directory_final.iterdir():\n",
    "        if file_path.is_file():\n",
    "            # Ensure we only delete individual CSVs used for concatenation\n",
    "            if file_path.name.startswith(\n",
    "                    f\"{symbol}-{chart_time}-\") and file_path.name.endswith(\n",
    "                        '.csv') and file_path.name != concatenated_file_name:\n",
    "                file_path.unlink()\n",
    "\n",
    "    return \"Data concatenated, individual CSVs deleted\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197992a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indicators_using_talib(timeperiods, df):\n",
    "    new_columns = pd.DataFrame()\n",
    "\n",
    "    # List to store indicator columns\n",
    "    indicator_columns = []\n",
    "    indicator_columns.append(('HT_TRENDLINE', talib.HT_TRENDLINE(df['close'])))\n",
    "    # indicator_columns.append(('MAMA', df['MAMA']), ('FAMA', df['FAMA']))\n",
    "    # indicator_columns.append(('MAVP', df['MAVP']))\n",
    "    indicator_columns.append(\n",
    "        ('SAR', talib.SAR(df['high'], df['low'], acceleration=0, maximum=0)))\n",
    "    indicator_columns.append(('SAREXT', talib.SAREXT(df['high'], df['low'])))\n",
    "    indicator_columns.append(\n",
    "        ('T3', talib.T3(df['close'], timeperiod=5, vfactor=0)))\n",
    "    # Momentum Indicators\n",
    "    indicator_columns.append(\n",
    "        ('APO', talib.APO(df['close'], fastperiod=12, slowperiod=26)))\n",
    "    indicator_columns.append(\n",
    "        ('BOP', talib.BOP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    macd, macd_signal, macd_hist = talib.MACD(df['close'],\n",
    "                                              fastperiod=12,\n",
    "                                              slowperiod=26,\n",
    "                                              signalperiod=9)\n",
    "    indicator_columns.append(('MACD', macd))\n",
    "    indicator_columns.append(('MACD_signal', macd_signal))\n",
    "    indicator_columns.append(('MACD_hist', macd_hist))\n",
    "    indicator_columns.append(\n",
    "        ('PPO', talib.PPO(df['close'], fastperiod=12, slowperiod=26,\n",
    "                          matype=0)))\n",
    "    indicator_columns.append(('TRIX', talib.TRIX(df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('ULTOSC', talib.ULTOSC(df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('WILLR', talib.WILLR(df['high'], df['low'], df['close'])))\n",
    "\n",
    "    #     # Not Working ATM\n",
    "    #     indicator_columns.append(('STOCH', talib.STOCH(df['high'], df['low'], df['close'])))\n",
    "    #     indicator_columns.append(('STOCHF', talib.STOCHF(df['high'], df['low'], df['close'])))\n",
    "    #     indicator_columns.append(('STOCHRSI', talib.STOCHRSI(df['close'])))\n",
    "    #     indicator_columns.append(('MACDEXT', talib.MACDEXT(df['close'], fastperiod=12, fastmatype=0, slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)))\n",
    "    #     indicator_columns.append(('MACDFIX', talib.MACDFIX(df['close'], signalperiod=9)))\n",
    "\n",
    "    #########Volume Indicators\n",
    "    indicator_columns.append(\n",
    "        ('AD', talib.AD(df['high'], df['low'], df['close'], df['volume'])))\n",
    "    indicator_columns.append(('ADOSC',\n",
    "                              talib.ADOSC(df['high'],\n",
    "                                          df['low'],\n",
    "                                          df['close'],\n",
    "                                          df['volume'],\n",
    "                                          fastperiod=3,\n",
    "                                          slowperiod=10)))\n",
    "    indicator_columns.append(('OBV', talib.OBV(df['close'], df['volume'])))\n",
    "\n",
    "    #########Cycle Indicators\n",
    "    indicator_columns.append(('HT_DCPERIOD', talib.HT_DCPERIOD(df['close'])))\n",
    "    indicator_columns.append(('HT_DCPHASE', talib.HT_DCPHASE(df['close'])))\n",
    "    phasor_inphase, phasor_quadrature = talib.HT_PHASOR(df['close'])\n",
    "    indicator_columns.append(('HT_PHASOR_inphase', phasor_inphase))\n",
    "    indicator_columns.append(('HT_PHASOR_quadrature', phasor_quadrature))\n",
    "    # indicator_columns.append(('HT_SINE', talib.HT_SINE(df['close'])))\n",
    "    indicator_columns.append(('HT_TRENDMODE', talib.HT_TRENDMODE(df['close'])))\n",
    "\n",
    "    #########Price Transform\n",
    "    indicator_columns.append(('AVGPRICE',\n",
    "                              talib.AVGPRICE(df['open'], df['high'], df['low'],\n",
    "                                             df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('MEDPRICE', talib.MEDPRICE(df['high'], df['low'])))\n",
    "    indicator_columns.append(\n",
    "        ('TYPPRICE', talib.TYPPRICE(df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('WCLPRICE', talib.WCLPRICE(df['high'], df['low'], df['close'])))\n",
    "    #########Volatility Indicators\n",
    "    indicator_columns.append(\n",
    "        ('TRANGE', talib.TRANGE(df['high'], df['low'], df['close'])))\n",
    "    #########Pattern Recognition\n",
    "    indicator_columns.append(('CDL2CROWS',\n",
    "                              talib.CDL2CROWS(df['open'], df['high'],\n",
    "                                              df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3BLACKCROWS',\n",
    "                              talib.CDL3BLACKCROWS(df['open'], df['high'],\n",
    "                                                   df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3INSIDE',\n",
    "                              talib.CDL3INSIDE(df['open'], df['high'],\n",
    "                                               df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3LINESTRIKE',\n",
    "                              talib.CDL3LINESTRIKE(df['open'], df['high'],\n",
    "                                                   df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3OUTSIDE',\n",
    "                              talib.CDL3OUTSIDE(df['open'], df['high'],\n",
    "                                                df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3STARSINSOUTH',\n",
    "                              talib.CDL3STARSINSOUTH(df['open'], df['high'],\n",
    "                                                     df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3WHITESOLDIERS',\n",
    "                              talib.CDL3WHITESOLDIERS(df['open'], df['high'],\n",
    "                                                      df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLABANDONEDBABY',\n",
    "                              talib.CDLABANDONEDBABY(df['open'],\n",
    "                                                     df['high'],\n",
    "                                                     df['low'],\n",
    "                                                     df['close'],\n",
    "                                                     penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLADVANCEBLOCK',\n",
    "                              talib.CDLADVANCEBLOCK(df['open'], df['high'],\n",
    "                                                    df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLBELTHOLD',\n",
    "                              talib.CDLBELTHOLD(df['open'], df['high'],\n",
    "                                                df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLBREAKAWAY',\n",
    "                              talib.CDLBREAKAWAY(df['open'], df['high'],\n",
    "                                                 df['low'], df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('CDLCLOSINGMARUBOZU',\n",
    "         talib.CDLCLOSINGMARUBOZU(df['open'], df['high'], df['low'],\n",
    "                                  df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('CDLCONCEALBABYSWALL',\n",
    "         talib.CDLCONCEALBABYSWALL(df['open'], df['high'], df['low'],\n",
    "                                   df['close'])))\n",
    "    indicator_columns.append(('CDLCOUNTERATTACK',\n",
    "                              talib.CDLCOUNTERATTACK(df['open'], df['high'],\n",
    "                                                     df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDARKCLOUDCOVER',\n",
    "                              talib.CDLDARKCLOUDCOVER(df['open'],\n",
    "                                                      df['high'],\n",
    "                                                      df['low'],\n",
    "                                                      df['close'],\n",
    "                                                      penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLDOJI',\n",
    "                              talib.CDLDOJI(df['open'], df['high'], df['low'],\n",
    "                                            df['close'])))\n",
    "    indicator_columns.append(('CDLDOJISTAR',\n",
    "                              talib.CDLDOJISTAR(df['open'], df['high'],\n",
    "                                                df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDRAGONFLYDOJI',\n",
    "                              talib.CDLDRAGONFLYDOJI(df['open'], df['high'],\n",
    "                                                     df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLENGULFING',\n",
    "                              talib.CDLENGULFING(df['open'], df['high'],\n",
    "                                                 df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLEVENINGDOJISTAR',\n",
    "                              talib.CDLEVENINGDOJISTAR(df['open'],\n",
    "                                                       df['high'],\n",
    "                                                       df['low'],\n",
    "                                                       df['close'],\n",
    "                                                       penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLEVENINGSTAR',\n",
    "                              talib.CDLEVENINGSTAR(df['open'],\n",
    "                                                   df['high'],\n",
    "                                                   df['low'],\n",
    "                                                   df['close'],\n",
    "                                                   penetration=0)))\n",
    "    indicator_columns.append(\n",
    "        ('CDLGAPSIDESIDEWHITE',\n",
    "         talib.CDLGAPSIDESIDEWHITE(df['open'], df['high'], df['low'],\n",
    "                                   df['close'])))\n",
    "    indicator_columns.append(('CDLGRAVESTONEDOJI',\n",
    "                              talib.CDLGRAVESTONEDOJI(df['open'], df['high'],\n",
    "                                                      df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHAMMER',\n",
    "                              talib.CDLHAMMER(df['open'], df['high'],\n",
    "                                              df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHANGINGMAN',\n",
    "                              talib.CDLHANGINGMAN(df['open'], df['high'],\n",
    "                                                  df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHARAMI',\n",
    "                              talib.CDLHARAMI(df['open'], df['high'],\n",
    "                                              df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHARAMICROSS',\n",
    "                              talib.CDLHARAMICROSS(df['open'], df['high'],\n",
    "                                                   df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIGHWAVE',\n",
    "                              talib.CDLHIGHWAVE(df['open'], df['high'],\n",
    "                                                df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIKKAKE',\n",
    "                              talib.CDLHIKKAKE(df['open'], df['high'],\n",
    "                                               df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIKKAKEMOD',\n",
    "                              talib.CDLHIKKAKEMOD(df['open'], df['high'],\n",
    "                                                  df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHOMINGPIGEON',\n",
    "                              talib.CDLHOMINGPIGEON(df['open'], df['high'],\n",
    "                                                    df['low'], df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('CDLIDENTICAL3CROWS',\n",
    "         talib.CDLIDENTICAL3CROWS(df['open'], df['high'], df['low'],\n",
    "                                  df['close'])))\n",
    "    indicator_columns.append(('CDLINNECK',\n",
    "                              talib.CDLINNECK(df['open'], df['high'],\n",
    "                                              df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLINVERTEDHAMMER',\n",
    "                              talib.CDLINVERTEDHAMMER(df['open'], df['high'],\n",
    "                                                      df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLKICKING',\n",
    "                              talib.CDLKICKING(df['open'], df['high'],\n",
    "                                               df['low'], df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('CDLKICKINGBYLENGTH',\n",
    "         talib.CDLKICKINGBYLENGTH(df['open'], df['high'], df['low'],\n",
    "                                  df['close'])))\n",
    "    indicator_columns.append(('CDLLADDERBOTTOM',\n",
    "                              talib.CDLLADDERBOTTOM(df['open'], df['high'],\n",
    "                                                    df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLONGLEGGEDDOJI',\n",
    "                              talib.CDLLONGLEGGEDDOJI(df['open'], df['high'],\n",
    "                                                      df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLONGLINE',\n",
    "                              talib.CDLLONGLINE(df['open'], df['high'],\n",
    "                                                df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMARUBOZU',\n",
    "                              talib.CDLMARUBOZU(df['open'], df['high'],\n",
    "                                                df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMATCHINGLOW',\n",
    "                              talib.CDLMATCHINGLOW(df['open'], df['high'],\n",
    "                                                   df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMATHOLD',\n",
    "                              talib.CDLMATHOLD(df['open'],\n",
    "                                               df['high'],\n",
    "                                               df['low'],\n",
    "                                               df['close'],\n",
    "                                               penetration=0)))\n",
    "    indicator_columns.append(\n",
    "        ('CDLMORNINGDOJISTAR',\n",
    "         talib.CDLMORNINGDOJISTAR(df['open'], df['high'], df['low'],\n",
    "                                  df['close'])))\n",
    "    indicator_columns.append(('CDLMORNINGSTAR',\n",
    "                              talib.CDLMORNINGSTAR(df['open'], df['high'],\n",
    "                                                   df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLONNECK',\n",
    "                              talib.CDLONNECK(df['open'], df['high'],\n",
    "                                              df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLPIERCING',\n",
    "                              talib.CDLPIERCING(df['open'], df['high'],\n",
    "                                                df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLRICKSHAWMAN',\n",
    "                              talib.CDLRICKSHAWMAN(df['open'], df['high'],\n",
    "                                                   df['low'], df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('CDLRISEFALL3METHODS',\n",
    "         talib.CDLRISEFALL3METHODS(df['open'], df['high'], df['low'],\n",
    "                                   df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('CDLSEPARATINGLINES',\n",
    "         talib.CDLSEPARATINGLINES(df['open'], df['high'], df['low'],\n",
    "                                  df['close'])))\n",
    "    indicator_columns.append(('CDLSHOOTINGSTAR',\n",
    "                              talib.CDLSHOOTINGSTAR(df['open'], df['high'],\n",
    "                                                    df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSHORTLINE',\n",
    "                              talib.CDLSHORTLINE(df['open'], df['high'],\n",
    "                                                 df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSPINNINGTOP',\n",
    "                              talib.CDLSPINNINGTOP(df['open'], df['high'],\n",
    "                                                   df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSTALLEDPATTERN',\n",
    "                              talib.CDLSTALLEDPATTERN(df['open'], df['high'],\n",
    "                                                      df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSTICKSANDWICH',\n",
    "                              talib.CDLSTICKSANDWICH(df['open'], df['high'],\n",
    "                                                     df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTAKURI',\n",
    "                              talib.CDLTAKURI(df['open'], df['high'],\n",
    "                                              df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTASUKIGAP',\n",
    "                              talib.CDLTASUKIGAP(df['open'], df['high'],\n",
    "                                                 df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTHRUSTING',\n",
    "                              talib.CDLTHRUSTING(df['open'], df['high'],\n",
    "                                                 df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTRISTAR',\n",
    "                              talib.CDLTRISTAR(df['open'], df['high'],\n",
    "                                               df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLUNIQUE3RIVER',\n",
    "                              talib.CDLUNIQUE3RIVER(df['open'], df['high'],\n",
    "                                                    df['low'], df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('CDLUPSIDEGAP2CROWS',\n",
    "         talib.CDLUPSIDEGAP2CROWS(df['open'], df['high'], df['low'],\n",
    "                                  df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('CDLXSIDEGAP3METHODS',\n",
    "         talib.CDLXSIDEGAP3METHODS(df['open'], df['high'], df['low'],\n",
    "                                   df['close'])))\n",
    "    #########Statistic Functions\n",
    "    indicator_columns.append(('LINEARREG', talib.LINEARREG(df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('LINEARREG_ANGLE', talib.LINEARREG_ANGLE(df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('LINEARREG_INTERCEPT', talib.LINEARREG_INTERCEPT(df['close'])))\n",
    "    indicator_columns.append(\n",
    "        ('LINEARREG_SLOPE', talib.LINEARREG_SLOPE(df['close'])))\n",
    "    # new_columns['STDDEV'] = df['close'].rolling(timeperiod).std()\n",
    "    indicator_columns.append(('TSF', talib.TSF(df['close'])))\n",
    "    indicator_columns.append(('VAR', talib.VAR(df['close'])))\n",
    "    # Iterate over the time periods\n",
    "    for timeperiod in timeperiods:\n",
    "        #########Overlap Studies\n",
    "        indicator_columns.append((f'BB_upper_{timeperiod}',\n",
    "                                  talib.BBANDS(df['close'],\n",
    "                                               timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'BB_middle_{timeperiod}',\n",
    "                                  talib.BBANDS(df['close'],\n",
    "                                               timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'BB_lower_{timeperiod}',\n",
    "                                  talib.BBANDS(df['close'],\n",
    "                                               timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'DEMA_{timeperiod}',\n",
    "                                  talib.DEMA(df['close'],\n",
    "                                             timeperiod=timeperiod)))\n",
    "        indicator_columns.append(\n",
    "            (f'EMA_{timeperiod}', talib.EMA(df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'KAMA_{timeperiod}',\n",
    "                                  talib.KAMA(df['close'],\n",
    "                                             timeperiod=timeperiod)))\n",
    "        indicator_columns.append(\n",
    "            (f'MA_{timeperiod}', talib.MA(df['close'], timeperiod=timeperiod)))\n",
    "        # new_columns['MAMA'], new_columns['FAMA'] = talib.MAMA(df['close'], fastlimit=0, slowlimit=0)\n",
    "        # new_columns['MAVP'] = talib.MAVP(df['close'], periods=None, minperiod=2, maxperiod=30, matype=0)\n",
    "        indicator_columns.append((f'MIDPOINT_{timeperiod}',\n",
    "                                  talib.MIDPOINT(df['close'],\n",
    "                                                 timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MIDPRICE_{timeperiod}',\n",
    "                                  talib.MIDPRICE(df['high'],\n",
    "                                                 df['low'],\n",
    "                                                 timeperiod=timeperiod)))\n",
    "        indicator_columns.append(\n",
    "            (f'SMA_{timeperiod}', talib.SMA(df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'TEMA_{timeperiod}',\n",
    "                                  talib.TEMA(df['close'],\n",
    "                                             timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'TRIMA_{timeperiod}',\n",
    "                                  talib.TRIMA(df['close'],\n",
    "                                              timeperiod=timeperiod)))\n",
    "        indicator_columns.append(\n",
    "            (f'WMA_{timeperiod}', talib.WMA(df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ADX_{timeperiod}',\n",
    "                                  talib.ADX(df['high'],\n",
    "                                            df['low'],\n",
    "                                            df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ADXR_{timeperiod}',\n",
    "                                  talib.ADXR(df['high'],\n",
    "                                             df['low'],\n",
    "                                             df['close'],\n",
    "                                             timeperiod=timeperiod)))\n",
    "        aroon_up, aroon_down = talib.AROON(df['high'],\n",
    "                                           df['low'],\n",
    "                                           timeperiod=timeperiod)\n",
    "        indicator_columns.append((f'AROON_up_{timeperiod}', aroon_up))\n",
    "        indicator_columns.append((f'AROON_down_{timeperiod}', aroon_down))\n",
    "        indicator_columns.append((f'AROONOSC_{timeperiod}',\n",
    "                                  talib.AROONOSC(df['high'],\n",
    "                                                 df['low'],\n",
    "                                                 timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CCI_{timeperiod}',\n",
    "                                  talib.CCI(df['high'],\n",
    "                                            df['low'],\n",
    "                                            df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append(\n",
    "            (f'CMO_{timeperiod}', talib.CMO(df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'DX_{timeperiod}',\n",
    "                                  talib.DX(df['high'],\n",
    "                                           df['low'],\n",
    "                                           df['close'],\n",
    "                                           timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MFI_{timeperiod}',\n",
    "                                  talib.MFI(df['high'],\n",
    "                                            df['low'],\n",
    "                                            df['close'],\n",
    "                                            df['volume'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MINUS_DI_{timeperiod}',\n",
    "                                  talib.MINUS_DI(df['high'],\n",
    "                                                 df['low'],\n",
    "                                                 df['close'],\n",
    "                                                 timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MINUS_DM_{timeperiod}',\n",
    "                                  talib.MINUS_DM(df['high'],\n",
    "                                                 df['low'],\n",
    "                                                 timeperiod=timeperiod)))\n",
    "        indicator_columns.append(\n",
    "            (f'MOM_{timeperiod}', talib.MOM(df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'PLUS_DI_{timeperiod}',\n",
    "                                  talib.PLUS_DI(df['high'],\n",
    "                                                df['low'],\n",
    "                                                df['close'],\n",
    "                                                timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'PLUS_DM_{timeperiod}',\n",
    "                                  talib.PLUS_DM(df['high'],\n",
    "                                                df['low'],\n",
    "                                                timeperiod=timeperiod)))\n",
    "        indicator_columns.append(\n",
    "            (f'ROC_{timeperiod}', talib.ROC(df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCP_{timeperiod}',\n",
    "                                  talib.ROCP(df['close'],\n",
    "                                             timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCR_{timeperiod}',\n",
    "                                  talib.ROCR(df['close'],\n",
    "                                             timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCR100_{timeperiod}',\n",
    "                                  talib.ROCR100(df['close'],\n",
    "                                                timeperiod=timeperiod)))\n",
    "        indicator_columns.append(\n",
    "            (f'RSI_{timeperiod}', talib.RSI(df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "\n",
    "        indicator_columns.append((f'ATR_{timeperiod}',\n",
    "                                  talib.ATR(df['high'],\n",
    "                                            df['low'],\n",
    "                                            df['close'],\n",
    "                                            timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'NATR_{timeperiod}',\n",
    "                                  talib.NATR(df['high'],\n",
    "                                             df['low'],\n",
    "                                             df['close'],\n",
    "                                             timeperiod=timeperiod)))\n",
    "        #########Statistic Functions\n",
    "        indicator_columns.append((f'BETA_{timeperiod}',\n",
    "                                  talib.BETA(df['high'],\n",
    "                                             df['low'],\n",
    "                                             timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CORREL_{timeperiod}',\n",
    "                                  talib.CORREL(df['high'],\n",
    "                                               df['low'],\n",
    "                                               timeperiod=timeperiod)))\n",
    "    new_columns = pd.concat([\n",
    "        pd.DataFrame(data, columns=[name]) for name, data in indicator_columns\n",
    "    ],\n",
    "                            axis=1)\n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indicators_using_talib_new(timeperiods, df):\n",
    "    def apply_indicator(func, *args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    def apply_indicator_with_timeperiod(func, *args, **kwargs):\n",
    "        return [\n",
    "            (f\"{func.__name__}_{timeperiod}\", apply_indicator(func, *args, timeperiod=timeperiod, **kwargs))\n",
    "            for timeperiod in timeperiods\n",
    "        ]\n",
    "\n",
    "    indicator_columns = [\n",
    "        ('HT_TRENDLINE', talib.HT_TRENDLINE(df['close'])),\n",
    "        ('SAR', talib.SAR(df['high'], df['low'], acceleration=0, maximum=0)),\n",
    "        ('SAREXT', talib.SAREXT(df['high'], df['low'])),\n",
    "        ('T3', talib.T3(df['close'], timeperiod=5, vfactor=0)),\n",
    "        ('APO', talib.APO(df['close'], fastperiod=12, slowperiod=26)),\n",
    "        ('BOP', talib.BOP(df['open'], df['high'], df['low'], df['close'])),\n",
    "        *zip(['MACD', 'MACD_signal', 'MACD_hist'], talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)),\n",
    "        ('PPO', talib.PPO(df['close'], fastperiod=12, slowperiod=26, matype=0)),\n",
    "        ('TRIX', talib.TRIX(df['close'])),\n",
    "        ('ULTOSC', talib.ULTOSC(df['high'], df['low'], df['close'])),\n",
    "        ('WILLR', talib.WILLR(df['high'], df['low'], df['close'])),\n",
    "        ('AD', talib.AD(df['high'], df['low'], df['close'], df['volume'])),\n",
    "        ('ADOSC', talib.ADOSC(df['high'], df['low'], df['close'], df['volume'], fastperiod=3, slowperiod=10)),\n",
    "        ('OBV', talib.OBV(df['close'], df['volume'])),\n",
    "        ('HT_DCPERIOD', talib.HT_DCPERIOD(df['close'])),\n",
    "        ('HT_DCPHASE', talib.HT_DCPHASE(df['close'])),\n",
    "        *zip(['HT_PHASOR_inphase', 'HT_PHASOR_quadrature'], talib.HT_PHASOR(df['close'])),\n",
    "        ('HT_TRENDMODE', talib.HT_TRENDMODE(df['close'])),\n",
    "        ('AVGPRICE', talib.AVGPRICE(df['open'], df['high'], df['low'], df['close'])),\n",
    "        ('MEDPRICE', talib.MEDPRICE(df['high'], df['low'])),\n",
    "        ('TYPPRICE', talib.TYPPRICE(df['high'], df['low'], df['close'])),\n",
    "        ('WCLPRICE', talib.WCLPRICE(df['high'], df['low'], df['close'])),\n",
    "        ('TRANGE', talib.TRANGE(df['high'], df['low'], df['close'])),\n",
    "    ]\n",
    "\n",
    "    # Add pattern recognition indicators\n",
    "    pattern_funcs = [getattr(talib, f) for f in dir(talib) if f.startswith('CDL')]\n",
    "    indicator_columns.extend([\n",
    "        (f.__name__, apply_indicator(f, df['open'], df['high'], df['low'], df['close']))\n",
    "        for f in pattern_funcs\n",
    "    ])\n",
    "\n",
    "    # Add statistic functions\n",
    "    stat_funcs = [talib.LINEARREG, talib.LINEARREG_ANGLE, talib.LINEARREG_INTERCEPT, talib.LINEARREG_SLOPE, talib.TSF, talib.VAR]\n",
    "    indicator_columns.extend([\n",
    "        (f.__name__, apply_indicator(f, df['close']))\n",
    "        for f in stat_funcs\n",
    "    ])\n",
    "\n",
    "    # Add indicators with timeperiods\n",
    "    timeperiod_funcs = [\n",
    "        (talib.BBANDS, ['high', 'low', 'close']),\n",
    "        (talib.DEMA, ['close']),\n",
    "        (talib.EMA, ['close']),\n",
    "        (talib.KAMA, ['close']),\n",
    "        (talib.MA, ['close']),\n",
    "        (talib.MIDPOINT, ['close']),\n",
    "        (talib.MIDPRICE, ['high', 'low']),\n",
    "        (talib.SMA, ['close']),\n",
    "        (talib.TEMA, ['close']),\n",
    "        (talib.TRIMA, ['close']),\n",
    "        (talib.WMA, ['close']),\n",
    "        (talib.ADX, ['high', 'low', 'close']),\n",
    "        (talib.ADXR, ['high', 'low', 'close']),\n",
    "        (talib.AROON, ['high', 'low']),\n",
    "        (talib.AROONOSC, ['high', 'low']),\n",
    "        (talib.CCI, ['high', 'low', 'close']),\n",
    "        (talib.CMO, ['close']),\n",
    "        (talib.DX, ['high', 'low', 'close']),\n",
    "        (talib.MFI, ['high', 'low', 'close', 'volume']),\n",
    "        (talib.MINUS_DI, ['high', 'low', 'close']),\n",
    "        (talib.MINUS_DM, ['high', 'low']),\n",
    "        (talib.MOM, ['close']),\n",
    "        (talib.PLUS_DI, ['high', 'low', 'close']),\n",
    "        (talib.PLUS_DM, ['high', 'low']),\n",
    "        (talib.ROC, ['close']),\n",
    "        (talib.ROCP, ['close']),\n",
    "        (talib.ROCR, ['close']),\n",
    "        (talib.ROCR100, ['close']),\n",
    "        (talib.RSI, ['close']),\n",
    "        (talib.ATR, ['high', 'low', 'close']),\n",
    "        (talib.NATR, ['high', 'low', 'close']),\n",
    "        (talib.BETA, ['high', 'low']),\n",
    "        (talib.CORREL, ['high', 'low']),\n",
    "    ]\n",
    "\n",
    "    for func, columns in timeperiod_funcs:\n",
    "        indicator_columns.extend(apply_indicator_with_timeperiod(func, *[df[col] for col in columns]))\n",
    "\n",
    "    return pd.concat([pd.DataFrame(data, columns=[name]) for name, data in indicator_columns], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb764fde",
   "metadata": {},
   "source": [
    "# new download and concatenate data but with modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92950392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T14:54:35.759222Z",
     "start_time": "2024-01-15T14:54:35.746474Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_data_and_concatenate(master_dictionary, month_array, day_array):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"Setting up things for {symbol}, {chart_time}\")\n",
    "\n",
    "            # Set up an empty list for the data frames\n",
    "            df_list = []\n",
    "\n",
    "            # Compile the regular expression pattern\n",
    "            pattern = re.compile(rf\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Compile the regular expression pattern for daily zip files\n",
    "            pattern_daily = re.compile(\n",
    "                rf\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Create the new folder path for daily ZIP files\n",
    "            new_daily_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-daily_data\")\n",
    "\n",
    "            # Create the new folder path for ZIP files\n",
    "            new_monthly_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "            # Create the new folder path for CSV files\n",
    "            new_csv_folder_path = os.path.join(output_dir,\n",
    "                                               f\"{symbol}-{chart_time}\")\n",
    "\n",
    "            # Set the file name\n",
    "            concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "            # Construct the file path\n",
    "            concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                                  concatenated_file_name)\n",
    "\n",
    "            download_monthly_data(month_array, symbol, chart_time)  \n",
    "            download_daily_data(day_array, symbol, chart_time)  \n",
    "\n",
    "            # Process the monthly ZIP folder and add to df_list\n",
    "            df_list = process_zip_folder(\n",
    "                new_monthly_zip_folder_path, \n",
    "                pattern, \n",
    "                new_csv_folder_path, \n",
    "                symbol, \n",
    "                chart_time, \n",
    "                df_list,\n",
    "                day_array,\n",
    "            )\n",
    "\n",
    "            # Process the daily ZIP folder and add to df_list\n",
    "            df_list = process_zip_folder(\n",
    "                new_daily_zip_folder_path, \n",
    "                pattern_daily, \n",
    "                new_csv_folder_path, \n",
    "                symbol, \n",
    "                chart_time, \n",
    "                df_list, \n",
    "                day_array,  \n",
    "                is_daily=True\n",
    "            )\n",
    "\n",
    "            # Call the function to concatenate and process the data frames\n",
    "            print(concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time))\n",
    "    return \"Data downloaded and concatenated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0323a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wins_losses_old(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            try:\n",
    "                print(f\"Calculating for {symbol} {chart_time}\")\n",
    "\n",
    "                # Define the directory for processed data\n",
    "                processed_data_dir = Path(output_dir) / f\"{symbol}-{chart_time}/processed_data\"\n",
    "\n",
    "                # Create the directory if it doesn't exist\n",
    "                if not processed_data_dir.exists():\n",
    "                    processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Construct the file name\n",
    "                og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "                og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "                new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "                new_file_path = processed_data_dir / new_file_name\n",
    "\n",
    "                # Read the CSV file into a dataframe\n",
    "                df = pd.read_csv(og_file_path)\n",
    "\n",
    "                # Initialize new columns\n",
    "                df[\"if_short\"] = 0\n",
    "                df[\"if_long\"] = 0\n",
    "                df[\"long_target\"] = np.nan\n",
    "                df[\"short_target\"] = np.nan\n",
    "                df[\"long_stop_loss\"] = np.nan\n",
    "                df[\"short_stop_loss\"] = np.nan\n",
    "                df[\"shorts_win_after\"] = np.nan\n",
    "                df[\"longs_win_after\"] = np.nan\n",
    "#                 df[\"dual_loss\"] = 0\n",
    "#                 df[\"entered_before\"] = np.nan\n",
    "\n",
    "\n",
    "                # Calculate targets and stop losses, then determine wins and losses\n",
    "                for i in tqdm(range(len(df)), desc=f\"Processing {symbol}-{chart_time}\", unit='row'):\n",
    "                    if pd.notna(df.loc[i, 'entry']):\n",
    "                        long_target = df.loc[i, 'entry'] * (1 + win_perc / 100)\n",
    "                        long_stop_loss = df.loc[i, 'entry'] * (1 - loss_perc / 100)\n",
    "                        short_target = df.loc[i, 'entry'] * (1 - win_perc / 100)\n",
    "                        short_stop_loss = df.loc[i, 'entry'] * (1 + loss_perc / 100)\n",
    "\n",
    "                        # Initialize columns for current row\n",
    "                        df.loc[i, 'if_long'] = np.nan\n",
    "                        df.loc[i, 'longs_win_after'] = np.nan\n",
    "                        df.loc[i, 'if_short'] = np.nan\n",
    "                        df.loc[i, 'shorts_win_after'] = np.nan\n",
    "                        df.loc[i, 'dual_loss'] = 0\n",
    "                        df.loc[i, 'entered_before'] = np.nan\n",
    "                        df.loc[i, 'long_target'] = long_target\n",
    "                        df.loc[i, 'long_stop_loss'] = long_stop_loss\n",
    "                        df.loc[i, 'short_stop_loss'] = short_stop_loss\n",
    "                        \n",
    "\n",
    "                        # Evaluate long trades\n",
    "                        for j in range(i, len(df)):\n",
    "                            if df.loc[j, 'high'] >= long_target:\n",
    "                                if df.loc[j, 'low'] <= long_stop_loss:\n",
    "                                    df.loc[i, 'if_long'] = -1\n",
    "#                                     df.loc[i, 'dual_loss'] = 1\n",
    "#                                     df.loc[i, 'entered_before'] = j - i\n",
    "                                else:\n",
    "                                    df.loc[i, 'if_long'] = 1\n",
    "                                    df.loc[i, 'longs_win_after'] = j - i\n",
    "                                break\n",
    "                            elif df.loc[j, 'low'] <= long_stop_loss:\n",
    "                                df.loc[i, 'if_long'] = -1\n",
    "                                break\n",
    "                        df.loc[i, 'short_target'] = short_target\n",
    "                        # Evaluate short trades\n",
    "                        for j in range(i, len(df)):\n",
    "                            if df.loc[j, 'low'] <= short_target:\n",
    "                                if df.loc[j, 'high'] >= short_stop_loss:\n",
    "                                    df.loc[i, 'if_short'] = -1\n",
    "#                                     df.loc[i, 'dual_loss'] = 1\n",
    "#                                     df.loc[i, 'entered_before'] = j - i\n",
    "                                else:\n",
    "                                    df.loc[i, 'if_short'] = 1\n",
    "                                    df.loc[i, 'shorts_win_after'] = j - i\n",
    "                                break\n",
    "                            elif df.loc[j, 'high'] >= short_stop_loss:\n",
    "                                df.loc[i, 'if_short'] = -1\n",
    "                                break\n",
    "\n",
    "                # Save the processed data\n",
    "                df.to_csv(new_file_path, index=False)\n",
    "                print(f\"Processed file saved as {new_file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {symbol} {chart_time}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69efb295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wins_losses(master_dictionary, win_perc=0.73, loss_perc=0.4, lookahead_window=10000):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            try:\n",
    "                print(f\"Calculating for {symbol} {chart_time}\")\n",
    "\n",
    "                # Define the directory for processed data\n",
    "                processed_data_dir = Path(output_dir) / f\"{symbol}-{chart_time}/processed_data\"\n",
    "                processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Construct the file names\n",
    "                og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "                og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "                new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}_{lookahead_window}cdls.csv\"\n",
    "                new_file_path = processed_data_dir / new_file_name\n",
    "\n",
    "                # Read the CSV file into a dataframe\n",
    "                df = pd.read_csv(og_file_path, usecols=['entry', 'high', 'low', 'open', 'close'])\n",
    "\n",
    "                # Initialize new columns\n",
    "                df[\"if_short\"] = np.nan\n",
    "                df[\"if_long\"] = np.nan\n",
    "                df[\"long_target\"] = df[\"entry\"] * (1 + win_perc / 100)\n",
    "                df[\"short_target\"] = df[\"entry\"] * (1 - win_perc / 100)\n",
    "                df[\"long_stop_loss\"] = df[\"entry\"] * (1 - loss_perc / 100)\n",
    "                df[\"short_stop_loss\"] = df[\"entry\"] * (1 + loss_perc / 100)\n",
    "                df[\"shorts_win_after\"] = np.nan\n",
    "                df[\"longs_win_after\"] = np.nan\n",
    "\n",
    "                # Convert DataFrame columns to numpy arrays for faster processing\n",
    "                highs = df['high'].values\n",
    "                lows = df['low'].values\n",
    "                long_targets = df['long_target'].values\n",
    "                short_targets = df['short_target'].values\n",
    "                long_stop_losses = df['long_stop_loss'].values\n",
    "                short_stop_losses = df['short_stop_loss'].values\n",
    "\n",
    "                # Process each entry row by row\n",
    "                for i in tqdm(range(len(df)), desc=\"Processing Rows\", unit='row'):\n",
    "                    # Define the lookahead window range\n",
    "                    lookahead_end = min(i + lookahead_window, len(df))\n",
    "                    \n",
    "                    # Slice the future highs and lows from current index onwards, respecting the lookahead window\n",
    "                    future_highs = highs[i:lookahead_end]\n",
    "                    future_lows = lows[i:lookahead_end]\n",
    "\n",
    "                    # Early stopping for performance improvement\n",
    "                    long_hit_idx = np.argmax(future_highs >= long_targets[i]) if np.any(future_highs >= long_targets[i]) else np.nan\n",
    "                    long_stop_idx = np.argmax(future_lows <= long_stop_losses[i]) if np.any(future_lows <= long_stop_losses[i]) else np.nan\n",
    "\n",
    "                    short_hit_idx = np.argmax(future_lows <= short_targets[i]) if np.any(future_lows <= short_targets[i]) else np.nan\n",
    "                    short_stop_idx = np.argmax(future_highs >= short_stop_losses[i]) if np.any(future_highs >= short_stop_losses[i]) else np.nan\n",
    "\n",
    "                    # Long trade logic\n",
    "                    if not np.isnan(long_hit_idx) and (np.isnan(long_stop_idx) or long_hit_idx < long_stop_idx):\n",
    "                        df.at[i, 'if_long'] = 1\n",
    "                        df.at[i, 'longs_win_after'] = long_hit_idx\n",
    "                    elif not np.isnan(long_stop_idx):\n",
    "                        df.at[i, 'if_long'] = -1\n",
    "\n",
    "                    # Short trade logic\n",
    "                    if not np.isnan(short_hit_idx) and (np.isnan(short_stop_idx) or short_hit_idx < short_stop_idx):\n",
    "                        df.at[i, 'if_short'] = 1\n",
    "                        df.at[i, 'shorts_win_after'] = short_hit_idx\n",
    "                    elif not np.isnan(short_stop_idx):\n",
    "                        df.at[i, 'if_short'] = -1\n",
    "\n",
    "                # Save the processed data\n",
    "                df.to_csv(new_file_path, index=False)\n",
    "                print(f\"Processed file saved as {new_file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {symbol} {chart_time}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c13d0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T12:21:19.054928Z",
     "start_time": "2024-09-15T12:21:19.041012Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wins_losses_optimized(master_dictionary, win_perc=0.73, loss_perc=0.4, lookahead_window=10000):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            try:\n",
    "                print(f\"Calculating for {symbol} {chart_time}\")\n",
    "\n",
    "                # Define the directory for processed data\n",
    "                processed_data_dir = Path(output_dir) / f\"{symbol}-{chart_time}/processed_data\"\n",
    "                processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Construct the file names\n",
    "                og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "                og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "                new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}_{lookahead_window}cdls.csv\"\n",
    "                new_file_path = processed_data_dir / new_file_name\n",
    "\n",
    "                # Read the CSV file into a dataframe\n",
    "                df = pd.read_csv(og_file_path, usecols=['entry', 'high', 'low', 'open', 'close'])\n",
    "\n",
    "                # Initialize new columns\n",
    "                df[\"if_short\"] = np.nan\n",
    "                df[\"if_long\"] = np.nan\n",
    "                df[\"long_target\"] = df[\"entry\"] * (1 + win_perc / 100)\n",
    "                df[\"short_target\"] = df[\"entry\"] * (1 - win_perc / 100)\n",
    "                df[\"long_stop_loss\"] = df[\"entry\"] * (1 - loss_perc / 100)\n",
    "                df[\"short_stop_loss\"] = df[\"entry\"] * (1 + loss_perc / 100)\n",
    "                df[\"shorts_win_after\"] = np.nan\n",
    "                df[\"longs_win_after\"] = np.nan\n",
    "\n",
    "                # Convert DataFrame columns to numpy arrays for faster processing\n",
    "                highs = df['high'].values\n",
    "                lows = df['low'].values\n",
    "                long_targets = df['long_target'].values\n",
    "                short_targets = df['short_target'].values\n",
    "                long_stop_losses = df['long_stop_loss'].values\n",
    "                short_stop_losses = df['short_stop_loss'].values\n",
    "\n",
    "                # Prepare result arrays\n",
    "                if_long_results = np.full(len(df), np.nan)\n",
    "                if_short_results = np.full(len(df), np.nan)\n",
    "                longs_win_after = np.full(len(df), np.nan)\n",
    "                shorts_win_after = np.full(len(df), np.nan)\n",
    "\n",
    "                # Process each entry row by row\n",
    "                for i in tqdm(range(len(df)), desc=\"Processing Rows\", unit='row'):\n",
    "                    # Define the lookahead window range\n",
    "                    lookahead_end = min(i + lookahead_window, len(df))\n",
    "                    \n",
    "                    # Slice the future highs and lows from current index onwards, respecting the lookahead window\n",
    "                    future_highs = highs[i:lookahead_end]\n",
    "                    future_lows = lows[i:lookahead_end]\n",
    "\n",
    "                    # Early stopping for performance improvement\n",
    "                    long_hit_idx = np.argmax(future_highs >= long_targets[i]) if np.any(future_highs >= long_targets[i]) else np.nan\n",
    "                    long_stop_idx = np.argmax(future_lows <= long_stop_losses[i]) if np.any(future_lows <= long_stop_losses[i]) else np.nan\n",
    "\n",
    "                    short_hit_idx = np.argmax(future_lows <= short_targets[i]) if np.any(future_lows <= short_targets[i]) else np.nan\n",
    "                    short_stop_idx = np.argmax(future_highs >= short_stop_losses[i]) if np.any(future_highs >= short_stop_losses[i]) else np.nan\n",
    "\n",
    "                    # Long trade logic\n",
    "                    if not np.isnan(long_hit_idx) and (np.isnan(long_stop_idx) or long_hit_idx < long_stop_idx):\n",
    "                        if_long_results[i] = 1\n",
    "                        longs_win_after[i] = long_hit_idx\n",
    "                    elif not np.isnan(long_stop_idx):\n",
    "                        if_long_results[i] = -1\n",
    "\n",
    "                    # Short trade logic\n",
    "                    if not np.isnan(short_hit_idx) and (np.isnan(short_stop_idx) or short_hit_idx < short_stop_idx):\n",
    "                        if_short_results[i] = 1\n",
    "                        shorts_win_after[i] = short_hit_idx\n",
    "                    elif not np.isnan(short_stop_idx):\n",
    "                        if_short_results[i] = -1\n",
    "\n",
    "                # Update DataFrame with results\n",
    "                df['if_short'] = if_short_results\n",
    "                df['if_long'] = if_long_results\n",
    "                df['shorts_win_after'] = shorts_win_after\n",
    "                df['longs_win_after'] = longs_win_after\n",
    "\n",
    "                # Save the processed data\n",
    "                df.to_csv(new_file_path, index=False)\n",
    "                print(f\"Processed file saved as {new_file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {symbol} {chart_time}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b2bb71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T19:59:08.910872Z",
     "start_time": "2024-09-07T19:59:08.892151Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_indicator_values(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            # Define the directory for processed data\n",
    "            data_dir = Path(output_dir) / f\"{symbol}-{chart_time}\"\n",
    "            \n",
    "            # Construct the file name and path\n",
    "            file_name = f\"{symbol}-{chart_time}-yo.csv\"\n",
    "            file_path = data_dir / file_name\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                print(f\"File path for {file_name} doesn't exist. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(df.dtypes)\n",
    "            \n",
    "            # Calculate indicators using TA-Lib\n",
    "            new_columns = calculate_indicators_using_talib_new(master_dictionary[\"timeperiods\"], df)\n",
    "            \n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df = pd.concat([df, new_columns], axis=1)\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "    return \"Indicators are added to the CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data creation utilities successfully initialized\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
