{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a6887b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:43:26.146950Z",
     "start_time": "2023-10-02T10:43:22.014955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.8.5)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.28.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Installing collected packages: webdriver-manager\n",
      "  Attempting uninstall: webdriver-manager\n",
      "    Found existing installation: webdriver-manager 3.8.5\n",
      "    Uninstalling webdriver-manager-3.8.5:\n",
      "      Successfully uninstalled webdriver-manager-3.8.5\n",
      "Successfully installed webdriver-manager-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bcec77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T09:27:12.812826Z",
     "start_time": "2024-01-15T09:27:07.946190Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Start a new instance of the Chrome driver\n",
    "webdriver_service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=webdriver_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "108c2a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T14:31:06.817707Z",
     "start_time": "2024-01-15T14:30:52.149685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\downloaded_data\n",
      "D:\\KISHORE\\Binance-Data-Downloader\\data\\extracted_data\n",
      "['1000BONKUSDT', '1000BTTCUSDT', '1000FLOKIUSDT', '1000LUNCBUSD', '1000LUNCUSDT', '1000PEPEUSDT', '1000RATSUSDT', '1000SATSUSDT', '1000SHIBBUSD', '1000SHIBUSDT', '1000XECUSDT', '1INCHUSDT', 'AAVEUSDT', 'ACEUSDT', 'ACHUSDT', 'ADABUSD', 'ADAUSDT', 'AGIXBUSD', 'AGIXUSDT', 'AGLDUSDT', 'AKROUSDT', 'ALGOUSDT', 'ALICEUSDT', 'ALPHAUSDT', 'AMBBUSD', 'AMBUSDT', 'ANCBUSD', 'ANCUSDT', 'ANKRUSDT', 'ANTUSDT', 'APEBUSD', 'APEUSDT', 'API3USDT', 'APTBUSD', 'APTUSDT', 'ARBUSDT', 'ARKMUSDT', 'ARKUSDT', 'ARPAUSDT', 'ARUSDT', 'ASTRUSDT', 'ATAUSDT', 'ATOMUSDT', 'AUCTIONBUSD', 'AUCTIONUSDT', 'AUDIOUSDT', 'AVAXBUSD', 'AVAXUSDT', 'AXSUSDT', 'BADGERUSDT', 'BAKEUSDT', 'BALUSDT', 'BANDUSDT', 'BATUSDT', 'BCHUSDT', 'BEAMXUSDT', 'BELUSDT', 'BICOUSDT', 'BIGTIMEUSDT', 'BLUEBIRDUSDT', 'BLURUSDT', 'BLZUSDT', 'BNBBUSD', 'BNBUSDT', 'BNTUSDT', 'BNXUSDT', 'BNXUSDTSETTLED', 'BONDUSDT', 'BSVUSDT', 'BTCBUSD', 'BTCBUSD_210129', 'BTCBUSD_210226', 'BTCDOMUSDT', 'BTCSTUSDT', 'BTCUSDT', 'BTCUSDT_210326', 'BTCUSDT_210625', 'BTCUSDT_210924', 'BTCUSDT_211231', 'BTCUSDT_220325', 'BTCUSDT_220624', 'BTCUSDT_220930', 'BTCUSDT_221230', 'BTCUSDT_230331', 'BTCUSDT_230630', 'BTCUSDT_230929', 'BTCUSDT_231229', 'BTCUSDT_240329', 'BTCUSDT_240628', 'BTSUSDT', 'BTTUSDT', 'BZRXUSDT', 'C98USDT', 'CAKEUSDT', 'CELOUSDT', 'CELRUSDT', 'CFXUSDT', 'CHRUSDT', 'CHZUSDT', 'CKBUSDT', 'COCOSUSDT', 'COMBOUSDT', 'COMPUSDT', 'COTIUSDT', 'CRVUSDT', 'CTKUSDT', 'CTSIUSDT', 'CVCUSDT', 'CVXBUSD', 'CVXUSDT', 'CYBERUSDT', 'DARUSDT', 'DASHUSDT', 'DEFIUSDT', 'DENTUSDT', 'DGBUSDT', 'DODOBUSD', 'DODOUSDT', 'DODOXUSDT', 'DOGEBUSD', 'DOGEUSDT', 'DOTBUSD', 'DOTECOUSDT', 'DOTUSDT', 'DUSKUSDT', 'DYDXUSDT', 'EDUUSDT', 'EGLDUSDT', 'ENJUSDT', 'ENSUSDT', 'EOSUSDT', 'ETCBUSD', 'ETCUSDT', 'ETHBTC', 'ETHBUSD', 'ETHUSDT', 'ETHUSDT_210326', 'ETHUSDT_210625', 'ETHUSDT_210924', 'ETHUSDT_211231', 'ETHUSDT_220325', 'ETHUSDT_220624', 'ETHUSDT_220930', 'ETHUSDT_221230', 'ETHUSDT_230331', 'ETHUSDT_230630', 'ETHUSDT_230929', 'ETHUSDT_231229', 'ETHUSDT_240329', 'ETHUSDT_240628', 'ETHWUSDT', 'FETUSDT', 'FILBUSD', 'FILUSDT', 'FLMUSDT', 'FLOWUSDT', 'FOOTBALLUSDT', 'FRONTUSDT', 'FTMBUSD', 'FTMUSDT', 'FTTBUSD', 'FTTUSDT', 'FXSUSDT', 'GALABUSD', 'GALAUSDT', 'GALBUSD', 'GALUSDT', 'GASUSDT', 'GLMRUSDT', 'GMTBUSD', 'GMTUSDT', 'GMXUSDT', 'GRTUSDT', 'GTCUSDT', 'HBARUSDT', 'HFTUSDT', 'HIFIUSDT', 'HIGHUSDT', 'HNTUSDT', 'HOOKUSDT', 'HOTUSDT', 'ICPBUSD', 'ICPUSDT', 'ICPUSDT_SETTLED', 'ICXUSDT', 'IDEXUSDT', 'IDUSDT', 'ILVUSDT', 'IMXUSDT', 'INJUSDT', 'IOSTUSDT', 'IOTAUSDT', 'IOTXUSDT', 'JASMYUSDT', 'JOEUSDT', 'JTOUSDT', 'KASUSDT', 'KAVAUSDT', 'KEEPUSDT', 'KEYUSDT', 'KLAYUSDT', 'KNCUSDT', 'KSMUSDT', 'LDOBUSD', 'LDOUSDT', 'LENDUSDT', 'LEVERBUSD', 'LEVERUSDT', 'LINAUSDT', 'LINKBUSD', 'LINKUSDT', 'LITUSDT', 'LOOMUSDT', 'LPTUSDT', 'LQTYUSDT', 'LRCUSDT', 'LTCBUSD', 'LTCUSDT', 'LUNA2BUSD', 'LUNA2USDT', 'LUNABUSD', 'LUNAUSDT', 'MAGICUSDT', 'MANAUSDT', 'MASKUSDT', 'MATICBUSD', 'MATICUSDT', 'MAVUSDT', 'MBLUSDT', 'MDTUSDT', 'MEMEUSDT', 'MINAUSDT', 'MINAUSDTSETTLED', 'MKRUSDT', 'MOVRUSDT', 'MTLUSDT', 'NEARBUSD', 'NEARUSDT', 'NEOUSDT', 'NFPUSDT', 'NKNUSDT', 'NMRUSDT', 'NTRNUSDT', 'NUUSDT', 'OCEANUSDT', 'OGNUSDT', 'OMGUSDT', 'ONEUSDT', 'ONGUSDT', 'ONTUSDT', 'OPUSDT', 'ORBSUSDT', 'ORDIUSDT', 'OXTUSDT', 'PENDLEUSDT', 'PEOPLEUSDT', 'PERPUSDT', 'PHBBUSD', 'PHBUSDT', 'POLYXUSDT', 'POWRUSDT', 'PYTHUSDT', 'QNTUSDT', 'QTUMUSDT', 'RADUSDT', 'RAYUSDT', 'RDNTUSDT', 'REEFUSDT', 'RENUSDT', 'RIFUSDT', 'RLCUSDT', 'RNDRUSDT', 'ROSEUSDT', 'RSRUSDT', 'RUNEUSDT', 'RVNUSDT', 'SANDBUSD', 'SANDUSDT', 'SCUSDT', 'SEIUSDT', 'SFPUSDT', 'SKLUSDT', 'SLPUSDT', 'SNTUSDT', 'SNXUSDT', 'SOLBUSD', 'SOLUSDT', 'SPELLUSDT', 'SRMUSDT', 'SSVUSDT', 'STEEMUSDT', 'STGUSDT', 'STMXUSDT', 'STORJUSDT', 'STPTUSDT', 'STRAXUSDT', 'STXUSDT', 'SUIUSDT', 'SUPERUSDT', 'SUSHIUSDT', 'SXPUSDT', 'THETAUSDT', 'TIAUSDT', 'TLMBUSD', 'TLMUSDT', 'TLMUSDTSETTLED', 'TOKENUSDT', 'TOMOUSDT', 'TRBUSDT', 'TRUUSDT', 'TRXBUSD', 'TRXUSDT', 'TUSDT', 'TWTUSDT', 'UMAUSDT', 'UNFIUSDT', 'UNIBUSD', 'UNIUSDT', 'USDCUSDT', 'USTCUSDT', 'VETUSDT', 'WAVESBUSD', 'WAVESUSDT', 'WAXPUSDT', 'WLDUSDT', 'WOOUSDT', 'XEMUSDT', 'XLMUSDT', 'XMRUSDT', 'XRPBUSD', 'XRPUSDT', 'XTZUSDT', 'XVGUSDT', 'XVSUSDT', 'YFIIUSDT', 'YFIUSDT', 'YGGUSDT', 'ZECUSDT', 'ZENUSDT', 'ZILUSDT', 'ZRXUSDT']\n",
      "['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02', '2024-03', '2024-04', '2024-05', '2024-07', '2024-08', '2024-09', '2024-10', '2024-11', '2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05', '2025-06', '2025-07', '2025-08', '2025-09', '2025-10', '2025-11', '2025-12', '2026-01', '2026-02', '2026-03', '2026-04', '2026-05', '2026-06', '2026-07', '2026-08', '2026-09', '2026-10', '2026-11', '2026-12', '2027-01', '2027-02', '2027-03', '2027-04', '2027-05', '2027-06', '2027-07', '2027-08', '2027-09', '2027-10', '2027-11', '2027-12', '2028-01', '2028-02', '2028-03', '2028-04', '2028-05', '2028-06', '2028-07', '2028-08', '2028-09', '2028-10', '2028-11', '2028-12', '2029-01', '2029-03', '2029-04', '2029-05', '2029-06', '2029-07', '2029-08', '2029-09', '2029-10', '2029-11', '2029-12', '2030-01', '2030-02', '2030-03', '2030-04', '2030-05', '2030-06', '2030-07', '2030-08', '2030-09', '2030-10', '2030-11', '2030-12', '2031-01', '2031-02', '2031-03', '2031-04', '2031-05', '2031-06', '2031-07', '2031-08', '2031-09', '2031-10', '2031-11', '2031-12', '2032-01', '2032-02', '2032-03', '2032-04', '2032-05', '2032-06', '2032-07', '2032-08', '2032-09', '2032-10', '2032-11', '2032-12', '2033-01', '2033-02', '2033-03', '2033-04', '2033-05', '2033-06', '2033-07', '2033-08', '2033-10', '2033-11', '2033-12', '2034-01', '2034-02', '2034-03', '2034-04', '2034-05', '2034-06', '2034-07', '2034-08', '2034-09', '2034-10', '2034-11', '2034-12', '2035-01', '2035-02', '2035-03', '2035-04', '2035-05', '2035-06', '2035-07', '2035-08', '2035-09', '2035-10', '2035-11', '2035-12', '2036-01', '2036-02', '2036-03', '2036-04', '2036-05', '2036-06', '2036-07', '2036-08', '2036-09', '2036-10', '2036-11', '2036-12', '2037-01', '2037-02', '2037-03', '2037-04', '2037-05', '2037-06', '2037-07', '2037-08', '2037-09', '2037-10', '2037-11', '2037-12', '2038-01', '2038-03', '2038-04', '2038-05', '2038-06', '2038-07', '2038-08', '2038-09', '2038-10', '2038-11', '2038-12', '2039-01', '2039-02', '2039-03', '2039-04', '2039-05', '2039-06', '2039-07', '2039-08', '2039-09', '2039-10', '2039-11', '2039-12', '2040-01', '2040-02', '2040-03', '2040-04', '2040-05', '2040-06', '2040-07', '2040-08', '2040-09', '2040-10', '2040-11', '2040-12', '2041-01', '2041-02', '2041-03', '2041-04', '2041-05', '2041-06', '2041-07', '2041-08', '2041-09', '2041-10', '2041-11', '2041-12', '2042-01', '2042-02', '2042-03', '2042-04', '2042-05', '2042-06', '2042-07', '2042-08', '2042-09', '2042-10', '2042-12', '2043-01', '2043-02', '2043-03', '2043-04', '2043-05', '2043-06', '2043-07', '2043-08', '2043-09', '2043-10', '2043-11', '2043-12', '2044-01', '2044-02', '2044-03', '2044-04', '2044-05', '2044-06', '2044-07', '2044-08', '2044-09', '2044-10', '2044-11', '2044-12', '2045-01', '2045-02', '2045-03', '2045-04', '2045-05', '2045-06', '2045-07', '2045-08', '2045-09', '2045-10', '2045-11', '2045-12', '2046-01', '2046-02', '2046-03', '2046-04', '2046-05', '2046-06', '2046-07', '2046-08', '2046-09', '2046-10', '2046-11', '2046-12', '2047-01', '2047-02', '2047-03', '2047-05', '2047-06', '2047-07', '2047-08', '2047-09', '2047-10', '2047-11', '2047-12', '2048-01', '2048-02', '2048-03', '2048-04', '2048-05', '2048-06', '2048-07', '2048-08', '2048-09', '2048-10', '2048-11', '2048-12', '2049-01', '2049-02', '2049-03', '2049-04', '2049-05', '2049-06', '2049-07', '2049-08', '2049-09', '2049-10', '2049-11', '2049-12', '2050-01', '2050-02', '2050-03', '2050-04', '2050-05', '2050-06', '2050-07', '2050-08', '2050-09', '2050-10', '2050-11', '2050-12', '2051-01', '2051-02', '2051-03', '2051-04', '2051-05', '2051-06', '2051-07', '2051-08', '2051-09', '2051-10', '2051-11', '2051-12', '2052-01', '2052-03', '2052-04', '2052-05', '2052-06', '2052-07', '2052-08', '2052-09', '2052-10', '2052-11', '2052-12', '2053-01', '2053-02', '2053-03', '2053-04', '2053-05', '2053-06', '2053-07', '2053-08', '2053-09', '2053-10', '2053-11', '2053-12', '2054-01', '2054-02', '2054-03', '2054-04', '2054-05', '2054-06', '2054-07', '2054-08', '2054-09', '2054-10', '2054-11', '2054-12', '2055-01', '2055-02', '2055-03', '2055-04', '2055-05', '2055-06', '2055-07', '2055-08', '2055-09', '2055-10', '2055-11', '2055-12', '2056-01', '2056-02', '2056-03', '2056-04', '2056-05', '2056-06', '2056-07', '2056-08', '2056-10', '2056-11', '2056-12', '2057-01', '2057-02', '2057-03', '2057-04', '2057-05', '2057-06', '2057-07', '2057-08', '2057-09', '2057-10', '2057-11', '2057-12', '2058-01', '2058-02', '2058-03', '2058-04', '2058-05', '2058-06', '2058-07', '2058-08', '2058-09', '2058-10', '2058-11', '2058-12', '2059-01', '2059-02', '2059-03', '2059-04', '2059-05', '2059-06', '2059-07', '2059-08', '2059-09', '2059-10', '2059-11', '2059-12', '2060-01', '2060-02', '2060-03', '2060-04', '2060-05', '2060-06', '2060-07', '2060-08', '2060-09', '2060-10', '2060-11', '2060-12', '2061-01', '2061-03', '2061-04', '2061-05', '2061-06', '2061-07', '2061-08', '2061-09', '2061-10', '2061-11', '2061-12', '2062-01', '2062-02', '2062-03', '2062-04', '2062-05', '2062-06', '2062-07', '2062-08', '2062-09', '2062-10', '2062-11', '2062-12', '2063-01', '2063-02', '2063-03', '2063-04', '2063-05', '2063-06', '2063-07', '2063-08', '2063-09', '2063-10', '2063-11', '2063-12', '2064-01', '2064-02', '2064-03', '2064-04', '2064-05', '2064-06', '2064-07', '2064-08', '2064-09', '2064-10', '2064-11', '2064-12', '2065-01', '2065-02', '2065-03', '2065-04', '2065-05', '2065-06', '2065-07', '2065-08', '2065-09', '2065-10', '2065-12', '2066-01', '2066-02', '2066-03', '2066-04', '2066-05', '2066-06', '2066-07', '2066-08', '2066-09', '2066-10', '2066-11', '2066-12', '2067-01', '2067-02', '2067-03', '2067-04', '2067-05', '2067-06', '2067-07', '2067-08', '2067-09', '2067-10', '2067-11', '2067-12', '2068-01', '2068-02', '2068-03', '2068-04', '2068-05', '2068-06', '2068-07', '2068-08', '2068-09', '2068-10', '2068-11', '2068-12', '2069-01', '2069-02', '2069-03', '2069-04', '2069-05', '2069-06', '2069-07', '2069-08', '2069-09', '2069-10', '2069-11', '2069-12', '2070-01', '2070-02', '2070-03', '2070-05', '2070-06', '2070-07', '2070-08', '2070-09', '2070-10', '2070-11', '2070-12', '2071-01', '2071-02', '2071-03', '2071-04', '2071-05', '2071-06', '2071-07', '2071-08', '2071-09', '2071-10', '2071-11', '2071-12', '2072-01', '2072-02', '2072-03', '2072-04', '2072-05', '2072-06', '2072-07', '2072-08', '2072-09', '2072-10', '2072-11', '2072-12', '2073-01', '2073-02', '2073-03', '2073-04', '2073-05', '2073-06', '2073-07', '2073-08', '2073-09', '2073-10', '2073-11', '2073-12', '2074-01', '2074-02', '2074-03', '2074-04', '2074-05', '2074-06', '2074-07', '2074-08', '2074-09', '2074-10', '2074-11', '2074-12', '2075-01', '2075-03', '2075-04', '2075-05', '2075-06', '2075-07', '2075-08', '2075-09', '2075-10', '2075-11', '2075-12', '2076-01', '2076-02', '2076-03', '2076-04', '2076-05', '2076-06', '2076-07', '2076-08', '2076-09', '2076-10', '2076-11', '2076-12', '2077-01', '2077-02', '2077-03', '2077-04', '2077-05', '2077-06', '2077-07', '2077-08', '2077-09', '2077-10', '2077-11', '2077-12', '2078-01', '2078-02', '2078-03', '2078-04', '2078-05', '2078-06', '2078-07', '2078-08', '2078-09', '2078-10', '2078-11', '2078-12', '2079-01', '2079-02', '2079-03', '2079-04', '2079-05', '2079-07', '2079-08', '2079-09', '2079-10', '2079-11', '2079-12']\n",
      "['12h', '15m', '1d', '1h', '1m', '1mo', '1w', '2h', '30m', '3d', '3m', '4h', '5m', '6h', '8h']\n",
      "Symbol,month and chart arrays are successfully created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data creation utilities successfully initialized\n"
     ]
    }
   ],
   "source": [
    "%run ./data_creation_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d6e7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T09:28:13.141263Z",
     "start_time": "2024-01-15T09:28:13.137718Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from selenium import webdriver\n",
    "import talib\n",
    "import datetime\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import glob\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import inspect\n",
    "import talib\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.request\n",
    "from datetime import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c20fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T09:28:01.465492Z",
     "start_time": "2024-01-15T09:28:01.465492Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f346cdf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T09:33:48.532612Z",
     "start_time": "2024-01-15T09:33:48.522540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01\n",
      "['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12', '2024-01']\n"
     ]
    }
   ],
   "source": [
    "# Get the current date\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date as a string in the desired format\n",
    "date_string = now.strftime(\"%Y-%m\")\n",
    "\n",
    "# Print the date string\n",
    "print(date_string)\n",
    "\n",
    "idx = MONTH_ARRAY.index(date_string)\n",
    "MONTH_ARRAY = MONTH_ARRAY[:idx + 1]\n",
    "# months = MONTH_ARRAY\n",
    "print(MONTH_ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79712b02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T09:33:48.994488Z",
     "start_time": "2024-01-15T09:33:48.983966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-06', '2024-01-07', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12', '2024-01-13', '2024-01-14', '2024-01-15']\n"
     ]
    }
   ],
   "source": [
    "# Get the first day of the current month\n",
    "first_day = now.replace(day=1)\n",
    "\n",
    "# Create an empty list to store the days\n",
    "DAY_ARRAY = []\n",
    "\n",
    "# Loop through the days from the first day of the current month to today\n",
    "while first_day <= now:\n",
    "    # Format the date as a string in the desired format\n",
    "    date_string = first_day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Add the date string to the list\n",
    "    DAY_ARRAY.append(date_string)\n",
    "\n",
    "    # Move to the next day\n",
    "    first_day += timedelta(days=1)\n",
    "print(DAY_ARRAY)\n",
    "# days = DAY_ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d5e1553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T11:39:14.351418Z",
     "start_time": "2024-01-15T11:39:14.335458Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "master_dictionary = {\n",
    "    \"symbols\": [\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('BTCUSDT')],\n",
    "#         SYMBOL_ARRAY[SYMBOL_ARRAY.index('1000PEPEUSDT')],\n",
    "        SYMBOL_ARRAY[SYMBOL_ARRAY.index('PYTHUSDT')],\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('ETHUSDT')],\n",
    "        #     SYMBOL_ARRAY[SYMBOL_ARRAY.index('ETHBUSD')],\n",
    "        #         SYMBOL_ARRAY[SYMBOL_ARRAY.index('BTCBUSD')]\n",
    "    ],\n",
    "    \"chart_times\": [\n",
    "        #     CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('5m')],\n",
    "        #     CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('1m')],\n",
    "        CHART_TIME_ARRAY[CHART_TIME_ARRAY.index('12h')]\n",
    "    ],\n",
    "    \"timeperiods\": [5, 8, 13, 21, 30, 34, 50, 55, 89, 100, 144, 200, 233],\n",
    "    \"win_percentage\":\n",
    "    0.8,\n",
    "    \"loss_percentage\":\n",
    "    0.4,\n",
    "    \"monthly_or_daily_1_or_2\":\n",
    "    1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1e13f",
   "metadata": {},
   "source": [
    "# code for downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d18c6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T09:29:12.460451Z",
     "start_time": "2024-01-15T09:29:12.409087Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'months'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m folder_path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m month \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmaster_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Construct the link\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBINANCE_MONTHLY_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m     symbol_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'months'"
     ]
    }
   ],
   "source": [
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "#         root_dir = Path.cwd()\n",
    "#         # Create the new folder path\n",
    "#         folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "#         folder_path.mkdir(parents=True, exist_ok=True)\n",
    "#         count = 0\n",
    "#         for month in master_dictionary[\"months\"]:\n",
    "#             # Construct the link\n",
    "#             link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "#             symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "#             # Create the file path\n",
    "#             file_path = Path(folder_path) / symbol_object\n",
    "#             if not file_path.exists():\n",
    "#                 try:\n",
    "#                     # Download the file\n",
    "#                     urllib.request.urlretrieve(link, file_path)\n",
    "#                     count += 1\n",
    "#                 except:\n",
    "# #                     print(f'{link} not found')\n",
    "#                     continue\n",
    "#         if count > 0:\n",
    "#             print(\"***                  ***\")\n",
    "#             print(f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "#         else:\n",
    "#             print(\"you're already up to date\")\n",
    "\n",
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "#         root_dir = Path.cwd()\n",
    "#         # Create the new folder path\n",
    "#         folder_path = Path(download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "#         folder_path.mkdir(parents=True, exist_ok=True)\n",
    "#         count = 0\n",
    "#         for day in master_dictionary[\"days\"]:\n",
    "#             # Construct the link\n",
    "#             link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "#             symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "#             # Create the file path\n",
    "#             file_path = Path(folder_path) / symbol_object\n",
    "#             if not file_path.exists():\n",
    "#                 try:\n",
    "#                     # Download the file\n",
    "#                     urllib.request.urlretrieve(link, file_path)\n",
    "#                     count += 1\n",
    "#                 except:\n",
    "# #                     print(f'{link} not found')\n",
    "#                     continue\n",
    "#         if count > 0:\n",
    "#             print(\"***                  ***\")\n",
    "#             print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "#         else:\n",
    "#             print(\"you're already up to date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a99449",
   "metadata": {},
   "source": [
    "## new pyspark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d565a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T10:04:29.268631Z",
     "start_time": "2023-08-20T10:04:28.161759Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for symbol in master_dictionary[\"symbols\"]:\n",
    "#     for chart_time in master_dictionary[\"chart_times\"]:\n",
    "# #         # Set up an empty list for the data frames\n",
    "# #         df_list = []\n",
    "#         spark = SparkSession.builder.appName('DataProcessing').getOrCreate()\n",
    "#         # Compile the regular expression pattern\n",
    "#         pattern = re.compile(f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "#         # Create the new folder path for ZIP files\n",
    "#         new_zip_folder_path = os.path.join(download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "#         # Create the new folder path for CSV files\n",
    "#         print(output_dir)\n",
    "#         new_csv_folder_path = os.path.join(output_dir, f\"{symbol}-{chart_time}\")\n",
    "#         df_final = None\n",
    "\n",
    "#         # Iterate over the files in the new zip folder\n",
    "#         for file in os.listdir(new_zip_folder_path):\n",
    "#             # Check if the file matches the pattern\n",
    "#             if pattern.match(file):\n",
    "#                 # Construct the file path\n",
    "#                 file_path = os.path.join(new_zip_folder_path, file)\n",
    "\n",
    "#                 # Extract the ZIP file\n",
    "#                 with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "#                     zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "#                 # Construct the CSV file path\n",
    "#                 csv_file_path = os.path.join(new_csv_folder_path, f\"{symbol}-{chart_time}{file[-12:-4]}.csv\")\n",
    "\n",
    "#                 # Read the CSV file into a Spark DataFrame\n",
    "#                 df = spark.read.csv(csv_file_path, header=True)\n",
    "                \n",
    "# #                 df.show()\n",
    "\n",
    "# #                 # Filter out the header row\n",
    "# #                 df = df.filter(df['open_time'] != 'open_time')\n",
    "        \n",
    "#                 if df_final is None:\n",
    "#                     df_final = df\n",
    "#                 else:\n",
    "#                     df_final = df_final.union(df)\n",
    "#                 # Add it to the list\n",
    "#                 df_list.append(df)\n",
    "\n",
    "#         df_final.show()\n",
    "#         # Convert 'open_time' and 'close_time' columns to timestamp\n",
    "#         df_final = df_final.withColumn(\"open_time\", df_final[\"open_time\"].cast(\"timestamp\"))\n",
    "#         df_final = df_final.withColumn(\"close_time\", df_final[\"close_time\"].cast(\"timestamp\"))\n",
    "\n",
    "#         # Drop the 'ignore' column\n",
    "#         df_final = df_final.drop(\"ignore\")\n",
    "\n",
    "#         # Your existing code to create the df_final DataFrame\n",
    "#         windowSpec = Window.orderBy(\"open_time\")\n",
    "#         df_final = df_final.withColumn(\"entry\", lag(\"close\").over(windowSpec))\n",
    "        \n",
    "#         df_final.show()\n",
    "#         # Convert PySpark DataFrame to Pandas DataFrame\n",
    "#         df_final_pandas = df_final.toPandas()\n",
    "        \n",
    "#         spark.stop()\n",
    "\n",
    "# #         # Set the file name\n",
    "# #         file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "# #         # Construct the file path\n",
    "# #         file_path = os.path.join(new_csv_folder_path, file_name)\n",
    "# #         print(file_path)\n",
    "\n",
    "# #         # Write the Pandas DataFrame to a CSV file\n",
    "# #         df_final_pandas.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d026ee3",
   "metadata": {},
   "source": [
    "## download data functions, process zip, concatenate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15b360c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T11:42:24.163646Z",
     "start_time": "2024-01-15T11:42:24.092792Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_monthly_data(month_array, symbol, chart_time):\n",
    "    #downloading monthly data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(\n",
    "        download_dir) / f\"{symbol}-{chart_time}-monthly_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for month in month_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_MONTHLY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{month}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{month}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print (f\"Monthly Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print (f\"you're already up to date for monthly data for {symbol},{chart_time}\")\n",
    "    \n",
    "\n",
    "    \n",
    "def download_daily_data(day_array, symbol, chart_time):\n",
    "    #downloading daily data\n",
    "    root_dir = Path.cwd()\n",
    "    # Create the new folder path\n",
    "    folder_path = Path(\n",
    "        download_dir) / f\"{symbol}-{chart_time}-daily_data\"\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    for day in day_array:\n",
    "        # Construct the link\n",
    "        link = f\"{BINANCE_DAILY_URL}{symbol}/{chart_time}/{symbol}-{chart_time}-{day}.zip\"\n",
    "        symbol_object = f\"{symbol}-{chart_time}-{day}.zip\"\n",
    "        # Create the file path\n",
    "        file_path = Path(folder_path) / symbol_object\n",
    "        if not file_path.exists():\n",
    "            try:\n",
    "                # Download the file\n",
    "                urllib.request.urlretrieve(link, file_path)\n",
    "                count += 1\n",
    "            except:\n",
    "                #                     print(f'{link} not found')\n",
    "                continue\n",
    "    if count > 0:\n",
    "        print(f\"Daily Data Downloaded for {symbol},{chart_time}\")\n",
    "    else:\n",
    "        print(f\"you're already up to date for daily data for {symbol},{chart_time}\")\n",
    "        \n",
    "def construct_csv_file_path(folder_path, symbol, chart_time, file, is_daily=False):\n",
    "    if is_daily:\n",
    "        # For daily data, use a different pattern\n",
    "        return os.path.join(\n",
    "            folder_path,\n",
    "            f\"{symbol}-{chart_time}-{file.split('-')[-3]}-{file.split('-')[-2]}-{file.split('-')[-1][:-4]}.csv\"\n",
    "        )\n",
    "    else:\n",
    "        # For monthly data, use the original pattern\n",
    "        return os.path.join(\n",
    "            folder_path,\n",
    "            f\"{symbol}-{chart_time}{file[-12:-4]}.csv\"\n",
    "        )\n",
    "\n",
    "def process_zip_folder(folder_path, pattern, new_csv_folder_path, symbol, chart_time, df_list, is_daily=False):\n",
    "    for file in os.listdir(folder_path):\n",
    "        # Check if the file matches the pattern\n",
    "        if pattern.match(file):\n",
    "            # Construct the file path\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            # Extract the ZIP file\n",
    "            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "            # Construct the CSV file path using the helper function\n",
    "            csv_file_path = construct_csv_file_path(\n",
    "                new_csv_folder_path, symbol, chart_time, file, is_daily)\n",
    "\n",
    "            # Read the CSV file into a data frame, ignoring the headers\n",
    "            df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "            # Remove the first row (which contains the header)\n",
    "            df = df.iloc[1:]\n",
    "\n",
    "            # Add it to the list\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list\n",
    "\n",
    "\n",
    "\n",
    "def concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time):\n",
    "    # Concatenate the data frames in the list\n",
    "    df_final = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    csv_file_path = os.path.join(new_csv_folder_path, os.listdir(new_csv_folder_path)[0])\n",
    "\n",
    "    # Read the headers from the first CSV file\n",
    "    headers = pd.read_csv(csv_file_path, nrows=1).columns\n",
    "    \n",
    "    # Set the headers as the column names of the final dataframe\n",
    "    df_final.columns = headers\n",
    "\n",
    "    # Convert 'open_time' and 'close_time' columns to datetime\n",
    "    df_final['open_time'] = pd.to_datetime(\n",
    "        df_final['open_time'],\n",
    "        unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "    df_final['close_time'] = pd.to_datetime(\n",
    "        df_final['close_time'],\n",
    "        unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "    # Delete the 'ignore' column\n",
    "    df_final = df_final.drop(['ignore'], axis=1)\n",
    "\n",
    "    # Add a new column called 'entry' that will take previous close\n",
    "    df_final['entry'] = df_final['close'].shift(1)\n",
    "\n",
    "    # Set the file name\n",
    "    concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "    # Construct the file path\n",
    "    concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                          concatenated_file_name)\n",
    "\n",
    "    # Write the data frame to the CSV file\n",
    "    df_final.to_csv(concatenated_file_path, index=False)\n",
    "\n",
    "    directory_final = Path(\n",
    "        concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "    # Deleting all the other CSVs\n",
    "    for file_path in directory_final.iterdir():\n",
    "        if file_path != Path(concatenated_file_path):\n",
    "            if file_path.is_file():\n",
    "                file_path.unlink()\n",
    "    return (\"data concatenated, individual csvs deleted\")\n",
    "\n",
    "\n",
    "def calculate_indicators_using_talib(timeperiods, df):\n",
    "    new_columns = pd.DataFrame()\n",
    "\n",
    "    # List to store indicator columns\n",
    "    indicator_columns = []\n",
    "    indicator_columns.append(('HT_TRENDLINE', talib.HT_TRENDLINE(df['close'])))\n",
    "    # indicator_columns.append(('MAMA', df['MAMA']), ('FAMA', df['FAMA']))\n",
    "    # indicator_columns.append(('MAVP', df['MAVP']))\n",
    "    indicator_columns.append(('SAR', talib.SAR(df['high'], df['low'], acceleration=0, maximum=0)))\n",
    "    indicator_columns.append(('SAREXT', talib.SAREXT(df['high'], df['low'])))\n",
    "    indicator_columns.append(('T3', talib.T3(df['close'], timeperiod=5, vfactor=0)))\n",
    "    # Momentum Indicators\n",
    "    indicator_columns.append(('APO', talib.APO(df['close'], fastperiod=12, slowperiod=26)))\n",
    "    indicator_columns.append(('BOP', talib.BOP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    macd, macd_signal, macd_hist = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    indicator_columns.append(('MACD', macd))\n",
    "    indicator_columns.append(('MACD_signal', macd_signal))\n",
    "    indicator_columns.append(('MACD_hist', macd_hist))\n",
    "    indicator_columns.append(('PPO', talib.PPO(df['close'], fastperiod=12, slowperiod=26, matype=0)))\n",
    "    indicator_columns.append(('TRIX', talib.TRIX(df['close'])))\n",
    "    indicator_columns.append(('ULTOSC', talib.ULTOSC(df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('WILLR', talib.WILLR(df['high'], df['low'], df['close'])))\n",
    "\n",
    "\n",
    "#     # Not Working ATM\n",
    "#     indicator_columns.append(('STOCH', talib.STOCH(df['high'], df['low'], df['close'])))\n",
    "#     indicator_columns.append(('STOCHF', talib.STOCHF(df['high'], df['low'], df['close'])))\n",
    "#     indicator_columns.append(('STOCHRSI', talib.STOCHRSI(df['close'])))\n",
    "#     indicator_columns.append(('MACDEXT', talib.MACDEXT(df['close'], fastperiod=12, fastmatype=0, slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)))\n",
    "#     indicator_columns.append(('MACDFIX', talib.MACDFIX(df['close'], signalperiod=9)))\n",
    "    \n",
    "    \n",
    "    #########Volume Indicators\n",
    "    indicator_columns.append(('AD', talib.AD(df['high'], df['low'], df['close'], df['volume'])))\n",
    "    indicator_columns.append(('ADOSC', talib.ADOSC(df['high'], df['low'], df['close'], df['volume'], fastperiod=3, slowperiod=10)))\n",
    "    indicator_columns.append(('OBV', talib.OBV(df['close'], df['volume'])))\n",
    "\n",
    "    #########Cycle Indicators\n",
    "    indicator_columns.append(('HT_DCPERIOD', talib.HT_DCPERIOD(df['close'])))\n",
    "    indicator_columns.append(('HT_DCPHASE', talib.HT_DCPHASE(df['close'])))\n",
    "    phasor_inphase, phasor_quadrature = talib.HT_PHASOR(df['close'])\n",
    "    indicator_columns.append(('HT_PHASOR_inphase', phasor_inphase))\n",
    "    indicator_columns.append(('HT_PHASOR_quadrature', phasor_quadrature))\n",
    "    # indicator_columns.append(('HT_SINE', talib.HT_SINE(df['close'])))\n",
    "    indicator_columns.append(('HT_TRENDMODE', talib.HT_TRENDMODE(df['close'])))\n",
    "\n",
    "    #########Price Transform\n",
    "    indicator_columns.append(('AVGPRICE', talib.AVGPRICE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('MEDPRICE', talib.MEDPRICE(df['high'], df['low'])))\n",
    "    indicator_columns.append(('TYPPRICE', talib.TYPPRICE(df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('WCLPRICE', talib.WCLPRICE(df['high'], df['low'], df['close'])))\n",
    "    #########Volatility Indicators\n",
    "    indicator_columns.append(('TRANGE', talib.TRANGE(df['high'], df['low'], df['close'])))\n",
    "    #########Pattern Recognition\n",
    "    indicator_columns.append(('CDL2CROWS', talib.CDL2CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3BLACKCROWS', talib.CDL3BLACKCROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3INSIDE', talib.CDL3INSIDE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3LINESTRIKE', talib.CDL3LINESTRIKE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3OUTSIDE', talib.CDL3OUTSIDE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3STARSINSOUTH', talib.CDL3STARSINSOUTH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDL3WHITESOLDIERS', talib.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLABANDONEDBABY', talib.CDLABANDONEDBABY(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLADVANCEBLOCK', talib.CDLADVANCEBLOCK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLBELTHOLD', talib.CDLBELTHOLD(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLBREAKAWAY', talib.CDLBREAKAWAY(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCLOSINGMARUBOZU', talib.CDLCLOSINGMARUBOZU(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCONCEALBABYSWALL', talib.CDLCONCEALBABYSWALL(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLCOUNTERATTACK', talib.CDLCOUNTERATTACK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDARKCLOUDCOVER', talib.CDLDARKCLOUDCOVER(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLDOJI', talib.CDLDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDOJISTAR', talib.CDLDOJISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLDRAGONFLYDOJI', talib.CDLDRAGONFLYDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLENGULFING', talib.CDLENGULFING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLEVENINGDOJISTAR', talib.CDLEVENINGDOJISTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "\n",
    "    indicator_columns.append(('CDLEVENINGSTAR', talib.CDLEVENINGSTAR(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "    indicator_columns.append(('CDLGAPSIDESIDEWHITE', talib.CDLGAPSIDESIDEWHITE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLGRAVESTONEDOJI', talib.CDLGRAVESTONEDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHAMMER', talib.CDLHAMMER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHANGINGMAN', talib.CDLHANGINGMAN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHARAMI', talib.CDLHARAMI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHARAMICROSS', talib.CDLHARAMICROSS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIGHWAVE', talib.CDLHIGHWAVE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIKKAKE', talib.CDLHIKKAKE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHIKKAKEMOD', talib.CDLHIKKAKEMOD(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLHOMINGPIGEON', talib.CDLHOMINGPIGEON(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLIDENTICAL3CROWS', talib.CDLIDENTICAL3CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLINNECK', talib.CDLINNECK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLINVERTEDHAMMER', talib.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLKICKING', talib.CDLKICKING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLKICKINGBYLENGTH', talib.CDLKICKINGBYLENGTH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLADDERBOTTOM', talib.CDLLADDERBOTTOM(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLONGLEGGEDDOJI', talib.CDLLONGLEGGEDDOJI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLLONGLINE', talib.CDLLONGLINE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMARUBOZU', talib.CDLMARUBOZU(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMATCHINGLOW', talib.CDLMATCHINGLOW(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMATHOLD', talib.CDLMATHOLD(df['open'], df['high'], df['low'], df['close'], penetration=0)))\n",
    "    indicator_columns.append(('CDLMORNINGDOJISTAR', talib.CDLMORNINGDOJISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLMORNINGSTAR', talib.CDLMORNINGSTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLONNECK', talib.CDLONNECK(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLPIERCING', talib.CDLPIERCING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLRICKSHAWMAN', talib.CDLRICKSHAWMAN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLRISEFALL3METHODS', talib.CDLRISEFALL3METHODS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSEPARATINGLINES', talib.CDLSEPARATINGLINES(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSHOOTINGSTAR', talib.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSHORTLINE', talib.CDLSHORTLINE(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSPINNINGTOP', talib.CDLSPINNINGTOP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSTALLEDPATTERN', talib.CDLSTALLEDPATTERN(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLSTICKSANDWICH', talib.CDLSTICKSANDWICH(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTAKURI', talib.CDLTAKURI(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTASUKIGAP', talib.CDLTASUKIGAP(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTHRUSTING', talib.CDLTHRUSTING(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLTRISTAR', talib.CDLTRISTAR(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLUNIQUE3RIVER', talib.CDLUNIQUE3RIVER(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLUPSIDEGAP2CROWS', talib.CDLUPSIDEGAP2CROWS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    indicator_columns.append(('CDLXSIDEGAP3METHODS', talib.CDLXSIDEGAP3METHODS(df['open'], df['high'], df['low'], df['close'])))\n",
    "    #########Statistic Functions\n",
    "    indicator_columns.append(('LINEARREG', talib.LINEARREG(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_ANGLE', talib.LINEARREG_ANGLE(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_INTERCEPT', talib.LINEARREG_INTERCEPT(df['close'])))\n",
    "    indicator_columns.append(('LINEARREG_SLOPE', talib.LINEARREG_SLOPE(df['close'])))\n",
    "    # new_columns['STDDEV'] = df['close'].rolling(timeperiod).std()\n",
    "    indicator_columns.append(('TSF', talib.TSF(df['close'])))\n",
    "    indicator_columns.append(('VAR', talib.VAR(df['close'])))\n",
    "    # Iterate over the time periods\n",
    "    for timeperiod in timeperiods:\n",
    "        #########Overlap Studies\n",
    "        upper_band, middle_band, lower_band = talib.BBANDS(df['close'], timeperiod=timeperiod)\n",
    "        indicator_columns.append((f'BB_upper_{timeperiod}', upper_band))\n",
    "        indicator_columns.append((f'BB_middle_{timeperiod}', middle_band))\n",
    "        indicator_columns.append((f'BB_lower_{timeperiod}', lower_band))\n",
    "        indicator_columns.append((f'DEMA_{timeperiod}', talib.DEMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'EMA_{timeperiod}', talib.EMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'KAMA_{timeperiod}', talib.KAMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MA_{timeperiod}', talib.MA(df['close'], timeperiod=timeperiod)))\n",
    "        # new_columns['MAMA'], new_columns['FAMA'] = talib.MAMA(df['close'], fastlimit=0, slowlimit=0)\n",
    "        # new_columns['MAVP'] = talib.MAVP(df['close'], periods=None, minperiod=2, maxperiod=30, matype=0)\n",
    "        indicator_columns.append((f'MIDPOINT_{timeperiod}', talib.MIDPOINT(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MIDPRICE_{timeperiod}', talib.MIDPRICE(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'SMA_{timeperiod}', talib.SMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'TEMA_{timeperiod}', talib.TEMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'TRIMA_{timeperiod}', talib.TRIMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'WMA_{timeperiod}', talib.WMA(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ADX_{timeperiod}', talib.ADX(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ADXR_{timeperiod}', talib.ADXR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        aroon_up, aroon_down = talib.AROON(df['high'], df['low'], timeperiod=timeperiod)\n",
    "        indicator_columns.append((f'AROON_up_{timeperiod}', aroon_up))\n",
    "        indicator_columns.append((f'AROON_down_{timeperiod}', aroon_down))\n",
    "        indicator_columns.append((f'AROONOSC_{timeperiod}', talib.AROONOSC(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CCI_{timeperiod}', talib.CCI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CMO_{timeperiod}', talib.CMO(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'DX_{timeperiod}', talib.DX(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MFI_{timeperiod}', talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MINUS_DI_{timeperiod}', talib.MINUS_DI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MINUS_DM_{timeperiod}', talib.MINUS_DM(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'MOM_{timeperiod}', talib.MOM(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'PLUS_DI_{timeperiod}', talib.PLUS_DI(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'PLUS_DM_{timeperiod}', talib.PLUS_DM(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROC_{timeperiod}', talib.ROC(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCP_{timeperiod}', talib.ROCP(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCR_{timeperiod}', talib.ROCR(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'ROCR100_{timeperiod}', talib.ROCR100(df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'RSI_{timeperiod}', talib.RSI(df['close'], timeperiod=timeperiod)))\n",
    "\n",
    "        indicator_columns.append((f'ATR_{timeperiod}', talib.ATR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'NATR_{timeperiod}', talib.NATR(df['high'], df['low'], df['close'], timeperiod=timeperiod)))\n",
    "        #########Statistic Functions\n",
    "        indicator_columns.append((f'BETA_{timeperiod}', talib.BETA(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "        indicator_columns.append((f'CORREL_{timeperiod}', talib.CORREL(df['high'], df['low'], timeperiod=timeperiod)))\n",
    "    new_columns = pd.concat([pd.DataFrame(data, columns=[name]) for name, data in indicator_columns], axis=1)\n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d2c7ad0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:11:46.482320Z",
     "start_time": "2024-01-15T10:11:46.468906Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_data_and_concatenate(master_dictionary, month_array, day_array):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"setting up things for {symbol},{chart_time}\")\n",
    "\n",
    "            # Set up an empty list for the data frames\n",
    "            df_list = []\n",
    "\n",
    "            # Compile the regular expression pattern\n",
    "            pattern = re.compile(f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Compile the regular expression pattern for daily zip files\n",
    "            pattern_daily = re.compile(\n",
    "                f\"^{symbol}-{chart_time}-\\d{{4}}-\\d{{2}}-\\d{{2}}\\.zip$\")\n",
    "\n",
    "            # Create the new folder path for daily ZIP files\n",
    "            new_daily_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-daily_data\")\n",
    "\n",
    "            # Create the new folder path for ZIP files\n",
    "            new_monthly_zip_folder_path = os.path.join(\n",
    "                download_dir, f\"{symbol}-{chart_time}-monthly_data\")\n",
    "\n",
    "            # Create the new folder path for CSV files\n",
    "            new_csv_folder_path = os.path.join(output_dir,\n",
    "                                               f\"{symbol}-{chart_time}\")\n",
    "\n",
    "            # Set the file name\n",
    "            concatenated_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "\n",
    "            # Construct the file path\n",
    "            concatenated_file_path = os.path.join(new_csv_folder_path,\n",
    "                                                  concatenated_file_name)\n",
    "\n",
    "            if not Path(concatenated_file_path).exists():\n",
    "                download_monthly_data(month_array, symbol, chart_time)  \n",
    "                download_daily_data(day_array, symbol, chart_time)  \n",
    "\n",
    "                # Process the monthly ZIP folder and add to df_list\n",
    "                df_list = process_zip_folder(new_monthly_zip_folder_path, pattern, new_csv_folder_path, symbol, chart_time, df_list)\n",
    "\n",
    "                # Process the daily ZIP folder and add to df_list\n",
    "                df_list = process_zip_folder(new_daily_zip_folder_path, pattern_daily, new_csv_folder_path, symbol, chart_time, df_list, is_daily=True)\n",
    "\n",
    "                # Call the function to concatenate and process the data frames\n",
    "                print(concatenate_data_frames(df_list, new_csv_folder_path, symbol, chart_time))\n",
    "#             else:\n",
    "#                 df_list = []\n",
    "#                 df = pd.read_csv(concatenated_file_path, header=None)\n",
    "#                 last_record = df.iloc[-1:]\n",
    "#                 df_list.append(last_record)\n",
    "\n",
    "#                 df = pd.read_csv(concatenated_file_path)\n",
    "#                 df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "#                 last_record = df.iloc[-1:]\n",
    "\n",
    "\n",
    "#                 # Check if the time part of 'open_time' is \"05:15:00+05:30\"\n",
    "#                 if last_record['open_time'].dt.strftime(\n",
    "#                         '%H:%M:%S%z').item() == \"05:15:00+0530\":\n",
    "#                     # Calculate the last processed date by subtracting 1 day from the date part\n",
    "#                     last_processed_date = (\n",
    "#                         last_record['open_time'].dt.date -\n",
    "#                         pd.DateOffset(days=1)).item().strftime('%Y-%m-%d')\n",
    "#                     last_processed_date = datetime.strptime(\n",
    "#                         last_processed_date, \"%Y-%m-%d\")\n",
    "\n",
    "#                     # Get today's date\n",
    "#                     today = datetime.today()\n",
    "\n",
    "#                     # Initialize DAY_ARRAY\n",
    "#                     DAY_ARRAY = []\n",
    "\n",
    "#                     # Start from the day after last_processed_date\n",
    "#                     current_day = last_processed_date + timedelta(days=1)\n",
    "\n",
    "#                     while current_day <= today:\n",
    "#                         # Format the date as a string in the desired format\n",
    "#                         date_string = current_day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#                         # Add the date string to DAY_ARRAY\n",
    "#                         DAY_ARRAY.append(date_string)\n",
    "\n",
    "#                         # Move to the next day\n",
    "#                         current_day += timedelta(days=1)\n",
    "\n",
    "#                     download_daily_data()\n",
    "#                     # Iterate over the files in the new daily zip folder\n",
    "#                     for file in Path(new_daily_zip_folder_path).iterdir():\n",
    "#                         # Check if the file matches the pattern\n",
    "#                         if pattern_daily.match(file.name):\n",
    "#                             # Extract the date part from the file name (e.g., \"2023-08-19\")\n",
    "#                             zip_date = file.name.split('-')[-3:]\n",
    "#                             zip_date_str = '-'.join(zip_date).replace('.zip', '')\n",
    "\n",
    "#                             # Check if the date is in DAY_ARRAY\n",
    "#                             if zip_date_str in DAY_ARRAY:\n",
    "#                                 # Construct the file path\n",
    "#                                 file_path = file\n",
    "\n",
    "#                                 # Extract the ZIP file\n",
    "#                                 with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "#                                     zip_ref.extractall(new_csv_folder_path)\n",
    "\n",
    "#                                 # Construct the CSV file path\n",
    "#                                 csv_file_path = Path(new_csv_folder_path) / f\"{symbol}-{chart_time}-{zip_date_str}.csv\"\n",
    "\n",
    "#                                 # Read the CSV file into a data frame, ignoring the headers\n",
    "#                                 df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "#                                 # Remove the first row (which contains the header)\n",
    "#                                 df = df.iloc[1:]\n",
    "\n",
    "#                                 # Add it to the list\n",
    "#                                 df_list.append(df)\n",
    "#                     # Concatenate the data frames in the list\n",
    "#                     df_final = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "#                     # Read the headers from the first CSV file\n",
    "#                     headers = pd.read_csv(concatenated_file_path, nrows=1).columns\n",
    "\n",
    "#                     # Set the headers as the column names of the final dataframe\n",
    "#                     df_final.columns = headers\n",
    "\n",
    "#                     # Convert 'open_time' and 'close_time' columns to datetime\n",
    "#                     unix_time_format = df_final['open_time'].str.contains(r'^\\d{13}$')\n",
    "#                     df_final.loc[unix_time_format, 'open_time'] = pd.to_datetime(\n",
    "#                         df_final.loc[unix_time_format, 'open_time'],\n",
    "#                         unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "#                     unix_time_format = df_final['close_time'].str.contains(r'^\\d{13}$')\n",
    "#                     df_final.loc[unix_time_format, 'close_time'] = pd.to_datetime(\n",
    "#                         df_final.loc[unix_time_format, 'close_time'],\n",
    "#                         unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "\n",
    "\n",
    "#                     # Add a new column called 'entry' that will take previous close\n",
    "#                     df_final['entry'] = df_final['close'].shift(1)\n",
    "#                     # Append the data frame to the existing CSV file in append mode without headers\n",
    "#                     df_final[1:].to_csv(concatenated_file_path, mode='a', header=False, index=False)\n",
    "#                     directory_final = Path(\n",
    "#                     concatenated_file_path).parent  # Get the parent directory\n",
    "\n",
    "#                     # deleting all the other csvs\n",
    "#                     for file_path in directory_final.iterdir():\n",
    "#                         if file_path != Path(concatenated_file_path):\n",
    "#                             if file_path.is_file():\n",
    "#                                 file_path.unlink()\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     print(\"Something is wrong with data\")\n",
    "    return (\"data downloaded and concatenated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3e9d97d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:16:03.585856Z",
     "start_time": "2024-01-15T10:15:27.610207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up things for PYTHUSDT,12h\n",
      "Monthly Data Downloaded for PYTHUSDT,12h\n",
      "Daily Data Downloaded for PYTHUSDT,12h\n",
      "data concatenated, individual csvs deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data downloaded and concatenated'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_data_and_concatenate(master_dictionary, MONTH_ARRAY, DAY_ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba603fd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:19:39.341715Z",
     "start_time": "2024-01-15T10:19:39.326821Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wins_losses(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            print(f\"calculating for {symbol}{chart_time}\")\n",
    "            # Construct the file name\n",
    "            og_file_name = f\"{symbol}-{chart_time}.csv\"\n",
    "            og_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{og_file_name}\"\n",
    "            new_file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            new_file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{new_file_name}\"\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(og_file_path)\n",
    "            df[\"if_short\"] = 0\n",
    "            df[\"if_long\"] = 0\n",
    "            df[\"long_target\"] = np.nan\n",
    "            df[\"short_target\"] = np.nan\n",
    "            df[\"long_stop_loss\"] = np.nan\n",
    "            df[\"short_stop_loss\"] = np.nan\n",
    "            df[\"shorts_win_after\"] = np.nan\n",
    "            df[\"longs_win_after\"] = np.nan\n",
    "            df[\"dual_loss\"] = 0\n",
    "            df[\"entered_before\"] = np.nan\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                if df[\"entry\"][i]:\n",
    "                    long_target = df[\"entry\"][i] * (1 + win_perc / 100)\n",
    "                    short_target = df[\"entry\"][i] * (1 - win_perc / 100)\n",
    "                    long_stop_loss = df[\"entry\"][i] * (1 - loss_perc / 100)\n",
    "                    short_stop_loss = df[\"entry\"][i] * (1 + loss_perc / 100)\n",
    "                    df.loc[i, 'long_target'] = long_target\n",
    "                    df.loc[i, 'long_stop_loss'] = long_stop_loss\n",
    "                    for j in range(i, len(df)):\n",
    "                        if df[\"high\"][j] >= long_target:\n",
    "                            if df[\"low\"][j] <= long_stop_loss:\n",
    "                                df.loc[i, 'if_long'] = -1\n",
    "                                df.loc[i, 'dual_loss'] = 1\n",
    "                                df.loc[i, 'entered_before'] = j - i\n",
    "                            else:\n",
    "                                df.loc[i, 'if_long'] = 1\n",
    "                                df.loc[i, 'longs_win_after'] = j - i\n",
    "                            break\n",
    "                        elif df[\"low\"][j] <= long_stop_loss:\n",
    "                            df.loc[i, 'if_long'] = -1\n",
    "                            break\n",
    "                    df.loc[i, 'short_target'] = short_target\n",
    "                    df.loc[i, 'short_stop_loss'] = short_stop_loss\n",
    "                    for j in range(i, len(df)):\n",
    "                        if df[\"low\"][j] <= short_target:\n",
    "                            if df[\"high\"][j] >= short_stop_loss:\n",
    "                                df.loc[i, 'if_short'] = -1\n",
    "                                df.loc[i, 'dual_loss'] = 1\n",
    "                                df.loc[i, 'entered_before'] = j - i\n",
    "                            else:\n",
    "                                df.loc[i, 'if_short'] = 1\n",
    "                                df.loc[i, 'shorts_win_after'] = j - i\n",
    "                            break\n",
    "                        elif df[\"high\"][j] >= short_stop_loss:\n",
    "                            df.loc[i, 'if_short'] = -1\n",
    "                            break\n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df.to_csv(new_file_path, index=False)\n",
    "\n",
    "    return (\"calculated wins and losses \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f7a7857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:26:36.250423Z",
     "start_time": "2024-01-15T10:26:36.140515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating for PYTHUSDT8h\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'calculated wins and losses '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_wins_losses(master_dictionary, win_perc=20, loss_perc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0110c233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T11:39:31.339511Z",
     "start_time": "2024-01-15T11:39:31.328923Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_indicator_values(master_dictionary, win_perc=0.73, loss_perc=0.4):\n",
    "    # Iterate over the symbols and chart times\n",
    "    for symbol in master_dictionary[\"symbols\"]:\n",
    "        for chart_time in master_dictionary[\"chart_times\"]:\n",
    "            # Construct the file name\n",
    "            file_name = f\"{symbol}-{chart_time}_W{win_perc}_L{loss_perc}.csv\"\n",
    "            file_path = Path(output_dir) / f\"{symbol}-{chart_time}/{file_name}\"\n",
    "            if not Path(file_path).exists():\n",
    "                print(f\"File path for {file_name} doesn't exist. Breaking.\")\n",
    "                break\n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(df.dtypes)\n",
    "            new_columns = calculate_indicators_using_talib(master_dictionary, df)\n",
    "            # Save the updated dataframe to the CSV file\n",
    "            df = pd.concat([df, new_columns], axis=1)\n",
    "            df.to_csv(file_path, index=False)\n",
    "    return (\"indicators are added to the csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "098e5767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T11:42:29.056020Z",
     "start_time": "2024-01-15T11:42:28.924183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_time                  object\n",
      "open                      float64\n",
      "high                      float64\n",
      "low                       float64\n",
      "close                     float64\n",
      "volume                      int64\n",
      "close_time                 object\n",
      "quote_volume              float64\n",
      "count                       int64\n",
      "taker_buy_volume            int64\n",
      "taker_buy_quote_volume    float64\n",
      "entry                     float64\n",
      "if_short                    int64\n",
      "if_long                     int64\n",
      "long_target               float64\n",
      "short_target              float64\n",
      "long_stop_loss            float64\n",
      "short_stop_loss           float64\n",
      "shorts_win_after          float64\n",
      "longs_win_after           float64\n",
      "dual_loss                   int64\n",
      "entered_before            float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'indicators are added to the csv'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_indicator_values_new(master_dictionary, win_perc=20, loss_perc=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
